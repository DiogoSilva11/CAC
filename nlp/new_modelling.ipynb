{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from xgboost import XGBClassifier\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLzom-2TITa4gasV7_fCCA</td>\n",
       "      <td>Great experience purchasing a washer and dryer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a5JHzBrWxRd_OmIvV7znDA</td>\n",
       "      <td>Went here based on the high ratings and raves ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X-o--dwf0HuFMittYi4wCA</td>\n",
       "      <td>oh Millers, how i wanted to like you.  You are...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INGNbsyo-MouZZzcxnCSGQ</td>\n",
       "      <td>This place gets two stars from me only because...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>k7VatXVLism-cTDJE8TTUw</td>\n",
       "      <td>This place was awesome. Clean, beautiful and t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11681</th>\n",
       "      <td>IlU-MQzMKc7jAHWwK5VFGQ</td>\n",
       "      <td>To be fair, I tried them in their first week. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11682</th>\n",
       "      <td>Qt3BsRvQuJccDQfFWM1XPw</td>\n",
       "      <td>Awful place. It's dirty. Had two birthday part...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11683</th>\n",
       "      <td>3CQQ8Im_UX6QqDECuXYK8A</td>\n",
       "      <td>A truly vegetarian delight!  I took a Jewish f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11684</th>\n",
       "      <td>ery1nBM7zKweFLBe-bT5ag</td>\n",
       "      <td>I have a 2011 Toyota Sienna Limited. During th...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11685</th>\n",
       "      <td>N5_SaVzmwkZUslgWDGGsQQ</td>\n",
       "      <td>I'm a single father raising my 17 year old son...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11686 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    review_id  \\\n",
       "0      LLzom-2TITa4gasV7_fCCA   \n",
       "1      a5JHzBrWxRd_OmIvV7znDA   \n",
       "2      X-o--dwf0HuFMittYi4wCA   \n",
       "3      INGNbsyo-MouZZzcxnCSGQ   \n",
       "4      k7VatXVLism-cTDJE8TTUw   \n",
       "...                       ...   \n",
       "11681  IlU-MQzMKc7jAHWwK5VFGQ   \n",
       "11682  Qt3BsRvQuJccDQfFWM1XPw   \n",
       "11683  3CQQ8Im_UX6QqDECuXYK8A   \n",
       "11684  ery1nBM7zKweFLBe-bT5ag   \n",
       "11685  N5_SaVzmwkZUslgWDGGsQQ   \n",
       "\n",
       "                                                    text  sentiment  \n",
       "0      Great experience purchasing a washer and dryer...          1  \n",
       "1      Went here based on the high ratings and raves ...         -1  \n",
       "2      oh Millers, how i wanted to like you.  You are...         -1  \n",
       "3      This place gets two stars from me only because...         -1  \n",
       "4      This place was awesome. Clean, beautiful and t...          1  \n",
       "...                                                  ...        ...  \n",
       "11681  To be fair, I tried them in their first week. ...          0  \n",
       "11682  Awful place. It's dirty. Had two birthday part...         -1  \n",
       "11683  A truly vegetarian delight!  I took a Jewish f...          1  \n",
       "11684  I have a 2011 Toyota Sienna Limited. During th...         -1  \n",
       "11685  I'm a single father raising my 17 year old son...         -1  \n",
       "\n",
       "[11686 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load review sentiment data\n",
    "\n",
    "review_df = pd.read_csv('data/review_sentiment.csv')\n",
    "\n",
    "review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = {\n",
    "    0: 'bag_of_words',\n",
    "    1: 'one_hot',\n",
    "    2: 'n_grams',\n",
    "    3: 'tf_idf',\n",
    "    4: 'word2vec',\n",
    "    5: 'combined_bow_negation',\n",
    "    6: 'lsa_topic_matrix',\n",
    "    7: 'lda_topic_matrix'\n",
    "}\n",
    "# Load all feature sets\n",
    "features = {}\n",
    "for key, feature_name in feature_set.items():\n",
    "    if key == 4 or key == 6 or key == 7:\n",
    "        features[key] = np.load('features/' + feature_name + '.npy')\n",
    "    else:\n",
    "        features[key] = sparse.load_npz('features/' + feature_name + '.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11686,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target labels\n",
    "\n",
    "y = review_df['sentiment'].to_numpy()\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers\n",
    "\n",
    "classifiers = {\n",
    "    'gaussian_nb': GaussianNB(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'svm': SVC(),\n",
    "    'perceptron': Perceptron(tol=1e-3, random_state=0),\n",
    "    'xgb': XGBClassifier(),\n",
    "    'logistic_regression': LogisticRegression(max_iter=1000, random_state=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'gaussian_nb': {},\n",
    "    'decision_tree': {\n",
    "        'max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1.0],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'perceptron': {\n",
    "        'alpha': [0.0001, 0.001],\n",
    "        'penalty': [None, 'l2']\n",
    "    },\n",
    "    'xgb': {\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'n_estimators': [100, 200]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'C': [0.1, 1.0],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec features for grid search\n",
    "word2vec_features = features[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets using word2vec features\n",
    "X_train_w2v, X_test_w2v, y_train, y_test = train_test_split(word2vec_features, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb algorithm expects [0 1 2], not [-1 0 1]\n",
    "\n",
    "# Convert numpy arrays to pandas Series\n",
    "y_train_series = pd.Series(y_train)\n",
    "y_test_series = pd.Series(y_test)\n",
    "\n",
    "# Remap labels: -1 -> 0, 0 -> 1, 1 -> 2\n",
    "y_train_mapped = y_train_series.map({-1: 0, 0: 1, 1: 2})\n",
    "y_test_mapped = y_test_series.map({-1: 0, 0: 1, 1: 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating gaussian_nb...\n",
      "Cross-validating decision_tree...\n",
      "Cross-validating random_forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating svm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating perceptron...\n",
      "Cross-validating xgb...\n",
      "Cross-validating logistic_regression...\n",
      "Classifier: gaussian_nb\n",
      "Accuracy: 0.49775366735049487\n",
      "Precision (Macro): 0.4680201446987492\n",
      "Recall (Macro): 0.4919967844158806\n",
      "F1 Score (Macro): 0.4497374358123659\n",
      "-------------------------\n",
      "Classifier: decision_tree\n",
      "Accuracy: 0.516260289611248\n",
      "Precision (Macro): 0.4132947214720045\n",
      "Recall (Macro): 0.4135580374557807\n",
      "F1 Score (Macro): 0.41329806193191787\n",
      "-------------------------\n",
      "Classifier: random_forest\n",
      "Accuracy: 0.6485884241336984\n",
      "Precision (Macro): 0.5338883666988832\n",
      "Recall (Macro): 0.4741140066981724\n",
      "F1 Score (Macro): 0.4505995440003511\n",
      "-------------------------\n",
      "Classifier: svm\n",
      "Accuracy: 0.7647618475377893\n",
      "Precision (Macro): 0.5761224022841114\n",
      "Recall (Macro): 0.568636253886672\n",
      "F1 Score (Macro): 0.5377405854249574\n",
      "-------------------------\n",
      "Classifier: perceptron\n",
      "Accuracy: 0.7139490075907788\n",
      "Precision (Macro): 0.577117976432924\n",
      "Recall (Macro): 0.5707040786038377\n",
      "F1 Score (Macro): 0.570952068470641\n",
      "-------------------------\n",
      "Classifier: xgb\n",
      "Accuracy: 0.7220800393701914\n",
      "Precision (Macro): 0.5874335724255854\n",
      "Recall (Macro): 0.5367930264956919\n",
      "F1 Score (Macro): 0.512933623102215\n",
      "-------------------------\n",
      "Classifier: logistic_regression\n",
      "Accuracy: 0.7230424345427651\n",
      "Precision (Macro): 0.5898280011771961\n",
      "Recall (Macro): 0.5838809138862653\n",
      "F1 Score (Macro): 0.5846841865791694\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "cv_scores = {}\n",
    "\n",
    "# Cross-validation for each classifier\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"Cross-validating {clf_name}...\")\n",
    "    if clf_name == 'xgb':\n",
    "        cv_results = cross_validate(clf, X_train_w2v, y_train_mapped, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], cv=5)\n",
    "\n",
    "    else:\n",
    "        cv_results = cross_validate(clf, X_train_w2v, y_train, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], cv=5)\n",
    "\n",
    "    cv_scores[clf_name] = cv_results\n",
    "\n",
    "for clf_name, cv_result in cv_scores.items():\n",
    "    print(f\"Classifier: {clf_name}\")\n",
    "    print(\"Accuracy:\", cv_result['test_accuracy'].mean())\n",
    "    print(\"Precision (Macro):\", cv_result['test_precision_macro'].mean())\n",
    "    print(\"Recall (Macro):\", cv_result['test_recall_macro'].mean())\n",
    "    print(\"F1 Score (Macro):\", cv_result['test_f1_macro'].mean())\n",
    "    print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_estimators = {}\n",
    "\n",
    "# # Apply grid search to each classifier using word2vec features\n",
    "# for clf_name, clf in classifiers.items():\n",
    "#     print(f\"Grid search for {clf_name}\")\n",
    "#     param_grid = param_grids[clf_name]\n",
    "#     if clf_name == 'xgb':\n",
    "#         grid_search = GridSearchCV(clf, param_grid, scoring='accuracy', cv=3)\n",
    "#         grid_search.fit(X_train_w2v, y_train_mapped)\n",
    "#     else:\n",
    "#         grid_search = GridSearchCV(clf, param_grid, scoring='accuracy', cv=3)\n",
    "#         grid_search.fit(X_train_w2v, y_train)\n",
    "#     best_estimators[clf_name] = grid_search.best_estimator_\n",
    "#     print(f\"Best parameters for {clf_name}: {grid_search.best_params_}\")\n",
    "#     print(f\"Best accuracy for {clf_name}: {grid_search.best_score_}\")\n",
    "#     print(\"---------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifiers = {\n",
    "    'xgb': XGBClassifier(learning_rate=0.1, max_depth=5, n_estimators=200),\n",
    "    'gaussian_nb': GaussianNB(),\n",
    "    'decision_tree': DecisionTreeClassifier(max_depth=10),\n",
    "    'random_forest': RandomForestClassifier(max_depth=10, n_estimators=200),\n",
    "    'svm': SVC(C=1.0, kernel='rbf'),\n",
    "    'perceptron': Perceptron(tol=1e-3, random_state=0, alpha=0.0001, penalty=None),\n",
    "    'logistic_regression': LogisticRegression(C=1.0, penalty='l2', solver='liblinear', max_iter=1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bag_of_words features:\n",
      "Training xgb with bag_of_words features...\n",
      "Accuracy of xgb with bag_of_words features: 0.7745936698032506\n",
      "Precision of xgb with bag_of_words features: 0.6817569352091645\n",
      "Recall of xgb with bag_of_words features: 0.6981402374889135\n",
      "F1 Score of xgb with bag_of_words features: 0.6863947300028698\n",
      "--------------------------------------\n",
      "Training gaussian_nb with bag_of_words features...\n",
      "Accuracy of gaussian_nb with bag_of_words features: 0.5209580838323353\n",
      "Precision of gaussian_nb with bag_of_words features: 0.4774245053273527\n",
      "Recall of gaussian_nb with bag_of_words features: 0.45666741721028226\n",
      "F1 Score of gaussian_nb with bag_of_words features: 0.4544013347189526\n",
      "--------------------------------------\n",
      "Training decision_tree with bag_of_words features...\n",
      "Accuracy of decision_tree with bag_of_words features: 0.6407185628742516\n",
      "Precision of decision_tree with bag_of_words features: 0.5690719819342406\n",
      "Recall of decision_tree with bag_of_words features: 0.5810768926648633\n",
      "F1 Score of decision_tree with bag_of_words features: 0.5640968116771952\n",
      "--------------------------------------\n",
      "Training random_forest with bag_of_words features...\n",
      "Accuracy of random_forest with bag_of_words features: 0.7540633019674936\n",
      "Precision of random_forest with bag_of_words features: 0.6629265058765236\n",
      "Recall of random_forest with bag_of_words features: 0.6687529664972129\n",
      "F1 Score of random_forest with bag_of_words features: 0.6649433064704428\n",
      "--------------------------------------\n",
      "Training svm with bag_of_words features...\n",
      "Accuracy of svm with bag_of_words features: 0.7981180496150556\n",
      "Precision of svm with bag_of_words features: 0.6956335072474099\n",
      "Recall of svm with bag_of_words features: 0.6928801952681924\n",
      "F1 Score of svm with bag_of_words features: 0.6933175405969195\n",
      "--------------------------------------\n",
      "Training perceptron with bag_of_words features...\n",
      "Accuracy of perceptron with bag_of_words features: 0.7989734816082121\n",
      "Precision of perceptron with bag_of_words features: 0.6754603533366712\n",
      "Recall of perceptron with bag_of_words features: 0.6597579655405341\n",
      "F1 Score of perceptron with bag_of_words features: 0.6641549766701939\n",
      "--------------------------------------\n",
      "Training logistic_regression with bag_of_words features...\n",
      "Accuracy of logistic_regression with bag_of_words features: 0.7964071856287425\n",
      "Precision of logistic_regression with bag_of_words features: 0.68309055519834\n",
      "Recall of logistic_regression with bag_of_words features: 0.6812041679484048\n",
      "F1 Score of logistic_regression with bag_of_words features: 0.6820975554224246\n",
      "--------------------------------------\n",
      "--------------------------------------------------------\n",
      "Using one_hot features:\n",
      "Training xgb with one_hot features...\n",
      "Accuracy of xgb with one_hot features: 0.7733105218135158\n",
      "Precision of xgb with one_hot features: 0.680928275893951\n",
      "Recall of xgb with one_hot features: 0.6980751199019855\n",
      "F1 Score of xgb with one_hot features: 0.6852363858521849\n",
      "--------------------------------------\n",
      "Training gaussian_nb with one_hot features...\n",
      "Accuracy of gaussian_nb with one_hot features: 0.5162532078699743\n",
      "Precision of gaussian_nb with one_hot features: 0.4736358076061387\n",
      "Recall of gaussian_nb with one_hot features: 0.45360604494170603\n",
      "F1 Score of gaussian_nb with one_hot features: 0.4506693020847945\n",
      "--------------------------------------\n",
      "Training decision_tree with one_hot features...\n",
      "Accuracy of decision_tree with one_hot features: 0.6407185628742516\n",
      "Precision of decision_tree with one_hot features: 0.5680534719252962\n",
      "Recall of decision_tree with one_hot features: 0.5791445071306929\n",
      "F1 Score of decision_tree with one_hot features: 0.5628075629405642\n",
      "--------------------------------------\n",
      "Training random_forest with one_hot features...\n",
      "Accuracy of random_forest with one_hot features: 0.7532078699743371\n",
      "Precision of random_forest with one_hot features: 0.6640467570733192\n",
      "Recall of random_forest with one_hot features: 0.6705333405216384\n",
      "F1 Score of random_forest with one_hot features: 0.6660294042999545\n",
      "--------------------------------------\n",
      "Training svm with one_hot features...\n",
      "Accuracy of svm with one_hot features: 0.8169375534644996\n",
      "Precision of svm with one_hot features: 0.7074492680182723\n",
      "Recall of svm with one_hot features: 0.6832178447558673\n",
      "F1 Score of svm with one_hot features: 0.6894795109146578\n",
      "--------------------------------------\n",
      "Training perceptron with one_hot features...\n",
      "Accuracy of perceptron with one_hot features: 0.7964071856287425\n",
      "Precision of perceptron with one_hot features: 0.6777516220101162\n",
      "Recall of perceptron with one_hot features: 0.6703970798567837\n",
      "F1 Score of perceptron with one_hot features: 0.6731495881360092\n",
      "--------------------------------------\n",
      "Training logistic_regression with one_hot features...\n",
      "Accuracy of logistic_regression with one_hot features: 0.8058169375534645\n",
      "Precision of logistic_regression with one_hot features: 0.697440356936263\n",
      "Recall of logistic_regression with one_hot features: 0.6962505589435729\n",
      "F1 Score of logistic_regression with one_hot features: 0.6967849133340023\n",
      "--------------------------------------\n",
      "--------------------------------------------------------\n",
      "Using n_grams features:\n",
      "Training xgb with n_grams features...\n",
      "Accuracy of xgb with n_grams features: 0.7754491017964071\n",
      "Precision of xgb with n_grams features: 0.6859477209154524\n",
      "Recall of xgb with n_grams features: 0.7052835099225038\n",
      "F1 Score of xgb with n_grams features: 0.6912876014637724\n",
      "--------------------------------------\n",
      "Training gaussian_nb with n_grams features...\n",
      "Accuracy of gaussian_nb with n_grams features: 0.7108639863130881\n",
      "Precision of gaussian_nb with n_grams features: 0.5810508967115792\n",
      "Recall of gaussian_nb with n_grams features: 0.5808524695527711\n",
      "F1 Score of gaussian_nb with n_grams features: 0.5808064819306095\n",
      "--------------------------------------\n",
      "Training decision_tree with n_grams features...\n",
      "Accuracy of decision_tree with n_grams features: 0.6398631308810949\n",
      "Precision of decision_tree with n_grams features: 0.5675897584713298\n",
      "Recall of decision_tree with n_grams features: 0.579177065924157\n",
      "F1 Score of decision_tree with n_grams features: 0.5627909143988977\n",
      "--------------------------------------\n",
      "Training random_forest with n_grams features...\n",
      "Accuracy of random_forest with n_grams features: 0.7536355859709153\n",
      "Precision of random_forest with n_grams features: 0.6684083159014369\n",
      "Recall of random_forest with n_grams features: 0.6765204586561464\n",
      "F1 Score of random_forest with n_grams features: 0.6710975818103702\n",
      "--------------------------------------\n",
      "Training svm with n_grams features...\n",
      "Accuracy of svm with n_grams features: 0.806672369546621\n",
      "Precision of svm with n_grams features: 0.7031292821190203\n",
      "Recall of svm with n_grams features: 0.6916261531679812\n",
      "F1 Score of svm with n_grams features: 0.6950648034480235\n",
      "--------------------------------------\n",
      "Training perceptron with n_grams features...\n",
      "Accuracy of perceptron with n_grams features: 0.8152266894781864\n",
      "Precision of perceptron with n_grams features: 0.7013990792801423\n",
      "Recall of perceptron with n_grams features: 0.6853020303790457\n",
      "F1 Score of perceptron with n_grams features: 0.6909962403503878\n",
      "--------------------------------------\n",
      "Training logistic_regression with n_grams features...\n",
      "Accuracy of logistic_regression with n_grams features: 0.8156544054747648\n",
      "Precision of logistic_regression with n_grams features: 0.7048217605360464\n",
      "Recall of logistic_regression with n_grams features: 0.6941877268148091\n",
      "F1 Score of logistic_regression with n_grams features: 0.6985208481748005\n",
      "--------------------------------------\n",
      "--------------------------------------------------------\n",
      "Using tf_idf features:\n",
      "Training xgb with tf_idf features...\n",
      "Accuracy of xgb with tf_idf features: 0.7810094097519247\n",
      "Precision of xgb with tf_idf features: 0.6878663376146598\n",
      "Recall of xgb with tf_idf features: 0.7048600341867332\n",
      "F1 Score of xgb with tf_idf features: 0.6933187322973163\n",
      "--------------------------------------\n",
      "Training gaussian_nb with tf_idf features...\n",
      "Accuracy of gaussian_nb with tf_idf features: 0.5196749358426005\n",
      "Precision of gaussian_nb with tf_idf features: 0.46827876236863286\n",
      "Recall of gaussian_nb with tf_idf features: 0.45794883829590655\n",
      "F1 Score of gaussian_nb with tf_idf features: 0.45313005968798814\n",
      "--------------------------------------\n",
      "Training decision_tree with tf_idf features...\n",
      "Accuracy of decision_tree with tf_idf features: 0.5958083832335329\n",
      "Precision of decision_tree with tf_idf features: 0.5508023392364495\n",
      "Recall of decision_tree with tf_idf features: 0.5548443361970272\n",
      "F1 Score of decision_tree with tf_idf features: 0.5286443266588966\n",
      "--------------------------------------\n",
      "Training random_forest with tf_idf features...\n",
      "Accuracy of random_forest with tf_idf features: 0.7352437981180496\n",
      "Precision of random_forest with tf_idf features: 0.6435221424947453\n",
      "Recall of random_forest with tf_idf features: 0.6509004937731308\n",
      "F1 Score of random_forest with tf_idf features: 0.6462535994455213\n",
      "--------------------------------------\n",
      "Training svm with tf_idf features...\n",
      "Accuracy of svm with tf_idf features: 0.8289136013686912\n",
      "Precision of svm with tf_idf features: 0.7189988867158833\n",
      "Recall of svm with tf_idf features: 0.6660486838530584\n",
      "F1 Score of svm with tf_idf features: 0.6699096549516956\n",
      "--------------------------------------\n",
      "Training perceptron with tf_idf features...\n",
      "Accuracy of perceptron with tf_idf features: 0.7917023096663816\n",
      "Precision of perceptron with tf_idf features: 0.6760143867504187\n",
      "Recall of perceptron with tf_idf features: 0.6420566586432497\n",
      "F1 Score of perceptron with tf_idf features: 0.6507807057807057\n",
      "--------------------------------------\n",
      "Training logistic_regression with tf_idf features...\n",
      "Accuracy of logistic_regression with tf_idf features: 0.7998289136013687\n",
      "Precision of logistic_regression with tf_idf features: 0.7015378093273311\n",
      "Recall of logistic_regression with tf_idf features: 0.7149829647741339\n",
      "F1 Score of logistic_regression with tf_idf features: 0.7057960204485703\n",
      "--------------------------------------\n",
      "--------------------------------------------------------\n",
      "Using word2vec features:\n",
      "Training xgb with word2vec features...\n",
      "Accuracy of xgb with word2vec features: 0.7177074422583405\n",
      "Precision of xgb with word2vec features: 0.5946123654125773\n",
      "Recall of xgb with word2vec features: 0.5510356973343017\n",
      "F1 Score of xgb with word2vec features: 0.5408088849164645\n",
      "--------------------------------------\n",
      "Training gaussian_nb with word2vec features...\n",
      "Accuracy of gaussian_nb with word2vec features: 0.48417450812660395\n",
      "Precision of gaussian_nb with word2vec features: 0.46228295389778024\n",
      "Recall of gaussian_nb with word2vec features: 0.4797712639048776\n",
      "F1 Score of gaussian_nb with word2vec features: 0.4410054145078117\n",
      "--------------------------------------\n",
      "Training decision_tree with word2vec features...\n",
      "Accuracy of decision_tree with word2vec features: 0.47818648417450815\n",
      "Precision of decision_tree with word2vec features: 0.42208018690691557\n",
      "Recall of decision_tree with word2vec features: 0.42842594090156144\n",
      "F1 Score of decision_tree with word2vec features: 0.41811786855312033\n",
      "--------------------------------------\n",
      "Training random_forest with word2vec features...\n",
      "Accuracy of random_forest with word2vec features: 0.6146278870829769\n",
      "Precision of random_forest with word2vec features: 0.5962559557631617\n",
      "Recall of random_forest with word2vec features: 0.47105079277490775\n",
      "F1 Score of random_forest with word2vec features: 0.4453313825603993\n",
      "--------------------------------------\n",
      "Training svm with word2vec features...\n",
      "Accuracy of svm with word2vec features: 0.7536355859709153\n",
      "Precision of svm with word2vec features: 0.6299436488746398\n",
      "Recall of svm with word2vec features: 0.599377471640551\n",
      "F1 Score of svm with word2vec features: 0.5980108301475296\n",
      "--------------------------------------\n",
      "Training perceptron with word2vec features...\n",
      "Accuracy of perceptron with word2vec features: 0.6993156544054747\n",
      "Precision of perceptron with word2vec features: 0.5790176792749228\n",
      "Recall of perceptron with word2vec features: 0.5740022262603582\n",
      "F1 Score of perceptron with word2vec features: 0.5755593555652291\n",
      "--------------------------------------\n",
      "Training logistic_regression with word2vec features...\n",
      "Accuracy of logistic_regression with word2vec features: 0.7134302822925578\n",
      "Precision of logistic_regression with word2vec features: 0.6037676193582869\n",
      "Recall of logistic_regression with word2vec features: 0.6010825798826827\n",
      "F1 Score of logistic_regression with word2vec features: 0.602096668863942\n",
      "--------------------------------------\n",
      "--------------------------------------------------------\n",
      "Using combined_bow_negation features:\n",
      "Training xgb with combined_bow_negation features...\n",
      "Accuracy of xgb with combined_bow_negation features: 0.7917023096663816\n",
      "Precision of xgb with combined_bow_negation features: 0.6907089764934318\n",
      "Recall of xgb with combined_bow_negation features: 0.6981126470827639\n",
      "F1 Score of xgb with combined_bow_negation features: 0.6931842105716978\n",
      "--------------------------------------\n",
      "Training gaussian_nb with combined_bow_negation features...\n",
      "Accuracy of gaussian_nb with combined_bow_negation features: 0.5226689478186484\n",
      "Precision of gaussian_nb with combined_bow_negation features: 0.47868781368064406\n",
      "Recall of gaussian_nb with combined_bow_negation features: 0.45825782970271073\n",
      "F1 Score of gaussian_nb with combined_bow_negation features: 0.4559843527790914\n",
      "--------------------------------------\n",
      "Training decision_tree with combined_bow_negation features...\n",
      "Accuracy of decision_tree with combined_bow_negation features: 0.6830624465355004\n",
      "Precision of decision_tree with combined_bow_negation features: 0.6145409794774518\n",
      "Recall of decision_tree with combined_bow_negation features: 0.6161349963688488\n",
      "F1 Score of decision_tree with combined_bow_negation features: 0.6028475264208485\n",
      "--------------------------------------\n",
      "Training random_forest with combined_bow_negation features...\n",
      "Accuracy of random_forest with combined_bow_negation features: 0.787852865697177\n",
      "Precision of random_forest with combined_bow_negation features: 0.6860421402577099\n",
      "Recall of random_forest with combined_bow_negation features: 0.685498017402041\n",
      "F1 Score of random_forest with combined_bow_negation features: 0.6857677473658287\n",
      "--------------------------------------\n",
      "Training svm with combined_bow_negation features...\n",
      "Accuracy of svm with combined_bow_negation features: 0.7972626176218991\n",
      "Precision of svm with combined_bow_negation features: 0.695369110026517\n",
      "Recall of svm with combined_bow_negation features: 0.7017874566191077\n",
      "F1 Score of svm with combined_bow_negation features: 0.6981980796657425\n",
      "--------------------------------------\n",
      "Training perceptron with combined_bow_negation features...\n",
      "Accuracy of perceptron with combined_bow_negation features: 0.795551753635586\n",
      "Precision of perceptron with combined_bow_negation features: 0.6526588347426459\n",
      "Recall of perceptron with combined_bow_negation features: 0.6222444213995841\n",
      "F1 Score of perceptron with combined_bow_negation features: 0.6234856661158068\n",
      "--------------------------------------\n",
      "Training logistic_regression with combined_bow_negation features...\n",
      "Accuracy of logistic_regression with combined_bow_negation features: 0.8062446535500428\n",
      "Precision of logistic_regression with combined_bow_negation features: 0.6934777582163566\n",
      "Recall of logistic_regression with combined_bow_negation features: 0.6903665084182452\n",
      "F1 Score of logistic_regression with combined_bow_negation features: 0.6918183577827691\n",
      "--------------------------------------\n",
      "--------------------------------------------------------\n",
      "Using lsa_topic_matrix features:\n",
      "Training xgb with lsa_topic_matrix features...\n",
      "Accuracy of xgb with lsa_topic_matrix features: 0.6206159110350727\n",
      "Precision of xgb with lsa_topic_matrix features: 0.5399498915358181\n",
      "Recall of xgb with lsa_topic_matrix features: 0.5455608516450119\n",
      "F1 Score of xgb with lsa_topic_matrix features: 0.5379602416612673\n",
      "--------------------------------------\n",
      "Training gaussian_nb with lsa_topic_matrix features...\n",
      "Accuracy of gaussian_nb with lsa_topic_matrix features: 0.5508982035928144\n",
      "Precision of gaussian_nb with lsa_topic_matrix features: 0.49013004526703857\n",
      "Recall of gaussian_nb with lsa_topic_matrix features: 0.4551236229903138\n",
      "F1 Score of gaussian_nb with lsa_topic_matrix features: 0.4467327378687726\n",
      "--------------------------------------\n",
      "Training decision_tree with lsa_topic_matrix features...\n",
      "Accuracy of decision_tree with lsa_topic_matrix features: 0.5449101796407185\n",
      "Precision of decision_tree with lsa_topic_matrix features: 0.492751636149441\n",
      "Recall of decision_tree with lsa_topic_matrix features: 0.4946635291829328\n",
      "F1 Score of decision_tree with lsa_topic_matrix features: 0.48183606685727254\n",
      "--------------------------------------\n",
      "Training random_forest with lsa_topic_matrix features...\n",
      "Accuracy of random_forest with lsa_topic_matrix features: 0.6313088109495295\n",
      "Precision of random_forest with lsa_topic_matrix features: 0.5542762192672411\n",
      "Recall of random_forest with lsa_topic_matrix features: 0.5644881873948843\n",
      "F1 Score of random_forest with lsa_topic_matrix features: 0.5540506663880814\n",
      "--------------------------------------\n",
      "Training svm with lsa_topic_matrix features...\n",
      "Accuracy of svm with lsa_topic_matrix features: 0.5915312232677502\n",
      "Precision of svm with lsa_topic_matrix features: 0.5647352331392478\n",
      "Recall of svm with lsa_topic_matrix features: 0.5795464179513119\n",
      "F1 Score of svm with lsa_topic_matrix features: 0.5442042013017087\n",
      "--------------------------------------\n",
      "Training perceptron with lsa_topic_matrix features...\n",
      "Accuracy of perceptron with lsa_topic_matrix features: 0.5513259195893926\n",
      "Precision of perceptron with lsa_topic_matrix features: 0.5115122480125657\n",
      "Recall of perceptron with lsa_topic_matrix features: 0.49937694308870895\n",
      "F1 Score of perceptron with lsa_topic_matrix features: 0.4704183632257277\n",
      "--------------------------------------\n",
      "Training logistic_regression with lsa_topic_matrix features...\n",
      "Accuracy of logistic_regression with lsa_topic_matrix features: 0.6039349871685201\n",
      "Precision of logistic_regression with lsa_topic_matrix features: 0.5414667451390978\n",
      "Recall of logistic_regression with lsa_topic_matrix features: 0.5480253831736578\n",
      "F1 Score of logistic_regression with lsa_topic_matrix features: 0.5348227695141202\n",
      "--------------------------------------\n",
      "--------------------------------------------------------\n",
      "Using lda_topic_matrix features:\n",
      "Training xgb with lda_topic_matrix features...\n",
      "Accuracy of xgb with lda_topic_matrix features: 0.5915312232677502\n",
      "Precision of xgb with lda_topic_matrix features: 0.5179899822560761\n",
      "Recall of xgb with lda_topic_matrix features: 0.5190510168808887\n",
      "F1 Score of xgb with lda_topic_matrix features: 0.5125815859538356\n",
      "--------------------------------------\n",
      "Training gaussian_nb with lda_topic_matrix features...\n",
      "Accuracy of gaussian_nb with lda_topic_matrix features: 0.4007698887938409\n",
      "Precision of gaussian_nb with lda_topic_matrix features: 0.48147858828732853\n",
      "Recall of gaussian_nb with lda_topic_matrix features: 0.4869763769039759\n",
      "F1 Score of gaussian_nb with lda_topic_matrix features: 0.3839451072355382\n",
      "--------------------------------------\n",
      "Training decision_tree with lda_topic_matrix features...\n",
      "Accuracy of decision_tree with lda_topic_matrix features: 0.49615055603079555\n",
      "Precision of decision_tree with lda_topic_matrix features: 0.47876957707499557\n",
      "Recall of decision_tree with lda_topic_matrix features: 0.4736258973488896\n",
      "F1 Score of decision_tree with lda_topic_matrix features: 0.4522628711387673\n",
      "--------------------------------------\n",
      "Training random_forest with lda_topic_matrix features...\n",
      "Accuracy of random_forest with lda_topic_matrix features: 0.6077844311377245\n",
      "Precision of random_forest with lda_topic_matrix features: 0.5276098950402998\n",
      "Recall of random_forest with lda_topic_matrix features: 0.5335111381729655\n",
      "F1 Score of random_forest with lda_topic_matrix features: 0.5262248481975563\n",
      "--------------------------------------\n",
      "Training svm with lda_topic_matrix features...\n",
      "Accuracy of svm with lda_topic_matrix features: 0.6077844311377245\n",
      "Precision of svm with lda_topic_matrix features: 0.5333861797692422\n",
      "Recall of svm with lda_topic_matrix features: 0.5435911503507999\n",
      "F1 Score of svm with lda_topic_matrix features: 0.5315119838777507\n",
      "--------------------------------------\n",
      "Training perceptron with lda_topic_matrix features...\n",
      "Accuracy of perceptron with lda_topic_matrix features: 0.3887938408896493\n",
      "Precision of perceptron with lda_topic_matrix features: 0.512426087465605\n",
      "Recall of perceptron with lda_topic_matrix features: 0.354383016149373\n",
      "F1 Score of perceptron with lda_topic_matrix features: 0.22462829319883826\n",
      "--------------------------------------\n",
      "Training logistic_regression with lda_topic_matrix features...\n",
      "Accuracy of logistic_regression with lda_topic_matrix features: 0.47604790419161674\n",
      "Precision of logistic_regression with lda_topic_matrix features: 0.5037435413747747\n",
      "Recall of logistic_regression with lda_topic_matrix features: 0.5151411074852456\n",
      "F1 Score of logistic_regression with lda_topic_matrix features: 0.44473281210260235\n",
      "--------------------------------------\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through each feature set\n",
    "for feature_key, feature_name in feature_set.items():\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features[feature_key], y, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Oversample to balance the classes\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # change -1 to 2 because of xbg\n",
    "    y_train_resampled_series = pd.Series(y_train_resampled)\n",
    "    y_test_series = pd.Series(y_test)\n",
    "    y_train_resampled_mapped = y_train_resampled_series.map({-1: 0, 0: 1, 1: 2})\n",
    "    y_test_mapped = y_test_series.map({-1: 0, 0: 1, 1: 2})\n",
    "\n",
    "\n",
    "    print(f\"Using {feature_name} features:\")\n",
    "    \n",
    "    # Loop through each classifier\n",
    "    for clf_name, clf in best_classifiers.items():\n",
    "        print(f\"Training {clf_name} with {feature_name} features...\")   \n",
    "        \n",
    "        # Convert sparse to dense if necessary\n",
    "        if (feature_key != 4 and feature_key != 6 and feature_key != 7) and clf_name in ['gaussian_nb', 'multinomial_nb', 'perceptron']:\n",
    "            X_train_dense = X_train_resampled.toarray()\n",
    "            X_test_dense = X_test.toarray()\n",
    "            clf.fit(X_train_dense, y_train_resampled)\n",
    "            y_pred = clf.predict(X_test_dense)\n",
    "        else:\n",
    "            if clf_name == 'xgb':\n",
    "                clf.fit(X_train_resampled, y_train_resampled_mapped)\n",
    "            else:\n",
    "                clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        if clf_name == 'xgb':\n",
    "            acc = accuracy_score(y_test_mapped, y_pred)\n",
    "            precision = precision_score(y_test_mapped, y_pred, average='macro')\n",
    "            recall = recall_score(y_test_mapped, y_pred, average='macro')\n",
    "            f1 = f1_score(y_test_mapped, y_pred, average='macro')\n",
    "        else:\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='macro')\n",
    "            recall = recall_score(y_test, y_pred, average='macro')\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        print(f\"Accuracy of {clf_name} with {feature_name} features: {acc}\")\n",
    "        print(f\"Precision of {clf_name} with {feature_name} features: {precision}\")\n",
    "        print(f\"Recall of {clf_name} with {feature_name} features: {recall}\")\n",
    "        print(f\"F1 Score of {clf_name} with {feature_name} features: {f1}\")\n",
    "            \n",
    "    # Append the results to the list\n",
    "        results.append({\n",
    "            'feature_set': feature_name,\n",
    "            'classifier': clf_name,\n",
    "            'accuracy': acc,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        })\n",
    "        print(\"--------------------------------------\")\n",
    "\n",
    "    print(\"-------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('classification_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
