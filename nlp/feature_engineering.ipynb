{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NvusujU9_5pIUbn9SZ6hMA</td>\n",
       "      <td>Stopped by to munch a burger during today's Se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vHOeBa7aMA_na4rfS2Db5A</td>\n",
       "      <td>Yelp doesn't allow to leave 0 star review, so ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hG9RTxxivb0ZXzEk4JXTXA</td>\n",
       "      <td>I find it hard to believe there are so many pe...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zIVkwgahZjOneChZFUYY4g</td>\n",
       "      <td>Love this place! Almost all of their menu item...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DLczAuvMAlAnY5EeDGhTVg</td>\n",
       "      <td>Excellent customer service. I wish I could ren...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63446</th>\n",
       "      <td>OgoBp7fbXnLSKvsQb4O_tw</td>\n",
       "      <td>I really loved the food and service. I mean, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63447</th>\n",
       "      <td>Q7e8EtZMmdknDrQE7huMoQ</td>\n",
       "      <td>Their Grove location was the bomb. Delicious f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63448</th>\n",
       "      <td>zzMW6zbsFaQMjoGu2bGVdA</td>\n",
       "      <td>A nice ean BBQ joint right across from some ne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63449</th>\n",
       "      <td>scgoa60EvhW2Mz7JMqLYGw</td>\n",
       "      <td>The perfect Hookah bar. I'm not sure what they...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63450</th>\n",
       "      <td>M7NAWZ9P99ucR66LrxrUOQ</td>\n",
       "      <td>Besides it being super busy often, usually, I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63451 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    review_id  \\\n",
       "0      NvusujU9_5pIUbn9SZ6hMA   \n",
       "1      vHOeBa7aMA_na4rfS2Db5A   \n",
       "2      hG9RTxxivb0ZXzEk4JXTXA   \n",
       "3      zIVkwgahZjOneChZFUYY4g   \n",
       "4      DLczAuvMAlAnY5EeDGhTVg   \n",
       "...                       ...   \n",
       "63446  OgoBp7fbXnLSKvsQb4O_tw   \n",
       "63447  Q7e8EtZMmdknDrQE7huMoQ   \n",
       "63448  zzMW6zbsFaQMjoGu2bGVdA   \n",
       "63449  scgoa60EvhW2Mz7JMqLYGw   \n",
       "63450  M7NAWZ9P99ucR66LrxrUOQ   \n",
       "\n",
       "                                                    text  sentiment  \n",
       "0      Stopped by to munch a burger during today's Se...          1  \n",
       "1      Yelp doesn't allow to leave 0 star review, so ...         -1  \n",
       "2      I find it hard to believe there are so many pe...         -1  \n",
       "3      Love this place! Almost all of their menu item...          1  \n",
       "4      Excellent customer service. I wish I could ren...          1  \n",
       "...                                                  ...        ...  \n",
       "63446  I really loved the food and service. I mean, t...          1  \n",
       "63447  Their Grove location was the bomb. Delicious f...          1  \n",
       "63448  A nice ean BBQ joint right across from some ne...          1  \n",
       "63449  The perfect Hookah bar. I'm not sure what they...          1  \n",
       "63450  Besides it being super busy often, usually, I ...          0  \n",
       "\n",
       "[63451 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load review sentiment data\n",
    "\n",
    "review_df = pd.read_csv('data/review_sentiment.csv')\n",
    "\n",
    "review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus data\n",
    "\n",
    "corpus = open('data/corpus.txt', 'r').read()\n",
    "corpus = corpus.split('\\n')\n",
    "corpus = corpus[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "stop munch burger today seahawk saint game place unsurprisingli pack good reason burger order fantast sat right next door get chanc get six feet place soon got meal place start get busier busier work way world shortest peopl maze get guess mean first person hear place go back might go say lunch tuesday less busi\n",
      "--------------------------------------------------\n",
      "yelp allow leav star review see one star wife move ny south california contact differ move compani sent initi email unit van line soon got email back virtual survey confirm ladi virtual survey meticul profession screen whole apart minut no one contact sent second email almost month ask everyth ok get quot need mention compani sent quot less hour sinc no one repli til today call direct phone left messag answer machin no one call back hope everyth ok no one hurt see reason explan avoid unprofession\n",
      "--------------------------------------------------\n",
      "find hard believ mani peopl low standard come daili bread ripoff st loui bread co panera bread mind eaten multipl time daili bread yet enjoy experi food best sandwich basic lack real flavor also tri bbq chicken pizza okay certainli steer anyon away place base food food basic simpl not bad also noth great biggest qualm daili bread servic major worker seem utterli uninterest help line peopl one person regist no sens urgenc help custom line wait good minut put order receiv cup get drink look clean place sit eventu settl tabl top least dirti tabl avail plenti open spot appear though lot trash left behind anyon bu tabl anticip peopl respons clean appear though tabl effect clean also trash floor address well order number call order halfway complet ask sandwich basic ignor girl put two order anoth minut rest order came began eat tri item state okay problem food unabl make lack organ slow speed serv cleanli restaur attempt write daili bread return rather go st loui bread co get level food clean environ time set\n",
      "--------------------------------------------------\n",
      "love place almost menu item made vegan super knowledg pad thai tofu veggi cashew perfect avocado egg roll amaz favorit\n",
      "--------------------------------------------------\n",
      "excel custom servic wish could rent often great experi vers car rental servic rent\n"
     ]
    }
   ],
   "source": [
    "# see contents of corpus\n",
    "\n",
    "for review in corpus[:5]:\n",
    "    print('-' * 50)\n",
    "    print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec vectors\n",
    "\n",
    "wv = KeyedVectors.load(\"embeddings/reviews_wv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(embeddings, text, sequence_len, strategy=None):\n",
    "    '''\n",
    "    Function to convert text to word embeddings\n",
    "    '''\n",
    "    tokens = text.split()\n",
    "    vec = []\n",
    "    n = 0\n",
    "    i = 0\n",
    "    while i < len(tokens) and n < sequence_len:\n",
    "        try:\n",
    "            vec.extend(embeddings.get_vector(tokens[i]))\n",
    "            n += 1\n",
    "        except KeyError:\n",
    "            True\n",
    "        finally:\n",
    "            i += 1\n",
    "    for _ in range(sequence_len - n):\n",
    "        vec.extend(np.zeros(embeddings.vector_size,))\n",
    "    if strategy == 'mean':\n",
    "        vec = np.mean(vec, axis=0)\n",
    "    elif strategy == 'max':\n",
    "        vec = np.max(vec, axis=0)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 63451\n",
      "Minimum number of words: 1\n",
      "Maximum number of words: 495\n",
      "Average number of words: 54.13619958708295\n",
      "Standard deviation of words: 49.504305404044175\n",
      "Mode of words: ModeResult(mode=14, count=1238)\n"
     ]
    }
   ],
   "source": [
    "# corpus statistics\n",
    "\n",
    "lens = [len(c.split()) for c in corpus]\n",
    "\n",
    "print('Number of reviews:', len(corpus))\n",
    "print('Minimum number of words:', np.min(lens))\n",
    "print('Maximum number of words:', np.max(lens))\n",
    "print('Average number of words:', np.mean(lens))\n",
    "print('Standard deviation of words:', np.std(lens))\n",
    "print('Mode of words:', stats.mode(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert corpus into dataset with appended embeddings representation\n",
    "\n",
    "simple_corpus = []\n",
    "for review in review_df['text']:\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review).lower()\n",
    "    simple_corpus.append(review)\n",
    "\n",
    "embeddings_corpus = []\n",
    "word_limit = 50\n",
    "for review in simple_corpus:\n",
    "    embeddings_corpus.append(text_to_vector(wv, review, word_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63451, 31988)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BoW\n",
    "\n",
    "bag_of_words = CountVectorizer()\n",
    "features = bag_of_words.fit_transform(corpus)\n",
    "\n",
    "sparse.save_npz('features/bag_of_words.npz', features)\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63451, 31988)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-hot encoding\n",
    "\n",
    "one_hot = CountVectorizer(binary=True)\n",
    "features = one_hot.fit_transform(corpus)\n",
    "\n",
    "sparse.save_npz('features/one_hot.npz', features)\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63451, 15000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N-grams\n",
    "\n",
    "n_grams = CountVectorizer(ngram_range=(1, 2), max_features=15000)\n",
    "features = n_grams.fit_transform(corpus)\n",
    "\n",
    "sparse.save_npz('features/n_grams.npz', features)\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63451, 31988)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "features = tf_idf.fit_transform(corpus)\n",
    "\n",
    "sparse.save_npz('features/tf_idf.npz', features)\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63451, 7500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec\n",
    "\n",
    "features = np.array(embeddings_corpus)\n",
    "\n",
    "np.save('features/word2vec.npy', features)\n",
    "\n",
    "features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
