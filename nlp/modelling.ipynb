{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from xgboost import XGBClassifier\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLzom-2TITa4gasV7_fCCA</td>\n",
       "      <td>Great experience purchasing a washer and dryer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a5JHzBrWxRd_OmIvV7znDA</td>\n",
       "      <td>Went here based on the high ratings and raves ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X-o--dwf0HuFMittYi4wCA</td>\n",
       "      <td>oh Millers, how i wanted to like you.  You are...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INGNbsyo-MouZZzcxnCSGQ</td>\n",
       "      <td>This place gets two stars from me only because...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>k7VatXVLism-cTDJE8TTUw</td>\n",
       "      <td>This place was awesome. Clean, beautiful and t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11681</th>\n",
       "      <td>IlU-MQzMKc7jAHWwK5VFGQ</td>\n",
       "      <td>To be fair, I tried them in their first week. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11682</th>\n",
       "      <td>Qt3BsRvQuJccDQfFWM1XPw</td>\n",
       "      <td>Awful place. It's dirty. Had two birthday part...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11683</th>\n",
       "      <td>3CQQ8Im_UX6QqDECuXYK8A</td>\n",
       "      <td>A truly vegetarian delight!  I took a Jewish f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11684</th>\n",
       "      <td>ery1nBM7zKweFLBe-bT5ag</td>\n",
       "      <td>I have a 2011 Toyota Sienna Limited. During th...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11685</th>\n",
       "      <td>N5_SaVzmwkZUslgWDGGsQQ</td>\n",
       "      <td>I'm a single father raising my 17 year old son...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11686 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    review_id  \\\n",
       "0      LLzom-2TITa4gasV7_fCCA   \n",
       "1      a5JHzBrWxRd_OmIvV7znDA   \n",
       "2      X-o--dwf0HuFMittYi4wCA   \n",
       "3      INGNbsyo-MouZZzcxnCSGQ   \n",
       "4      k7VatXVLism-cTDJE8TTUw   \n",
       "...                       ...   \n",
       "11681  IlU-MQzMKc7jAHWwK5VFGQ   \n",
       "11682  Qt3BsRvQuJccDQfFWM1XPw   \n",
       "11683  3CQQ8Im_UX6QqDECuXYK8A   \n",
       "11684  ery1nBM7zKweFLBe-bT5ag   \n",
       "11685  N5_SaVzmwkZUslgWDGGsQQ   \n",
       "\n",
       "                                                    text  sentiment  \n",
       "0      Great experience purchasing a washer and dryer...          1  \n",
       "1      Went here based on the high ratings and raves ...         -1  \n",
       "2      oh Millers, how i wanted to like you.  You are...         -1  \n",
       "3      This place gets two stars from me only because...         -1  \n",
       "4      This place was awesome. Clean, beautiful and t...          1  \n",
       "...                                                  ...        ...  \n",
       "11681  To be fair, I tried them in their first week. ...          0  \n",
       "11682  Awful place. It's dirty. Had two birthday part...         -1  \n",
       "11683  A truly vegetarian delight!  I took a Jewish f...          1  \n",
       "11684  I have a 2011 Toyota Sienna Limited. During th...         -1  \n",
       "11685  I'm a single father raising my 17 year old son...         -1  \n",
       "\n",
       "[11686 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load review sentiment data\n",
    "\n",
    "review_df = pd.read_csv('data/review_sentiment.csv')\n",
    "\n",
    "review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = {\n",
    "    0: 'bag_of_words',\n",
    "    1: 'one_hot',\n",
    "    2: 'n_grams',\n",
    "    3: 'tf_idf',\n",
    "    4: 'word2vec',\n",
    "    5: 'combined_bow_negation',\n",
    "    6: 'lsa_topic_matrix',\n",
    "    7: 'lda_topic_matrix'\n",
    "}\n",
    "# Load all feature sets\n",
    "features = {}\n",
    "for key, feature_name in feature_set.items():\n",
    "    if key == 4 or key == 6 or key == 7:\n",
    "        features[key] = np.load('features/' + feature_name + '.npy')\n",
    "    else:\n",
    "        features[key] = sparse.load_npz('features/' + feature_name + '.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11686,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target labels\n",
    "\n",
    "y = review_df['sentiment'].to_numpy()\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers\n",
    "\n",
    "classifiers = {\n",
    "    'gaussian_nb': GaussianNB(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'svm': SVC(),\n",
    "    'perceptron': Perceptron(tol=1e-3, random_state=0),\n",
    "    'xgb': XGBClassifier(),\n",
    "    'logistic_regression': LogisticRegression(max_iter=1000, random_state=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'gaussian_nb': {},\n",
    "    'decision_tree': {\n",
    "        'max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1.0],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'perceptron': {\n",
    "        'alpha': [0.0001, 0.001],\n",
    "        'penalty': [None, 'l2']\n",
    "    },\n",
    "    'xgb': {\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'n_estimators': [100, 200]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'C': [0.1, 1.0],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec features for grid search\n",
    "word2vec_features = features[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets using word2vec features\n",
    "X_train_w2v, X_test_w2v, y_train, y_test = train_test_split(word2vec_features, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# oversampler = RandomOverSampler(random_state=42)\n",
    "# X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_w2v, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb algorithm expects [0 1 2], not [-1 0 1]\n",
    "\n",
    "# Convert numpy arrays to pandas Series\n",
    "y_train_series = pd.Series(y_train)\n",
    "y_test_series = pd.Series(y_test)\n",
    "\n",
    "# Remap labels: -1 -> 0, 0 -> 1, 1 -> 2\n",
    "y_train_mapped = y_train_series.map({-1: 0, 0: 1, 1: 2})\n",
    "y_test_mapped = y_test_series.map({-1: 0, 0: 1, 1: 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for gaussian_nb\n",
      "Best parameters for gaussian_nb: {}\n",
      "Best score for gaussian_nb: 0.5065980589216799\n",
      "---------------------------------------------\n",
      "Grid search for decision_tree\n",
      "Best parameters for decision_tree: {'max_depth': 10}\n",
      "Best score for decision_tree: 0.5306139337130708\n",
      "---------------------------------------------\n",
      "Grid search for random_forest\n",
      "Best parameters for random_forest: {'max_depth': None, 'n_estimators': 200}\n",
      "Best score for random_forest: 0.623274152507785\n",
      "---------------------------------------------\n",
      "Grid search for svm\n",
      "Best parameters for svm: {'C': 1.0, 'kernel': 'rbf'}\n",
      "Best score for svm: 0.7215333767954311\n",
      "---------------------------------------------\n",
      "Grid search for perceptron\n",
      "Best parameters for perceptron: {'alpha': 0.0001, 'penalty': None}\n",
      "Best score for perceptron: 0.7033121484291686\n",
      "---------------------------------------------\n",
      "Grid search for xgb\n",
      "Best parameters for xgb: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "Best score for xgb: 0.6850281456634859\n",
      "---------------------------------------------\n",
      "Grid search for logistic_regression\n",
      "Best parameters for logistic_regression: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best score for logistic_regression: 0.7217286577375768\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_estimators = {}\n",
    "grid_results = []\n",
    "\n",
    "# Apply grid search to each classifier using word2vec features\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"Grid search for {clf_name}\")\n",
    "    param_grid = param_grids[clf_name]\n",
    "    if clf_name == 'xgb':\n",
    "        grid_search = GridSearchCV(clf, param_grid, scoring='f1_weighted', cv=5)\n",
    "        grid_search.fit(X_train_w2v, y_train_mapped)\n",
    "    else:\n",
    "        grid_search = GridSearchCV(clf, param_grid, scoring='f1_weighted', cv=5)\n",
    "        grid_search.fit(X_train_w2v, y_train)\n",
    "    best_estimators[clf_name] = grid_search.best_estimator_\n",
    "    grid_results.append({\n",
    "        'classifier': clf_name,\n",
    "        'best_parameters': grid_search.best_params_,\n",
    "        'best_f1': grid_search.best_score_,\n",
    "    })\n",
    "    print(f\"Best parameters for {clf_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {clf_name}: {grid_search.best_score_}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "grid_results_df = pd.DataFrame(grid_results)\n",
    "grid_results_df.to_csv('grid_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators = {\n",
    "    'multinomial_nb': MultinomialNB(),\n",
    "    'xgb': XGBClassifier(learning_rate=0.1, max_depth=5, n_estimators=200),\n",
    "    'decision_tree': DecisionTreeClassifier(max_depth=10),\n",
    "    'random_forest': RandomForestClassifier(max_depth=None, n_estimators=200),\n",
    "    'svm': SVC(C=1.0, kernel='rbf'),\n",
    "    'perceptron': Perceptron(tol=1e-3, random_state=0, alpha=0.0001, penalty=None),\n",
    "    'logistic_regression': LogisticRegression(C=1.0, penalty='l2', solver='liblinear', max_iter=1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bag_of_words features:\n",
      "Training multinomial_nb with bag_of_words features...\n",
      "Precision of multinomial_nb with bag_of_words features: 0.6607395889655369\n",
      "Recall of multinomial_nb with bag_of_words features: 0.6411506150757784\n",
      "F1 Score of multinomial_nb with bag_of_words features: 0.6438441506749296\n",
      "Accuracy of multinomial_nb with bag_of_words features: 0.7976903336184773\n",
      "--------------------------------------\n",
      "Training xgb with bag_of_words features...\n",
      "Precision of xgb with bag_of_words features: 0.7424430262446066\n",
      "Recall of xgb with bag_of_words features: 0.6458782998812872\n",
      "F1 Score of xgb with bag_of_words features: 0.6495154640549461\n",
      "Accuracy of xgb with bag_of_words features: 0.8122326775021386\n",
      "--------------------------------------\n",
      "Training decision_tree with bag_of_words features...\n",
      "Precision of decision_tree with bag_of_words features: 0.5579153751762319\n",
      "Recall of decision_tree with bag_of_words features: 0.5260026364165876\n",
      "F1 Score of decision_tree with bag_of_words features: 0.5077980589367398\n",
      "Accuracy of decision_tree with bag_of_words features: 0.6852010265183918\n",
      "--------------------------------------\n",
      "Training random_forest with bag_of_words features...\n",
      "Precision of random_forest with bag_of_words features: 0.7890027469036536\n",
      "Recall of random_forest with bag_of_words features: 0.6054178678007275\n",
      "F1 Score of random_forest with bag_of_words features: 0.5755155142509908\n",
      "Accuracy of random_forest with bag_of_words features: 0.8045337895637297\n",
      "--------------------------------------\n",
      "Training svm with bag_of_words features...\n",
      "Precision of svm with bag_of_words features: 0.7055055738433795\n",
      "Recall of svm with bag_of_words features: 0.6126590280354468\n",
      "F1 Score of svm with bag_of_words features: 0.582655648684771\n",
      "Accuracy of svm with bag_of_words features: 0.8083832335329342\n",
      "--------------------------------------\n",
      "Training perceptron with bag_of_words features...\n",
      "Precision of perceptron with bag_of_words features: 0.6650907836585246\n",
      "Recall of perceptron with bag_of_words features: 0.6568539960104908\n",
      "F1 Score of perceptron with bag_of_words features: 0.6598964781766355\n",
      "Accuracy of perceptron with bag_of_words features: 0.7925577416595381\n",
      "--------------------------------------\n",
      "Training logistic_regression with bag_of_words features...\n",
      "Precision of logistic_regression with bag_of_words features: 0.693669262406417\n",
      "Recall of logistic_regression with bag_of_words features: 0.6672158320304531\n",
      "F1 Score of logistic_regression with bag_of_words features: 0.6728720165751235\n",
      "Accuracy of logistic_regression with bag_of_words features: 0.8147989734816082\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using one_hot features:\n",
      "Training multinomial_nb with one_hot features...\n",
      "Precision of multinomial_nb with one_hot features: 0.6507525365275978\n",
      "Recall of multinomial_nb with one_hot features: 0.6279279393560758\n",
      "F1 Score of multinomial_nb with one_hot features: 0.6231323177660472\n",
      "Accuracy of multinomial_nb with one_hot features: 0.8023952095808383\n",
      "--------------------------------------\n",
      "Training xgb with one_hot features...\n",
      "Precision of xgb with one_hot features: 0.7326891283998725\n",
      "Recall of xgb with one_hot features: 0.6432837445995215\n",
      "F1 Score of xgb with one_hot features: 0.6455090119939276\n",
      "Accuracy of xgb with one_hot features: 0.8109495295124037\n",
      "--------------------------------------\n",
      "Training decision_tree with one_hot features...\n",
      "Precision of decision_tree with one_hot features: 0.5865293842067044\n",
      "Recall of decision_tree with one_hot features: 0.5300573690169252\n",
      "F1 Score of decision_tree with one_hot features: 0.5156136828865239\n",
      "Accuracy of decision_tree with one_hot features: 0.6856287425149701\n",
      "--------------------------------------\n",
      "Training random_forest with one_hot features...\n",
      "Precision of random_forest with one_hot features: 0.7876038720365218\n",
      "Recall of random_forest with one_hot features: 0.6041043107631126\n",
      "F1 Score of random_forest with one_hot features: 0.5741805406297308\n",
      "Accuracy of random_forest with one_hot features: 0.8028229255774166\n",
      "--------------------------------------\n",
      "Training svm with one_hot features...\n",
      "Precision of svm with one_hot features: 0.7473158665482992\n",
      "Recall of svm with one_hot features: 0.6241390683322392\n",
      "F1 Score of svm with one_hot features: 0.5995125447544289\n",
      "Accuracy of svm with one_hot features: 0.8195038494439693\n",
      "--------------------------------------\n",
      "Training perceptron with one_hot features...\n",
      "Precision of perceptron with one_hot features: 0.67824178575302\n",
      "Recall of perceptron with one_hot features: 0.6641708448689773\n",
      "F1 Score of perceptron with one_hot features: 0.6691116277734723\n",
      "Accuracy of perceptron with one_hot features: 0.7989734816082121\n",
      "--------------------------------------\n",
      "Training logistic_regression with one_hot features...\n",
      "Precision of logistic_regression with one_hot features: 0.7209481702255834\n",
      "Recall of logistic_regression with one_hot features: 0.6839123618761899\n",
      "F1 Score of logistic_regression with one_hot features: 0.6930119946673138\n",
      "Accuracy of logistic_regression with one_hot features: 0.823781009409752\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using n_grams features:\n",
      "Training multinomial_nb with n_grams features...\n",
      "Precision of multinomial_nb with n_grams features: 0.6951673512769866\n",
      "Recall of multinomial_nb with n_grams features: 0.7129529028595712\n",
      "F1 Score of multinomial_nb with n_grams features: 0.69730854722543\n",
      "Accuracy of multinomial_nb with n_grams features: 0.7844311377245509\n",
      "--------------------------------------\n",
      "Training xgb with n_grams features...\n",
      "Precision of xgb with n_grams features: 0.732488115280038\n",
      "Recall of xgb with n_grams features: 0.6473113096351829\n",
      "F1 Score of xgb with n_grams features: 0.6509575759364077\n",
      "Accuracy of xgb with n_grams features: 0.8126603934987169\n",
      "--------------------------------------\n",
      "Training decision_tree with n_grams features...\n",
      "Precision of decision_tree with n_grams features: 0.5763812576312576\n",
      "Recall of decision_tree with n_grams features: 0.530030201452249\n",
      "F1 Score of decision_tree with n_grams features: 0.5147333884385109\n",
      "Accuracy of decision_tree with n_grams features: 0.6869118905047049\n",
      "--------------------------------------\n",
      "Training random_forest with n_grams features...\n",
      "Precision of random_forest with n_grams features: 0.7887789079315759\n",
      "Recall of random_forest with n_grams features: 0.6072851357479695\n",
      "F1 Score of random_forest with n_grams features: 0.5764813327729396\n",
      "Accuracy of random_forest with n_grams features: 0.8062446535500428\n",
      "--------------------------------------\n",
      "Training svm with n_grams features...\n",
      "Precision of svm with n_grams features: 0.7775946441473648\n",
      "Recall of svm with n_grams features: 0.6142494405278753\n",
      "F1 Score of svm with n_grams features: 0.583859953812078\n",
      "Accuracy of svm with n_grams features: 0.8100940975192472\n",
      "--------------------------------------\n",
      "Training perceptron with n_grams features...\n",
      "Precision of perceptron with n_grams features: 0.7003481495293146\n",
      "Recall of perceptron with n_grams features: 0.6846073018379862\n",
      "F1 Score of perceptron with n_grams features: 0.6899491740195586\n",
      "Accuracy of perceptron with n_grams features: 0.8147989734816082\n",
      "--------------------------------------\n",
      "Training logistic_regression with n_grams features...\n",
      "Precision of logistic_regression with n_grams features: 0.711507080293456\n",
      "Recall of logistic_regression with n_grams features: 0.6765573515747145\n",
      "F1 Score of logistic_regression with n_grams features: 0.6837203309289607\n",
      "Accuracy of logistic_regression with n_grams features: 0.8242087254063302\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using tf_idf features:\n",
      "Training multinomial_nb with tf_idf features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of multinomial_nb with tf_idf features: 0.5447854105522766\n",
      "Recall of multinomial_nb with tf_idf features: 0.5905827918319712\n",
      "F1 Score of multinomial_nb with tf_idf features: 0.5622180475094294\n",
      "Accuracy of multinomial_nb with tf_idf features: 0.7946963216424294\n",
      "--------------------------------------\n",
      "Training xgb with tf_idf features...\n",
      "Precision of xgb with tf_idf features: 0.7401572460207285\n",
      "Recall of xgb with tf_idf features: 0.6488583808765714\n",
      "F1 Score of xgb with tf_idf features: 0.6511038845043191\n",
      "Accuracy of xgb with tf_idf features: 0.8143712574850299\n",
      "--------------------------------------\n",
      "Training decision_tree with tf_idf features...\n",
      "Precision of decision_tree with tf_idf features: 0.6001096491228071\n",
      "Recall of decision_tree with tf_idf features: 0.5416411111851084\n",
      "F1 Score of decision_tree with tf_idf features: 0.5240468616156232\n",
      "Accuracy of decision_tree with tf_idf features: 0.693327630453379\n",
      "--------------------------------------\n",
      "Training random_forest with tf_idf features...\n",
      "Precision of random_forest with tf_idf features: 0.7876398727344341\n",
      "Recall of random_forest with tf_idf features: 0.6033552470927006\n",
      "F1 Score of random_forest with tf_idf features: 0.5737611471561895\n",
      "Accuracy of random_forest with tf_idf features: 0.80196749358426\n",
      "--------------------------------------\n",
      "Training svm with tf_idf features...\n",
      "Precision of svm with tf_idf features: 0.7995362789734775\n",
      "Recall of svm with tf_idf features: 0.638713039691072\n",
      "F1 Score of svm with tf_idf features: 0.6229510570972975\n",
      "Accuracy of svm with tf_idf features: 0.8314798973481609\n",
      "--------------------------------------\n",
      "Training perceptron with tf_idf features...\n",
      "Precision of perceptron with tf_idf features: 0.6917316209447462\n",
      "Recall of perceptron with tf_idf features: 0.6512140307257757\n",
      "F1 Score of perceptron with tf_idf features: 0.6511585127817324\n",
      "Accuracy of perceptron with tf_idf features: 0.8182207014542344\n",
      "--------------------------------------\n",
      "Training logistic_regression with tf_idf features...\n",
      "Precision of logistic_regression with tf_idf features: 0.7281494463070203\n",
      "Recall of logistic_regression with tf_idf features: 0.6383928429852185\n",
      "F1 Score of logistic_regression with tf_idf features: 0.6243973508570634\n",
      "Accuracy of logistic_regression with tf_idf features: 0.8284858853721129\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using word2vec features:\n",
      "Training multinomial_nb with word2vec features...\n",
      "Skipping multinomial_nb with word2vec features due to negative values or unsuitable feature type.\n",
      "Training xgb with word2vec features...\n",
      "Precision of xgb with word2vec features: 0.6123135529255402\n",
      "Recall of xgb with word2vec features: 0.5330036226943248\n",
      "F1 Score of xgb with word2vec features: 0.5068112574813474\n",
      "Accuracy of xgb with word2vec features: 0.7151411462788708\n",
      "--------------------------------------\n",
      "Training decision_tree with word2vec features...\n",
      "Precision of decision_tree with word2vec features: 0.43115454422307486\n",
      "Recall of decision_tree with word2vec features: 0.42092283037397155\n",
      "F1 Score of decision_tree with word2vec features: 0.41073368792337295\n",
      "Accuracy of decision_tree with word2vec features: 0.5560307955517536\n",
      "--------------------------------------\n",
      "Training random_forest with word2vec features...\n",
      "Precision of random_forest with word2vec features: 0.6712239528372237\n",
      "Recall of random_forest with word2vec features: 0.48692003327762395\n",
      "F1 Score of random_forest with word2vec features: 0.4644980561949342\n",
      "Accuracy of random_forest with word2vec features: 0.6612489307100086\n",
      "--------------------------------------\n",
      "Training svm with word2vec features...\n",
      "Precision of svm with word2vec features: 0.8395158767045472\n",
      "Recall of svm with word2vec features: 0.5682962977057678\n",
      "F1 Score of svm with word2vec features: 0.5370498860368055\n",
      "Accuracy of svm with word2vec features: 0.7583404619332763\n",
      "--------------------------------------\n",
      "Training perceptron with word2vec features...\n",
      "Precision of perceptron with word2vec features: 0.5789757449921015\n",
      "Recall of perceptron with word2vec features: 0.5720155055968354\n",
      "F1 Score of perceptron with word2vec features: 0.5736911523441502\n",
      "Accuracy of perceptron with word2vec features: 0.7018819503849444\n",
      "--------------------------------------\n",
      "Training logistic_regression with word2vec features...\n",
      "Precision of logistic_regression with word2vec features: 0.5988310451011416\n",
      "Recall of logistic_regression with word2vec features: 0.5882180508910856\n",
      "F1 Score of logistic_regression with word2vec features: 0.5901855088116977\n",
      "Accuracy of logistic_regression with word2vec features: 0.7177074422583405\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using combined_bow_negation features:\n",
      "Training multinomial_nb with combined_bow_negation features...\n",
      "Precision of multinomial_nb with combined_bow_negation features: 0.6644843814772554\n",
      "Recall of multinomial_nb with combined_bow_negation features: 0.64674681626798\n",
      "F1 Score of multinomial_nb with combined_bow_negation features: 0.6487211345654349\n",
      "Accuracy of multinomial_nb with combined_bow_negation features: 0.8058169375534645\n",
      "--------------------------------------\n",
      "Training xgb with combined_bow_negation features...\n",
      "Precision of xgb with combined_bow_negation features: 0.7165861931710018\n",
      "Recall of xgb with combined_bow_negation features: 0.6402982723754495\n",
      "F1 Score of xgb with combined_bow_negation features: 0.6382715538449512\n",
      "Accuracy of xgb with combined_bow_negation features: 0.8143712574850299\n",
      "--------------------------------------\n",
      "Training decision_tree with combined_bow_negation features...\n",
      "Precision of decision_tree with combined_bow_negation features: 0.6127407491035405\n",
      "Recall of decision_tree with combined_bow_negation features: 0.5834351852732772\n",
      "F1 Score of decision_tree with combined_bow_negation features: 0.5750088549433113\n",
      "Accuracy of decision_tree with combined_bow_negation features: 0.7523524379811805\n",
      "--------------------------------------\n",
      "Training random_forest with combined_bow_negation features...\n",
      "Precision of random_forest with combined_bow_negation features: 0.7907107868311568\n",
      "Recall of random_forest with combined_bow_negation features: 0.60850640763398\n",
      "F1 Score of random_forest with combined_bow_negation features: 0.5780033481948696\n",
      "Accuracy of random_forest with combined_bow_negation features: 0.8079555175363559\n",
      "--------------------------------------\n",
      "Training svm with combined_bow_negation features...\n",
      "Precision of svm with combined_bow_negation features: 0.6584497002094669\n",
      "Recall of svm with combined_bow_negation features: 0.608278496079731\n",
      "F1 Score of svm with combined_bow_negation features: 0.5806798828660429\n",
      "Accuracy of svm with combined_bow_negation features: 0.804961505560308\n",
      "--------------------------------------\n",
      "Training perceptron with combined_bow_negation features...\n",
      "Precision of perceptron with combined_bow_negation features: 0.6687085114466168\n",
      "Recall of perceptron with combined_bow_negation features: 0.6701094419443941\n",
      "F1 Score of perceptron with combined_bow_negation features: 0.6691839116588786\n",
      "Accuracy of perceptron with combined_bow_negation features: 0.7865697177074422\n",
      "--------------------------------------\n",
      "Training logistic_regression with combined_bow_negation features...\n",
      "Precision of logistic_regression with combined_bow_negation features: 0.6988522882562617\n",
      "Recall of logistic_regression with combined_bow_negation features: 0.6745598484536158\n",
      "F1 Score of logistic_regression with combined_bow_negation features: 0.6803958364670623\n",
      "Accuracy of logistic_regression with combined_bow_negation features: 0.8199315654405475\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using lsa_topic_matrix features:\n",
      "Training multinomial_nb with lsa_topic_matrix features...\n",
      "Skipping multinomial_nb with lsa_topic_matrix features due to negative values or unsuitable feature type.\n",
      "Training xgb with lsa_topic_matrix features...\n",
      "Precision of xgb with lsa_topic_matrix features: 0.5601004723186515\n",
      "Recall of xgb with lsa_topic_matrix features: 0.5146901470536934\n",
      "F1 Score of xgb with lsa_topic_matrix features: 0.5016478055778185\n",
      "Accuracy of xgb with lsa_topic_matrix features: 0.6775021385799829\n",
      "--------------------------------------\n",
      "Training decision_tree with lsa_topic_matrix features...\n",
      "Precision of decision_tree with lsa_topic_matrix features: 0.46740309248996287\n",
      "Recall of decision_tree with lsa_topic_matrix features: 0.4749252891971403\n",
      "F1 Score of decision_tree with lsa_topic_matrix features: 0.4628385424991868\n",
      "Accuracy of decision_tree with lsa_topic_matrix features: 0.6266039349871685\n",
      "--------------------------------------\n",
      "Training random_forest with lsa_topic_matrix features...\n",
      "Precision of random_forest with lsa_topic_matrix features: 0.5789119009363726\n",
      "Recall of random_forest with lsa_topic_matrix features: 0.5060054060282394\n",
      "F1 Score of random_forest with lsa_topic_matrix features: 0.4875345337088534\n",
      "Accuracy of random_forest with lsa_topic_matrix features: 0.6719418306244653\n",
      "--------------------------------------\n",
      "Training svm with lsa_topic_matrix features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of svm with lsa_topic_matrix features: 0.4482677394960919\n",
      "Recall of svm with lsa_topic_matrix features: 0.4906763455079964\n",
      "F1 Score of svm with lsa_topic_matrix features: 0.46457438832341413\n",
      "Accuracy of svm with lsa_topic_matrix features: 0.6659538066723696\n",
      "--------------------------------------\n",
      "Training perceptron with lsa_topic_matrix features...\n",
      "Precision of perceptron with lsa_topic_matrix features: 0.42667303796437067\n",
      "Recall of perceptron with lsa_topic_matrix features: 0.4422297065163043\n",
      "F1 Score of perceptron with lsa_topic_matrix features: 0.41879524370686066\n",
      "Accuracy of perceptron with lsa_topic_matrix features: 0.5585970915312233\n",
      "--------------------------------------\n",
      "Training logistic_regression with lsa_topic_matrix features...\n",
      "Precision of logistic_regression with lsa_topic_matrix features: 0.5824204955526814\n",
      "Recall of logistic_regression with lsa_topic_matrix features: 0.4862794284451802\n",
      "F1 Score of logistic_regression with lsa_topic_matrix features: 0.463657911688615\n",
      "Accuracy of logistic_regression with lsa_topic_matrix features: 0.6616766467065869\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using lda_topic_matrix features:\n",
      "Training multinomial_nb with lda_topic_matrix features...\n",
      "Skipping multinomial_nb with lda_topic_matrix features due to negative values or unsuitable feature type.\n",
      "Training xgb with lda_topic_matrix features...\n",
      "Precision of xgb with lda_topic_matrix features: 0.512953406867488\n",
      "Recall of xgb with lda_topic_matrix features: 0.4929998594052101\n",
      "F1 Score of xgb with lda_topic_matrix features: 0.47314222158546454\n",
      "Accuracy of xgb with lda_topic_matrix features: 0.6608212147134302\n",
      "--------------------------------------\n",
      "Training decision_tree with lda_topic_matrix features...\n",
      "Precision of decision_tree with lda_topic_matrix features: 0.4970797254028911\n",
      "Recall of decision_tree with lda_topic_matrix features: 0.48072202295817784\n",
      "F1 Score of decision_tree with lda_topic_matrix features: 0.46970692685019744\n",
      "Accuracy of decision_tree with lda_topic_matrix features: 0.6364414029084687\n",
      "--------------------------------------\n",
      "Training random_forest with lda_topic_matrix features...\n",
      "Precision of random_forest with lda_topic_matrix features: 0.5179599061984965\n",
      "Recall of random_forest with lda_topic_matrix features: 0.4988241835723974\n",
      "F1 Score of random_forest with lda_topic_matrix features: 0.48713745137831893\n",
      "Accuracy of random_forest with lda_topic_matrix features: 0.6595380667236954\n",
      "--------------------------------------\n",
      "Training svm with lda_topic_matrix features...\n",
      "Precision of svm with lda_topic_matrix features: 0.4449932911036914\n",
      "Recall of svm with lda_topic_matrix features: 0.4936132966729776\n",
      "F1 Score of svm with lda_topic_matrix features: 0.46656363882459173\n",
      "Accuracy of svm with lda_topic_matrix features: 0.6646706586826348\n",
      "--------------------------------------\n",
      "Training perceptron with lda_topic_matrix features...\n",
      "Precision of perceptron with lda_topic_matrix features: 0.41924274405220824\n",
      "Recall of perceptron with lda_topic_matrix features: 0.45946398500604135\n",
      "F1 Score of perceptron with lda_topic_matrix features: 0.4100527407764282\n",
      "Accuracy of perceptron with lda_topic_matrix features: 0.5838323353293413\n",
      "--------------------------------------\n",
      "Training logistic_regression with lda_topic_matrix features...\n",
      "Precision of logistic_regression with lda_topic_matrix features: 0.44463704716336294\n",
      "Recall of logistic_regression with lda_topic_matrix features: 0.4927665566221731\n",
      "F1 Score of logistic_regression with lda_topic_matrix features: 0.46591480844972627\n",
      "Accuracy of logistic_regression with lda_topic_matrix features: 0.6633875106928999\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through each feature set\n",
    "for feature_key, feature_name in feature_set.items():\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features[feature_key], y, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Oversample to balance the classes\n",
    "    # oversampler = RandomOverSampler(random_state=42)\n",
    "    # X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # change -1 to 2 because of xbg\n",
    "    y_train_series = pd.Series(y_train)\n",
    "    y_test_series = pd.Series(y_test)\n",
    "    y_train_mapped = y_train_series.map({-1: 0, 0: 1, 1: 2})\n",
    "    y_test_mapped = y_test_series.map({-1: 0, 0: 1, 1: 2})\n",
    "\n",
    "\n",
    "    print(f\"Using {feature_name} features:\")\n",
    "    \n",
    "    # Loop through each classifier\n",
    "    for clf_name, clf in best_estimators.items():\n",
    "        print(f\"Training {clf_name} with {feature_name} features...\")   \n",
    "        \n",
    "        if clf_name == 'multinomial_nb' and feature_key in [4, 6, 7]:\n",
    "            print(f\"Skipping {clf_name} with {feature_name} features due to negative values or unsuitable feature type.\")\n",
    "            continue\n",
    "        # Convert sparse to dense if necessary\n",
    "        if (feature_key != 4 and feature_key != 6 and feature_key != 7) and clf_name in ['gaussian_nb', 'multinomial_nb', 'perceptron']:\n",
    "            X_train_dense = X_train.toarray()\n",
    "            X_test_dense = X_test.toarray()\n",
    "            clf.fit(X_train_dense, y_train)\n",
    "            y_pred = clf.predict(X_test_dense)\n",
    "        else:\n",
    "            if clf_name == 'xgb':\n",
    "                clf.fit(X_train, y_train_mapped)\n",
    "            else:\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        if clf_name == 'xgb':\n",
    "            acc = accuracy_score(y_test_mapped, y_pred)\n",
    "            precision = precision_score(y_test_mapped, y_pred, average='macro')\n",
    "            recall = recall_score(y_test_mapped, y_pred, average='macro')\n",
    "            f1 = f1_score(y_test_mapped, y_pred, average='macro')\n",
    "        else:\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='macro')\n",
    "            recall = recall_score(y_test, y_pred, average='macro')\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        print(f\"Precision of {clf_name} with {feature_name} features: {precision}\")\n",
    "        print(f\"Recall of {clf_name} with {feature_name} features: {recall}\")\n",
    "        print(f\"F1 Score of {clf_name} with {feature_name} features: {f1}\")\n",
    "        print(f\"Accuracy of {clf_name} with {feature_name} features: {acc}\")\n",
    "            \n",
    "    # Append the results to the list\n",
    "        results.append({\n",
    "            'feature_set': feature_name,\n",
    "            'classifier': clf_name,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'accuracy': acc\n",
    "        })\n",
    "        print(\"--------------------------------------\")\n",
    "\n",
    "    print(\"-------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('classification_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
