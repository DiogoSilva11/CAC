{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from xgboost import XGBClassifier\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLzom-2TITa4gasV7_fCCA</td>\n",
       "      <td>Great experience purchasing a washer and dryer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a5JHzBrWxRd_OmIvV7znDA</td>\n",
       "      <td>Went here based on the high ratings and raves ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X-o--dwf0HuFMittYi4wCA</td>\n",
       "      <td>oh Millers, how i wanted to like you.  You are...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INGNbsyo-MouZZzcxnCSGQ</td>\n",
       "      <td>This place gets two stars from me only because...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>k7VatXVLism-cTDJE8TTUw</td>\n",
       "      <td>This place was awesome. Clean, beautiful and t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11681</th>\n",
       "      <td>IlU-MQzMKc7jAHWwK5VFGQ</td>\n",
       "      <td>To be fair, I tried them in their first week. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11682</th>\n",
       "      <td>Qt3BsRvQuJccDQfFWM1XPw</td>\n",
       "      <td>Awful place. It's dirty. Had two birthday part...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11683</th>\n",
       "      <td>3CQQ8Im_UX6QqDECuXYK8A</td>\n",
       "      <td>A truly vegetarian delight!  I took a Jewish f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11684</th>\n",
       "      <td>ery1nBM7zKweFLBe-bT5ag</td>\n",
       "      <td>I have a 2011 Toyota Sienna Limited. During th...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11685</th>\n",
       "      <td>N5_SaVzmwkZUslgWDGGsQQ</td>\n",
       "      <td>I'm a single father raising my 17 year old son...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11686 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    review_id  \\\n",
       "0      LLzom-2TITa4gasV7_fCCA   \n",
       "1      a5JHzBrWxRd_OmIvV7znDA   \n",
       "2      X-o--dwf0HuFMittYi4wCA   \n",
       "3      INGNbsyo-MouZZzcxnCSGQ   \n",
       "4      k7VatXVLism-cTDJE8TTUw   \n",
       "...                       ...   \n",
       "11681  IlU-MQzMKc7jAHWwK5VFGQ   \n",
       "11682  Qt3BsRvQuJccDQfFWM1XPw   \n",
       "11683  3CQQ8Im_UX6QqDECuXYK8A   \n",
       "11684  ery1nBM7zKweFLBe-bT5ag   \n",
       "11685  N5_SaVzmwkZUslgWDGGsQQ   \n",
       "\n",
       "                                                    text  sentiment  \n",
       "0      Great experience purchasing a washer and dryer...          1  \n",
       "1      Went here based on the high ratings and raves ...         -1  \n",
       "2      oh Millers, how i wanted to like you.  You are...         -1  \n",
       "3      This place gets two stars from me only because...         -1  \n",
       "4      This place was awesome. Clean, beautiful and t...          1  \n",
       "...                                                  ...        ...  \n",
       "11681  To be fair, I tried them in their first week. ...          0  \n",
       "11682  Awful place. It's dirty. Had two birthday part...         -1  \n",
       "11683  A truly vegetarian delight!  I took a Jewish f...          1  \n",
       "11684  I have a 2011 Toyota Sienna Limited. During th...         -1  \n",
       "11685  I'm a single father raising my 17 year old son...         -1  \n",
       "\n",
       "[11686 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load review sentiment data\n",
    "\n",
    "review_df = pd.read_csv('data/review_sentiment.csv')\n",
    "\n",
    "review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = {\n",
    "    0: 'bag_of_words',\n",
    "    1: 'one_hot',\n",
    "    2: 'n_grams',\n",
    "    3: 'tf_idf',\n",
    "    4: 'word2vec',\n",
    "    5: 'combined_bow_negation',\n",
    "    6: 'lsa_topic_matrix',\n",
    "    7: 'lda_topic_matrix'\n",
    "}\n",
    "# Load all feature sets\n",
    "features = {}\n",
    "for key, feature_name in feature_set.items():\n",
    "    if key == 4 or key == 6 or key == 7:\n",
    "        features[key] = np.load('features/' + feature_name + '.npy')\n",
    "    else:\n",
    "        features[key] = sparse.load_npz('features/' + feature_name + '.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11686,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target labels\n",
    "\n",
    "y = review_df['sentiment'].to_numpy()\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers\n",
    "\n",
    "classifiers = {\n",
    "    'gaussian_nb': GaussianNB(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'svm': SVC(),\n",
    "    'perceptron': Perceptron(tol=1e-3, random_state=0),\n",
    "    'xgb': XGBClassifier(),\n",
    "    'logistic_regression': LogisticRegression(max_iter=1000, random_state=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'gaussian_nb': {},\n",
    "    'decision_tree': {\n",
    "        'max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1.0],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'perceptron': {\n",
    "        'alpha': [0.0001, 0.001],\n",
    "        'penalty': [None, 'l2']\n",
    "    },\n",
    "    'xgb': {\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'n_estimators': [100, 200]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'C': [0.1, 1.0],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec features for grid search\n",
    "word2vec_features = features[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets using word2vec features\n",
    "X_train_w2v, X_test_w2v, y_train, y_test = train_test_split(word2vec_features, y, test_size=0.20, random_state=42)\n",
    "\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_w2v, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb algorithm expects [0 1 2], not [-1 0 1]\n",
    "\n",
    "# Convert numpy arrays to pandas Series\n",
    "y_train_series = pd.Series(y_train_resampled)\n",
    "y_test_series = pd.Series(y_test)\n",
    "\n",
    "# Remap labels: -1 -> 0, 0 -> 1, 1 -> 2\n",
    "y_train_mapped = y_train_series.map({-1: 0, 0: 1, 1: 2})\n",
    "y_test_mapped = y_test_series.map({-1: 0, 0: 1, 1: 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators = {}\n",
    "grid_results = []\n",
    "\n",
    "# Apply grid search to each classifier using word2vec features\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"Grid search for {clf_name}\")\n",
    "    param_grid = param_grids[clf_name]\n",
    "    if clf_name == 'xgb':\n",
    "        grid_search = GridSearchCV(clf, param_grid, scoring='accuracy', cv=5)\n",
    "        grid_search.fit(X_train_resampled, y_train_mapped)\n",
    "    else:\n",
    "        grid_search = GridSearchCV(clf, param_grid, scoring='accuracy', cv=5)\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "    best_estimators[clf_name] = grid_search.best_estimator_\n",
    "    grid_results.append({\n",
    "        'classifier': clf_name,\n",
    "        'best_parameters': grid_search.best_params_,\n",
    "        'best_accuracy': grid_search.best_score_,\n",
    "    })\n",
    "    print(f\"Best parameters for {clf_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best accuracy for {clf_name}: {grid_search.best_score_}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "grid_results_df = pd.DataFrame(grid_results)\n",
    "grid_results_df.to_csv('grid_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators = {\n",
    "    'multinomial_nb': MultinomialNB(),\n",
    "    'gaussian_nb': GaussianNB(),\n",
    "    'xgb': XGBClassifier(learning_rate=0.1, max_depth=5, n_estimators=200),\n",
    "    'decision_tree': DecisionTreeClassifier(max_depth=10),\n",
    "    'random_forest': RandomForestClassifier(max_depth=10, n_estimators=200),\n",
    "    'svm': SVC(C=1.0, kernel='rbf'),\n",
    "    'perceptron': Perceptron(tol=1e-3, random_state=0, alpha=0.0001, penalty=None),\n",
    "    'logistic_regression': LogisticRegression(C=1.0, penalty='l2', solver='liblinear', max_iter=1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bag_of_words features:\n",
      "Training multinomial_nb with bag_of_words features...\n",
      "Accuracy of multinomial_nb with bag_of_words features: 0.7416595380667237\n",
      "Precision of multinomial_nb with bag_of_words features: 0.6755983110089385\n",
      "Recall of multinomial_nb with bag_of_words features: 0.6942433304685823\n",
      "F1 Score of multinomial_nb with bag_of_words features: 0.6674018722762441\n",
      "--------------------------------------\n",
      "Training gaussian_nb with bag_of_words features...\n",
      "Accuracy of gaussian_nb with bag_of_words features: 0.5209580838323353\n",
      "Precision of gaussian_nb with bag_of_words features: 0.4774245053273527\n",
      "Recall of gaussian_nb with bag_of_words features: 0.45666741721028226\n",
      "F1 Score of gaussian_nb with bag_of_words features: 0.4544013347189526\n",
      "--------------------------------------\n",
      "Training xgb with bag_of_words features...\n",
      "Accuracy of xgb with bag_of_words features: 0.7745936698032506\n",
      "Precision of xgb with bag_of_words features: 0.6817569352091645\n",
      "Recall of xgb with bag_of_words features: 0.6981402374889135\n",
      "F1 Score of xgb with bag_of_words features: 0.6863947300028698\n",
      "--------------------------------------\n",
      "Training decision_tree with bag_of_words features...\n",
      "Accuracy of decision_tree with bag_of_words features: 0.6398631308810949\n",
      "Precision of decision_tree with bag_of_words features: 0.5662971955254874\n",
      "Recall of decision_tree with bag_of_words features: 0.5767399133809241\n",
      "F1 Score of decision_tree with bag_of_words features: 0.560862483175789\n",
      "--------------------------------------\n",
      "Training random_forest with bag_of_words features...\n",
      "Accuracy of random_forest with bag_of_words features: 0.7570573139435415\n",
      "Precision of random_forest with bag_of_words features: 0.6670370430508489\n",
      "Recall of random_forest with bag_of_words features: 0.6734427012804698\n",
      "F1 Score of random_forest with bag_of_words features: 0.6691317704004739\n",
      "--------------------------------------\n",
      "Training svm with bag_of_words features...\n",
      "Accuracy of svm with bag_of_words features: 0.7981180496150556\n",
      "Precision of svm with bag_of_words features: 0.6956335072474099\n",
      "Recall of svm with bag_of_words features: 0.6928801952681924\n",
      "F1 Score of svm with bag_of_words features: 0.6933175405969195\n",
      "--------------------------------------\n",
      "Training perceptron with bag_of_words features...\n",
      "Accuracy of perceptron with bag_of_words features: 0.7989734816082121\n",
      "Precision of perceptron with bag_of_words features: 0.6754603533366712\n",
      "Recall of perceptron with bag_of_words features: 0.6597579655405341\n",
      "F1 Score of perceptron with bag_of_words features: 0.6641549766701939\n",
      "--------------------------------------\n",
      "Training logistic_regression with bag_of_words features...\n",
      "Accuracy of logistic_regression with bag_of_words features: 0.7968349016253208\n",
      "Precision of logistic_regression with bag_of_words features: 0.6835917905562159\n",
      "Recall of logistic_regression with bag_of_words features: 0.681578699783611\n",
      "F1 Score of logistic_regression with bag_of_words features: 0.682534483929552\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using one_hot features:\n",
      "Training multinomial_nb with one_hot features...\n",
      "Accuracy of multinomial_nb with one_hot features: 0.7523524379811805\n",
      "Precision of multinomial_nb with one_hot features: 0.6782345093102929\n",
      "Recall of multinomial_nb with one_hot features: 0.6987756625133063\n",
      "F1 Score of multinomial_nb with one_hot features: 0.6746147595759006\n",
      "--------------------------------------\n",
      "Training gaussian_nb with one_hot features...\n",
      "Accuracy of gaussian_nb with one_hot features: 0.5162532078699743\n",
      "Precision of gaussian_nb with one_hot features: 0.4736358076061387\n",
      "Recall of gaussian_nb with one_hot features: 0.45360604494170603\n",
      "F1 Score of gaussian_nb with one_hot features: 0.4506693020847945\n",
      "--------------------------------------\n",
      "Training xgb with one_hot features...\n",
      "Accuracy of xgb with one_hot features: 0.7733105218135158\n",
      "Precision of xgb with one_hot features: 0.680928275893951\n",
      "Recall of xgb with one_hot features: 0.6980751199019855\n",
      "F1 Score of xgb with one_hot features: 0.6852363858521849\n",
      "--------------------------------------\n",
      "Training decision_tree with one_hot features...\n",
      "Accuracy of decision_tree with one_hot features: 0.6407185628742516\n",
      "Precision of decision_tree with one_hot features: 0.5686577405263115\n",
      "Recall of decision_tree with one_hot features: 0.580110699897778\n",
      "F1 Score of decision_tree with one_hot features: 0.5635031403723456\n",
      "--------------------------------------\n",
      "Training random_forest with one_hot features...\n",
      "Accuracy of random_forest with one_hot features: 0.7681779298545766\n",
      "Precision of random_forest with one_hot features: 0.6796159663963847\n",
      "Recall of random_forest with one_hot features: 0.6904486453744844\n",
      "F1 Score of random_forest with one_hot features: 0.683732024758504\n",
      "--------------------------------------\n",
      "Training svm with one_hot features...\n",
      "Accuracy of svm with one_hot features: 0.8169375534644996\n",
      "Precision of svm with one_hot features: 0.7074492680182723\n",
      "Recall of svm with one_hot features: 0.6832178447558673\n",
      "F1 Score of svm with one_hot features: 0.6894795109146578\n",
      "--------------------------------------\n",
      "Training perceptron with one_hot features...\n",
      "Accuracy of perceptron with one_hot features: 0.7964071856287425\n",
      "Precision of perceptron with one_hot features: 0.6777516220101162\n",
      "Recall of perceptron with one_hot features: 0.6703970798567837\n",
      "F1 Score of perceptron with one_hot features: 0.6731495881360092\n",
      "--------------------------------------\n",
      "Training logistic_regression with one_hot features...\n",
      "Accuracy of logistic_regression with one_hot features: 0.8058169375534645\n",
      "Precision of logistic_regression with one_hot features: 0.697440356936263\n",
      "Recall of logistic_regression with one_hot features: 0.6962505589435729\n",
      "F1 Score of logistic_regression with one_hot features: 0.6967849133340023\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using n_grams features:\n",
      "Training multinomial_nb with n_grams features...\n",
      "Accuracy of multinomial_nb with n_grams features: 0.7715996578272027\n",
      "Precision of multinomial_nb with n_grams features: 0.6892407575758538\n",
      "Recall of multinomial_nb with n_grams features: 0.7082579882682634\n",
      "F1 Score of multinomial_nb with n_grams features: 0.688555990711251\n",
      "--------------------------------------\n",
      "Training gaussian_nb with n_grams features...\n",
      "Accuracy of gaussian_nb with n_grams features: 0.7108639863130881\n",
      "Precision of gaussian_nb with n_grams features: 0.5810508967115792\n",
      "Recall of gaussian_nb with n_grams features: 0.5808524695527711\n",
      "F1 Score of gaussian_nb with n_grams features: 0.5808064819306095\n",
      "--------------------------------------\n",
      "Training xgb with n_grams features...\n",
      "Accuracy of xgb with n_grams features: 0.7754491017964071\n",
      "Precision of xgb with n_grams features: 0.6859477209154524\n",
      "Recall of xgb with n_grams features: 0.7052835099225038\n",
      "F1 Score of xgb with n_grams features: 0.6912876014637724\n",
      "--------------------------------------\n",
      "Training decision_tree with n_grams features...\n",
      "Accuracy of decision_tree with n_grams features: 0.6411462788708298\n",
      "Precision of decision_tree with n_grams features: 0.5673354136212665\n",
      "Recall of decision_tree with n_grams features: 0.5781837055923956\n",
      "F1 Score of decision_tree with n_grams features: 0.5625611232719847\n",
      "--------------------------------------\n",
      "Training random_forest with n_grams features...\n",
      "Accuracy of random_forest with n_grams features: 0.7557741659538066\n",
      "Precision of random_forest with n_grams features: 0.6679982728426118\n",
      "Recall of random_forest with n_grams features: 0.6762761619947969\n",
      "F1 Score of random_forest with n_grams features: 0.6708059468912948\n",
      "--------------------------------------\n",
      "Training svm with n_grams features...\n",
      "Accuracy of svm with n_grams features: 0.806672369546621\n",
      "Precision of svm with n_grams features: 0.7031292821190203\n",
      "Recall of svm with n_grams features: 0.6916261531679812\n",
      "F1 Score of svm with n_grams features: 0.6950648034480235\n",
      "--------------------------------------\n",
      "Training perceptron with n_grams features...\n",
      "Accuracy of perceptron with n_grams features: 0.8152266894781864\n",
      "Precision of perceptron with n_grams features: 0.7013990792801423\n",
      "Recall of perceptron with n_grams features: 0.6853020303790457\n",
      "F1 Score of perceptron with n_grams features: 0.6909962403503878\n",
      "--------------------------------------\n",
      "Training logistic_regression with n_grams features...\n",
      "Accuracy of logistic_regression with n_grams features: 0.8156544054747648\n",
      "Precision of logistic_regression with n_grams features: 0.7048217605360464\n",
      "Recall of logistic_regression with n_grams features: 0.6941877268148091\n",
      "F1 Score of logistic_regression with n_grams features: 0.6985208481748005\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using tf_idf features:\n",
      "Training multinomial_nb with tf_idf features...\n",
      "Accuracy of multinomial_nb with tf_idf features: 0.7172797262617622\n",
      "Precision of multinomial_nb with tf_idf features: 0.6790990607881499\n",
      "Recall of multinomial_nb with tf_idf features: 0.7048884702758299\n",
      "F1 Score of multinomial_nb with tf_idf features: 0.6597621619801322\n",
      "--------------------------------------\n",
      "Training gaussian_nb with tf_idf features...\n",
      "Accuracy of gaussian_nb with tf_idf features: 0.5196749358426005\n",
      "Precision of gaussian_nb with tf_idf features: 0.46827876236863286\n",
      "Recall of gaussian_nb with tf_idf features: 0.45794883829590655\n",
      "F1 Score of gaussian_nb with tf_idf features: 0.45313005968798814\n",
      "--------------------------------------\n",
      "Training xgb with tf_idf features...\n",
      "Accuracy of xgb with tf_idf features: 0.7810094097519247\n",
      "Precision of xgb with tf_idf features: 0.6878663376146598\n",
      "Recall of xgb with tf_idf features: 0.7048600341867332\n",
      "F1 Score of xgb with tf_idf features: 0.6933187322973163\n",
      "--------------------------------------\n",
      "Training decision_tree with tf_idf features...\n",
      "Accuracy of decision_tree with tf_idf features: 0.5970915312232677\n",
      "Precision of decision_tree with tf_idf features: 0.5520326413276453\n",
      "Recall of decision_tree with tf_idf features: 0.5567495541665214\n",
      "F1 Score of decision_tree with tf_idf features: 0.5302055904839098\n",
      "--------------------------------------\n",
      "Training random_forest with tf_idf features...\n",
      "Accuracy of random_forest with tf_idf features: 0.7493584260051326\n",
      "Precision of random_forest with tf_idf features: 0.6588504796408885\n",
      "Recall of random_forest with tf_idf features: 0.6670758715027046\n",
      "F1 Score of random_forest with tf_idf features: 0.6622075662507086\n",
      "--------------------------------------\n",
      "Training svm with tf_idf features...\n",
      "Accuracy of svm with tf_idf features: 0.8289136013686912\n",
      "Precision of svm with tf_idf features: 0.7189988867158833\n",
      "Recall of svm with tf_idf features: 0.6660486838530584\n",
      "F1 Score of svm with tf_idf features: 0.6699096549516956\n",
      "--------------------------------------\n",
      "Training perceptron with tf_idf features...\n",
      "Accuracy of perceptron with tf_idf features: 0.7917023096663816\n",
      "Precision of perceptron with tf_idf features: 0.6760143867504187\n",
      "Recall of perceptron with tf_idf features: 0.6420566586432497\n",
      "F1 Score of perceptron with tf_idf features: 0.6507807057807057\n",
      "--------------------------------------\n",
      "Training logistic_regression with tf_idf features...\n",
      "Accuracy of logistic_regression with tf_idf features: 0.7998289136013687\n",
      "Precision of logistic_regression with tf_idf features: 0.7015378093273311\n",
      "Recall of logistic_regression with tf_idf features: 0.7149829647741339\n",
      "F1 Score of logistic_regression with tf_idf features: 0.7057960204485703\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using word2vec features:\n",
      "Training multinomial_nb with word2vec features...\n",
      "Skipping multinomial_nb with word2vec features due to negative values or unsuitable feature type.\n",
      "Training gaussian_nb with word2vec features...\n",
      "Accuracy of gaussian_nb with word2vec features: 0.48417450812660395\n",
      "Precision of gaussian_nb with word2vec features: 0.4602849170461482\n",
      "Recall of gaussian_nb with word2vec features: 0.4754017258274743\n",
      "F1 Score of gaussian_nb with word2vec features: 0.43875030838936785\n",
      "--------------------------------------\n",
      "Training xgb with word2vec features...\n",
      "Accuracy of xgb with word2vec features: 0.7271171941830624\n",
      "Precision of xgb with word2vec features: 0.6489996610050763\n",
      "Recall of xgb with word2vec features: 0.5697189478435614\n",
      "F1 Score of xgb with word2vec features: 0.5676203764445326\n",
      "--------------------------------------\n",
      "Training decision_tree with word2vec features...\n",
      "Accuracy of decision_tree with word2vec features: 0.48545765611633873\n",
      "Precision of decision_tree with word2vec features: 0.4185720885868431\n",
      "Recall of decision_tree with word2vec features: 0.4191549301730162\n",
      "F1 Score of decision_tree with word2vec features: 0.4150795874193742\n",
      "--------------------------------------\n",
      "Training random_forest with word2vec features...\n",
      "Accuracy of random_forest with word2vec features: 0.6077844311377245\n",
      "Precision of random_forest with word2vec features: 0.5398857843191541\n",
      "Recall of random_forest with word2vec features: 0.46464580155415386\n",
      "F1 Score of random_forest with word2vec features: 0.4383819755556522\n",
      "--------------------------------------\n",
      "Training svm with word2vec features...\n",
      "Accuracy of svm with word2vec features: 0.7489307100085543\n",
      "Precision of svm with word2vec features: 0.6057213552698353\n",
      "Recall of svm with word2vec features: 0.5899218906087965\n",
      "F1 Score of svm with word2vec features: 0.5857070878476741\n",
      "--------------------------------------\n",
      "Training perceptron with word2vec features...\n",
      "Accuracy of perceptron with word2vec features: 0.7014542343883661\n",
      "Precision of perceptron with word2vec features: 0.5785936285936285\n",
      "Recall of perceptron with word2vec features: 0.5724659374765455\n",
      "F1 Score of perceptron with word2vec features: 0.574507713633382\n",
      "--------------------------------------\n",
      "Training logistic_regression with word2vec features...\n",
      "Accuracy of logistic_regression with word2vec features: 0.6993156544054747\n",
      "Precision of logistic_regression with word2vec features: 0.5817478565953803\n",
      "Recall of logistic_regression with word2vec features: 0.5786486197925751\n",
      "F1 Score of logistic_regression with word2vec features: 0.5799336930534291\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using combined_bow_negation features:\n",
      "Training multinomial_nb with combined_bow_negation features...\n",
      "Accuracy of multinomial_nb with combined_bow_negation features: 0.7549187339606501\n",
      "Precision of multinomial_nb with combined_bow_negation features: 0.6839079298221473\n",
      "Recall of multinomial_nb with combined_bow_negation features: 0.7028573512575834\n",
      "F1 Score of multinomial_nb with combined_bow_negation features: 0.6775812521724606\n",
      "--------------------------------------\n",
      "Training gaussian_nb with combined_bow_negation features...\n",
      "Accuracy of gaussian_nb with combined_bow_negation features: 0.5226689478186484\n",
      "Precision of gaussian_nb with combined_bow_negation features: 0.47868781368064406\n",
      "Recall of gaussian_nb with combined_bow_negation features: 0.45825782970271073\n",
      "F1 Score of gaussian_nb with combined_bow_negation features: 0.4559843527790914\n",
      "--------------------------------------\n",
      "Training xgb with combined_bow_negation features...\n",
      "Accuracy of xgb with combined_bow_negation features: 0.7917023096663816\n",
      "Precision of xgb with combined_bow_negation features: 0.6907089764934318\n",
      "Recall of xgb with combined_bow_negation features: 0.6981126470827639\n",
      "F1 Score of xgb with combined_bow_negation features: 0.6931842105716978\n",
      "--------------------------------------\n",
      "Training decision_tree with combined_bow_negation features...\n",
      "Accuracy of decision_tree with combined_bow_negation features: 0.6783575705731394\n",
      "Precision of decision_tree with combined_bow_negation features: 0.6121130867620127\n",
      "Recall of decision_tree with combined_bow_negation features: 0.6114614352719557\n",
      "F1 Score of decision_tree with combined_bow_negation features: 0.5983129384568429\n",
      "--------------------------------------\n",
      "Training random_forest with combined_bow_negation features...\n",
      "Accuracy of random_forest with combined_bow_negation features: 0.7703165098374679\n",
      "Precision of random_forest with combined_bow_negation features: 0.6738236427184073\n",
      "Recall of random_forest with combined_bow_negation features: 0.6731330756114552\n",
      "F1 Score of random_forest with combined_bow_negation features: 0.6732986159752965\n",
      "--------------------------------------\n",
      "Training svm with combined_bow_negation features...\n",
      "Accuracy of svm with combined_bow_negation features: 0.7972626176218991\n",
      "Precision of svm with combined_bow_negation features: 0.695369110026517\n",
      "Recall of svm with combined_bow_negation features: 0.7017874566191077\n",
      "F1 Score of svm with combined_bow_negation features: 0.6981980796657425\n",
      "--------------------------------------\n",
      "Training perceptron with combined_bow_negation features...\n",
      "Accuracy of perceptron with combined_bow_negation features: 0.795551753635586\n",
      "Precision of perceptron with combined_bow_negation features: 0.6526588347426459\n",
      "Recall of perceptron with combined_bow_negation features: 0.6222444213995841\n",
      "F1 Score of perceptron with combined_bow_negation features: 0.6234856661158068\n",
      "--------------------------------------\n",
      "Training logistic_regression with combined_bow_negation features...\n",
      "Accuracy of logistic_regression with combined_bow_negation features: 0.806672369546621\n",
      "Precision of logistic_regression with combined_bow_negation features: 0.6939979828481805\n",
      "Recall of logistic_regression with combined_bow_negation features: 0.6907410402534512\n",
      "F1 Score of logistic_regression with combined_bow_negation features: 0.6922618433824267\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using lsa_topic_matrix features:\n",
      "Training multinomial_nb with lsa_topic_matrix features...\n",
      "Skipping multinomial_nb with lsa_topic_matrix features due to negative values or unsuitable feature type.\n",
      "Training gaussian_nb with lsa_topic_matrix features...\n",
      "Accuracy of gaussian_nb with lsa_topic_matrix features: 0.5496150556030795\n",
      "Precision of gaussian_nb with lsa_topic_matrix features: 0.48835518250348026\n",
      "Recall of gaussian_nb with lsa_topic_matrix features: 0.45252906770854806\n",
      "F1 Score of gaussian_nb with lsa_topic_matrix features: 0.4446099520675861\n",
      "--------------------------------------\n",
      "Training xgb with lsa_topic_matrix features...\n",
      "Accuracy of xgb with lsa_topic_matrix features: 0.6218990590248076\n",
      "Precision of xgb with lsa_topic_matrix features: 0.5423742185966206\n",
      "Recall of xgb with lsa_topic_matrix features: 0.5482043508273422\n",
      "F1 Score of xgb with lsa_topic_matrix features: 0.5400555145214313\n",
      "--------------------------------------\n",
      "Training decision_tree with lsa_topic_matrix features...\n",
      "Accuracy of decision_tree with lsa_topic_matrix features: 0.5568862275449101\n",
      "Precision of decision_tree with lsa_topic_matrix features: 0.5028509211979275\n",
      "Recall of decision_tree with lsa_topic_matrix features: 0.5062843756904208\n",
      "F1 Score of decision_tree with lsa_topic_matrix features: 0.4925019262227157\n",
      "--------------------------------------\n",
      "Training random_forest with lsa_topic_matrix features...\n",
      "Accuracy of random_forest with lsa_topic_matrix features: 0.6223267750213858\n",
      "Precision of random_forest with lsa_topic_matrix features: 0.5451194234632739\n",
      "Recall of random_forest with lsa_topic_matrix features: 0.5528994768393869\n",
      "F1 Score of random_forest with lsa_topic_matrix features: 0.5436258583557079\n",
      "--------------------------------------\n",
      "Training svm with lsa_topic_matrix features...\n",
      "Accuracy of svm with lsa_topic_matrix features: 0.5911035072711719\n",
      "Precision of svm with lsa_topic_matrix features: 0.5642204057769836\n",
      "Recall of svm with lsa_topic_matrix features: 0.5770115890276866\n",
      "F1 Score of svm with lsa_topic_matrix features: 0.5430917794651701\n",
      "--------------------------------------\n",
      "Training perceptron with lsa_topic_matrix features...\n",
      "Accuracy of perceptron with lsa_topic_matrix features: 0.31394354148845166\n",
      "Precision of perceptron with lsa_topic_matrix features: 0.5566747035452951\n",
      "Recall of perceptron with lsa_topic_matrix features: 0.44015862897880614\n",
      "F1 Score of perceptron with lsa_topic_matrix features: 0.31745155515529627\n",
      "--------------------------------------\n",
      "Training logistic_regression with lsa_topic_matrix features...\n",
      "Accuracy of logistic_regression with lsa_topic_matrix features: 0.6047904191616766\n",
      "Precision of logistic_regression with lsa_topic_matrix features: 0.5423194863077613\n",
      "Recall of logistic_regression with lsa_topic_matrix features: 0.5481773946834027\n",
      "F1 Score of logistic_regression with lsa_topic_matrix features: 0.5353567381860217\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using lda_topic_matrix features:\n",
      "Training multinomial_nb with lda_topic_matrix features...\n",
      "Skipping multinomial_nb with lda_topic_matrix features due to negative values or unsuitable feature type.\n",
      "Training gaussian_nb with lda_topic_matrix features...\n",
      "Accuracy of gaussian_nb with lda_topic_matrix features: 0.4007698887938409\n",
      "Precision of gaussian_nb with lda_topic_matrix features: 0.48147858828732853\n",
      "Recall of gaussian_nb with lda_topic_matrix features: 0.4869763769039759\n",
      "F1 Score of gaussian_nb with lda_topic_matrix features: 0.3839451072355382\n",
      "--------------------------------------\n",
      "Training xgb with lda_topic_matrix features...\n",
      "Accuracy of xgb with lda_topic_matrix features: 0.5915312232677502\n",
      "Precision of xgb with lda_topic_matrix features: 0.5179899822560761\n",
      "Recall of xgb with lda_topic_matrix features: 0.5190510168808887\n",
      "F1 Score of xgb with lda_topic_matrix features: 0.5125815859538356\n",
      "--------------------------------------\n",
      "Training decision_tree with lda_topic_matrix features...\n",
      "Accuracy of decision_tree with lda_topic_matrix features: 0.49615055603079555\n",
      "Precision of decision_tree with lda_topic_matrix features: 0.4781859690517347\n",
      "Recall of decision_tree with lda_topic_matrix features: 0.4736258973488896\n",
      "F1 Score of decision_tree with lda_topic_matrix features: 0.45218313891632594\n",
      "--------------------------------------\n",
      "Training random_forest with lda_topic_matrix features...\n",
      "Accuracy of random_forest with lda_topic_matrix features: 0.6039349871685201\n",
      "Precision of random_forest with lda_topic_matrix features: 0.5222930188753092\n",
      "Recall of random_forest with lda_topic_matrix features: 0.5273340585064604\n",
      "F1 Score of random_forest with lda_topic_matrix features: 0.5210695623406071\n",
      "--------------------------------------\n",
      "Training svm with lda_topic_matrix features...\n",
      "Accuracy of svm with lda_topic_matrix features: 0.6077844311377245\n",
      "Precision of svm with lda_topic_matrix features: 0.5333861797692422\n",
      "Recall of svm with lda_topic_matrix features: 0.5435911503507999\n",
      "F1 Score of svm with lda_topic_matrix features: 0.5315119838777507\n",
      "--------------------------------------\n",
      "Training perceptron with lda_topic_matrix features...\n",
      "Accuracy of perceptron with lda_topic_matrix features: 0.3887938408896493\n",
      "Precision of perceptron with lda_topic_matrix features: 0.512426087465605\n",
      "Recall of perceptron with lda_topic_matrix features: 0.354383016149373\n",
      "F1 Score of perceptron with lda_topic_matrix features: 0.22462829319883826\n",
      "--------------------------------------\n",
      "Training logistic_regression with lda_topic_matrix features...\n",
      "Accuracy of logistic_regression with lda_topic_matrix features: 0.47604790419161674\n",
      "Precision of logistic_regression with lda_topic_matrix features: 0.5037435413747747\n",
      "Recall of logistic_regression with lda_topic_matrix features: 0.5151411074852456\n",
      "F1 Score of logistic_regression with lda_topic_matrix features: 0.44473281210260235\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through each feature set\n",
    "for feature_key, feature_name in feature_set.items():\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features[feature_key], y, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Oversample to balance the classes\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # change -1 to 2 because of xbg\n",
    "    y_train_resampled_series = pd.Series(y_train_resampled)\n",
    "    y_test_series = pd.Series(y_test)\n",
    "    y_train_resampled_mapped = y_train_resampled_series.map({-1: 0, 0: 1, 1: 2})\n",
    "    y_test_mapped = y_test_series.map({-1: 0, 0: 1, 1: 2})\n",
    "\n",
    "\n",
    "    print(f\"Using {feature_name} features:\")\n",
    "    \n",
    "    # Loop through each classifier\n",
    "    for clf_name, clf in best_estimators.items():\n",
    "        print(f\"Training {clf_name} with {feature_name} features...\")   \n",
    "        \n",
    "        if clf_name == 'multinomial_nb' and feature_key in [4, 6, 7]:\n",
    "            print(f\"Skipping {clf_name} with {feature_name} features due to negative values or unsuitable feature type.\")\n",
    "            continue\n",
    "        # Convert sparse to dense if necessary\n",
    "        if (feature_key != 4 and feature_key != 6 and feature_key != 7) and clf_name in ['gaussian_nb', 'multinomial_nb', 'perceptron']:\n",
    "            X_train_dense = X_train_resampled.toarray()\n",
    "            X_test_dense = X_test.toarray()\n",
    "            clf.fit(X_train_dense, y_train_resampled)\n",
    "            y_pred = clf.predict(X_test_dense)\n",
    "        else:\n",
    "            if clf_name == 'xgb':\n",
    "                clf.fit(X_train_resampled, y_train_resampled_mapped)\n",
    "            else:\n",
    "                clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        if clf_name == 'xgb':\n",
    "            acc = accuracy_score(y_test_mapped, y_pred)\n",
    "            precision = precision_score(y_test_mapped, y_pred, average='macro')\n",
    "            recall = recall_score(y_test_mapped, y_pred, average='macro')\n",
    "            f1 = f1_score(y_test_mapped, y_pred, average='macro')\n",
    "        else:\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='macro')\n",
    "            recall = recall_score(y_test, y_pred, average='macro')\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        print(f\"Accuracy of {clf_name} with {feature_name} features: {acc}\")\n",
    "        print(f\"Precision of {clf_name} with {feature_name} features: {precision}\")\n",
    "        print(f\"Recall of {clf_name} with {feature_name} features: {recall}\")\n",
    "        print(f\"F1 Score of {clf_name} with {feature_name} features: {f1}\")\n",
    "            \n",
    "    # Append the results to the list\n",
    "        results.append({\n",
    "            'feature_set': feature_name,\n",
    "            'classifier': clf_name,\n",
    "            'accuracy': acc,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        })\n",
    "        print(\"--------------------------------------\")\n",
    "\n",
    "    print(\"-------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('classification_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
