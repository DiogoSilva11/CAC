{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from xgboost import XGBClassifier\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLzom-2TITa4gasV7_fCCA</td>\n",
       "      <td>Great experience purchasing a washer and dryer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a5JHzBrWxRd_OmIvV7znDA</td>\n",
       "      <td>Went here based on the high ratings and raves ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X-o--dwf0HuFMittYi4wCA</td>\n",
       "      <td>oh Millers, how i wanted to like you.  You are...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INGNbsyo-MouZZzcxnCSGQ</td>\n",
       "      <td>This place gets two stars from me only because...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>k7VatXVLism-cTDJE8TTUw</td>\n",
       "      <td>This place was awesome. Clean, beautiful and t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11681</th>\n",
       "      <td>IlU-MQzMKc7jAHWwK5VFGQ</td>\n",
       "      <td>To be fair, I tried them in their first week. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11682</th>\n",
       "      <td>Qt3BsRvQuJccDQfFWM1XPw</td>\n",
       "      <td>Awful place. It's dirty. Had two birthday part...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11683</th>\n",
       "      <td>3CQQ8Im_UX6QqDECuXYK8A</td>\n",
       "      <td>A truly vegetarian delight!  I took a Jewish f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11684</th>\n",
       "      <td>ery1nBM7zKweFLBe-bT5ag</td>\n",
       "      <td>I have a 2011 Toyota Sienna Limited. During th...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11685</th>\n",
       "      <td>N5_SaVzmwkZUslgWDGGsQQ</td>\n",
       "      <td>I'm a single father raising my 17 year old son...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11686 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    review_id  \\\n",
       "0      LLzom-2TITa4gasV7_fCCA   \n",
       "1      a5JHzBrWxRd_OmIvV7znDA   \n",
       "2      X-o--dwf0HuFMittYi4wCA   \n",
       "3      INGNbsyo-MouZZzcxnCSGQ   \n",
       "4      k7VatXVLism-cTDJE8TTUw   \n",
       "...                       ...   \n",
       "11681  IlU-MQzMKc7jAHWwK5VFGQ   \n",
       "11682  Qt3BsRvQuJccDQfFWM1XPw   \n",
       "11683  3CQQ8Im_UX6QqDECuXYK8A   \n",
       "11684  ery1nBM7zKweFLBe-bT5ag   \n",
       "11685  N5_SaVzmwkZUslgWDGGsQQ   \n",
       "\n",
       "                                                    text  sentiment  \n",
       "0      Great experience purchasing a washer and dryer...          1  \n",
       "1      Went here based on the high ratings and raves ...         -1  \n",
       "2      oh Millers, how i wanted to like you.  You are...         -1  \n",
       "3      This place gets two stars from me only because...         -1  \n",
       "4      This place was awesome. Clean, beautiful and t...          1  \n",
       "...                                                  ...        ...  \n",
       "11681  To be fair, I tried them in their first week. ...          0  \n",
       "11682  Awful place. It's dirty. Had two birthday part...         -1  \n",
       "11683  A truly vegetarian delight!  I took a Jewish f...          1  \n",
       "11684  I have a 2011 Toyota Sienna Limited. During th...         -1  \n",
       "11685  I'm a single father raising my 17 year old son...         -1  \n",
       "\n",
       "[11686 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load review sentiment data\n",
    "\n",
    "review_df = pd.read_csv('data/review_sentiment.csv')\n",
    "\n",
    "review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = {\n",
    "    0: 'bag_of_words',\n",
    "    1: 'one_hot',\n",
    "    2: 'n_grams',\n",
    "    3: 'tf_idf',\n",
    "    4: 'word2vec',\n",
    "    5: 'combined_bow_negation'\n",
    "}\n",
    "# Load all feature sets\n",
    "features = {}\n",
    "for key, feature_name in feature_set.items():\n",
    "    if key == 4:\n",
    "        features[key] = np.load('features/' + feature_name + '.npy')\n",
    "    else:\n",
    "        features[key] = sparse.load_npz('features/' + feature_name + '.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11686,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target labels\n",
    "\n",
    "y = review_df['sentiment'].to_numpy()\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers\n",
    "\n",
    "classifiers = {\n",
    "    'gaussian_nb': GaussianNB(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'svm': SVC(),\n",
    "    'perceptron': Perceptron(tol=1e-3, random_state=0),\n",
    "    'xgb': XGBClassifier(),\n",
    "    'logistic_regression': LogisticRegression(max_iter=1000, random_state=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'decision_tree': {\n",
    "        'max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1.0],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'perceptron': {\n",
    "        'alpha': [0.0001, 0.001],\n",
    "        'penalty': [None, 'l2']\n",
    "    },\n",
    "    'xgb': {\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'n_estimators': [100, 200]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'C': [0.1, 1.0],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec features for grid search\n",
    "word2vec_features = features[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets using word2vec features\n",
    "X_train_w2v, X_test_w2v, y_train, y_test = train_test_split(word2vec_features, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# oversampler = RandomOverSampler(random_state=42)\n",
    "# X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_w2v, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb algorithm expects [0 1 2], not [-1 0 1]\n",
    "\n",
    "# Convert numpy arrays to pandas Series\n",
    "y_train_series = pd.Series(y_train)\n",
    "y_test_series = pd.Series(y_test)\n",
    "\n",
    "# Remap labels: -1 -> 0, 0 -> 1, 1 -> 2\n",
    "y_train_mapped = y_train_series.map({-1: 0, 0: 1, 1: 2})\n",
    "y_test_mapped = y_test_series.map({-1: 0, 0: 1, 1: 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for gaussian_nb\n",
      "Best parameters for gaussian_nb: {}\n",
      "Best score for gaussian_nb: 0.5065980589216799\n",
      "---------------------------------------------\n",
      "Grid search for decision_tree\n",
      "Best parameters for decision_tree: {'max_depth': 10}\n",
      "Best score for decision_tree: 0.5306139337130708\n",
      "---------------------------------------------\n",
      "Grid search for random_forest\n",
      "Best parameters for random_forest: {'max_depth': None, 'n_estimators': 200}\n",
      "Best score for random_forest: 0.623274152507785\n",
      "---------------------------------------------\n",
      "Grid search for svm\n",
      "Best parameters for svm: {'C': 1.0, 'kernel': 'rbf'}\n",
      "Best score for svm: 0.7215333767954311\n",
      "---------------------------------------------\n",
      "Grid search for perceptron\n",
      "Best parameters for perceptron: {'alpha': 0.0001, 'penalty': None}\n",
      "Best score for perceptron: 0.7033121484291686\n",
      "---------------------------------------------\n",
      "Grid search for xgb\n",
      "Best parameters for xgb: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "Best score for xgb: 0.6850281456634859\n",
      "---------------------------------------------\n",
      "Grid search for logistic_regression\n",
      "Best parameters for logistic_regression: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best score for logistic_regression: 0.7217286577375768\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_estimators = {}\n",
    "grid_results = []\n",
    "\n",
    "# Apply grid search to each classifier using word2vec features\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"Grid search for {clf_name}\")\n",
    "    param_grid = param_grids[clf_name]\n",
    "    if clf_name == 'xgb':\n",
    "        grid_search = GridSearchCV(clf, param_grid, scoring='f1_weighted', cv=5)\n",
    "        grid_search.fit(X_train_w2v, y_train_mapped)\n",
    "    else:\n",
    "        grid_search = GridSearchCV(clf, param_grid, scoring='f1_weighted', cv=5)\n",
    "        grid_search.fit(X_train_w2v, y_train)\n",
    "    best_estimators[clf_name] = grid_search.best_estimator_\n",
    "    grid_results.append({\n",
    "        'classifier': clf_name,\n",
    "        'best_parameters': grid_search.best_params_,\n",
    "        'best_f1': grid_search.best_score_,\n",
    "    })\n",
    "    print(f\"Best parameters for {clf_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {clf_name}: {grid_search.best_score_}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "grid_results_df = pd.DataFrame(grid_results)\n",
    "grid_results_df.to_csv('grid_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators = {\n",
    "    'multinomial_nb': MultinomialNB(),\n",
    "    'xgb': XGBClassifier(learning_rate=0.1, max_depth=5, n_estimators=200),\n",
    "    'decision_tree': DecisionTreeClassifier(max_depth=10),\n",
    "    'random_forest': RandomForestClassifier(max_depth=None, n_estimators=200),\n",
    "    'svm': SVC(C=1.0, kernel='rbf'),\n",
    "    'perceptron': Perceptron(tol=1e-3, random_state=0, alpha=0.0001, penalty=None),\n",
    "    'logistic_regression': LogisticRegression(C=1.0, penalty='l2', solver='liblinear', max_iter=1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate all models for all feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bag_of_words features:\n",
      "Training multinomial_nb with bag_of_words features...\n",
      "Precision of multinomial_nb with bag_of_words features: 0.6607395889655369\n",
      "Recall of multinomial_nb with bag_of_words features: 0.6411506150757784\n",
      "F1 Score of multinomial_nb with bag_of_words features: 0.6438441506749296\n",
      "Accuracy of multinomial_nb with bag_of_words features: 0.7976903336184773\n",
      "--------------------------------------\n",
      "Training xgb with bag_of_words features...\n",
      "Precision of xgb with bag_of_words features: 0.7424430262446066\n",
      "Recall of xgb with bag_of_words features: 0.6458782998812872\n",
      "F1 Score of xgb with bag_of_words features: 0.6495154640549461\n",
      "Accuracy of xgb with bag_of_words features: 0.8122326775021386\n",
      "--------------------------------------\n",
      "Training decision_tree with bag_of_words features...\n",
      "Precision of decision_tree with bag_of_words features: 0.5579153751762319\n",
      "Recall of decision_tree with bag_of_words features: 0.5260026364165876\n",
      "F1 Score of decision_tree with bag_of_words features: 0.5077980589367398\n",
      "Accuracy of decision_tree with bag_of_words features: 0.6852010265183918\n",
      "--------------------------------------\n",
      "Training random_forest with bag_of_words features...\n",
      "Precision of random_forest with bag_of_words features: 0.7890027469036536\n",
      "Recall of random_forest with bag_of_words features: 0.6054178678007275\n",
      "F1 Score of random_forest with bag_of_words features: 0.5755155142509908\n",
      "Accuracy of random_forest with bag_of_words features: 0.8045337895637297\n",
      "--------------------------------------\n",
      "Training svm with bag_of_words features...\n",
      "Precision of svm with bag_of_words features: 0.7055055738433795\n",
      "Recall of svm with bag_of_words features: 0.6126590280354468\n",
      "F1 Score of svm with bag_of_words features: 0.582655648684771\n",
      "Accuracy of svm with bag_of_words features: 0.8083832335329342\n",
      "--------------------------------------\n",
      "Training perceptron with bag_of_words features...\n",
      "Precision of perceptron with bag_of_words features: 0.6650907836585246\n",
      "Recall of perceptron with bag_of_words features: 0.6568539960104908\n",
      "F1 Score of perceptron with bag_of_words features: 0.6598964781766355\n",
      "Accuracy of perceptron with bag_of_words features: 0.7925577416595381\n",
      "--------------------------------------\n",
      "Training logistic_regression with bag_of_words features...\n",
      "Precision of logistic_regression with bag_of_words features: 0.693669262406417\n",
      "Recall of logistic_regression with bag_of_words features: 0.6672158320304531\n",
      "F1 Score of logistic_regression with bag_of_words features: 0.6728720165751235\n",
      "Accuracy of logistic_regression with bag_of_words features: 0.8147989734816082\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using one_hot features:\n",
      "Training multinomial_nb with one_hot features...\n",
      "Precision of multinomial_nb with one_hot features: 0.6507525365275978\n",
      "Recall of multinomial_nb with one_hot features: 0.6279279393560758\n",
      "F1 Score of multinomial_nb with one_hot features: 0.6231323177660472\n",
      "Accuracy of multinomial_nb with one_hot features: 0.8023952095808383\n",
      "--------------------------------------\n",
      "Training xgb with one_hot features...\n",
      "Precision of xgb with one_hot features: 0.7326891283998725\n",
      "Recall of xgb with one_hot features: 0.6432837445995215\n",
      "F1 Score of xgb with one_hot features: 0.6455090119939276\n",
      "Accuracy of xgb with one_hot features: 0.8109495295124037\n",
      "--------------------------------------\n",
      "Training decision_tree with one_hot features...\n",
      "Precision of decision_tree with one_hot features: 0.5865293842067044\n",
      "Recall of decision_tree with one_hot features: 0.5300573690169252\n",
      "F1 Score of decision_tree with one_hot features: 0.5156136828865239\n",
      "Accuracy of decision_tree with one_hot features: 0.6856287425149701\n",
      "--------------------------------------\n",
      "Training random_forest with one_hot features...\n",
      "Precision of random_forest with one_hot features: 0.7876038720365218\n",
      "Recall of random_forest with one_hot features: 0.6041043107631126\n",
      "F1 Score of random_forest with one_hot features: 0.5741805406297308\n",
      "Accuracy of random_forest with one_hot features: 0.8028229255774166\n",
      "--------------------------------------\n",
      "Training svm with one_hot features...\n",
      "Precision of svm with one_hot features: 0.7473158665482992\n",
      "Recall of svm with one_hot features: 0.6241390683322392\n",
      "F1 Score of svm with one_hot features: 0.5995125447544289\n",
      "Accuracy of svm with one_hot features: 0.8195038494439693\n",
      "--------------------------------------\n",
      "Training perceptron with one_hot features...\n",
      "Precision of perceptron with one_hot features: 0.67824178575302\n",
      "Recall of perceptron with one_hot features: 0.6641708448689773\n",
      "F1 Score of perceptron with one_hot features: 0.6691116277734723\n",
      "Accuracy of perceptron with one_hot features: 0.7989734816082121\n",
      "--------------------------------------\n",
      "Training logistic_regression with one_hot features...\n",
      "Precision of logistic_regression with one_hot features: 0.7209481702255834\n",
      "Recall of logistic_regression with one_hot features: 0.6839123618761899\n",
      "F1 Score of logistic_regression with one_hot features: 0.6930119946673138\n",
      "Accuracy of logistic_regression with one_hot features: 0.823781009409752\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using n_grams features:\n",
      "Training multinomial_nb with n_grams features...\n",
      "Precision of multinomial_nb with n_grams features: 0.6951673512769866\n",
      "Recall of multinomial_nb with n_grams features: 0.7129529028595712\n",
      "F1 Score of multinomial_nb with n_grams features: 0.69730854722543\n",
      "Accuracy of multinomial_nb with n_grams features: 0.7844311377245509\n",
      "--------------------------------------\n",
      "Training xgb with n_grams features...\n",
      "Precision of xgb with n_grams features: 0.732488115280038\n",
      "Recall of xgb with n_grams features: 0.6473113096351829\n",
      "F1 Score of xgb with n_grams features: 0.6509575759364077\n",
      "Accuracy of xgb with n_grams features: 0.8126603934987169\n",
      "--------------------------------------\n",
      "Training decision_tree with n_grams features...\n",
      "Precision of decision_tree with n_grams features: 0.5763812576312576\n",
      "Recall of decision_tree with n_grams features: 0.530030201452249\n",
      "F1 Score of decision_tree with n_grams features: 0.5147333884385109\n",
      "Accuracy of decision_tree with n_grams features: 0.6869118905047049\n",
      "--------------------------------------\n",
      "Training random_forest with n_grams features...\n",
      "Precision of random_forest with n_grams features: 0.7887789079315759\n",
      "Recall of random_forest with n_grams features: 0.6072851357479695\n",
      "F1 Score of random_forest with n_grams features: 0.5764813327729396\n",
      "Accuracy of random_forest with n_grams features: 0.8062446535500428\n",
      "--------------------------------------\n",
      "Training svm with n_grams features...\n",
      "Precision of svm with n_grams features: 0.7775946441473648\n",
      "Recall of svm with n_grams features: 0.6142494405278753\n",
      "F1 Score of svm with n_grams features: 0.583859953812078\n",
      "Accuracy of svm with n_grams features: 0.8100940975192472\n",
      "--------------------------------------\n",
      "Training perceptron with n_grams features...\n",
      "Precision of perceptron with n_grams features: 0.7003481495293146\n",
      "Recall of perceptron with n_grams features: 0.6846073018379862\n",
      "F1 Score of perceptron with n_grams features: 0.6899491740195586\n",
      "Accuracy of perceptron with n_grams features: 0.8147989734816082\n",
      "--------------------------------------\n",
      "Training logistic_regression with n_grams features...\n",
      "Precision of logistic_regression with n_grams features: 0.711507080293456\n",
      "Recall of logistic_regression with n_grams features: 0.6765573515747145\n",
      "F1 Score of logistic_regression with n_grams features: 0.6837203309289607\n",
      "Accuracy of logistic_regression with n_grams features: 0.8242087254063302\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using tf_idf features:\n",
      "Training multinomial_nb with tf_idf features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of multinomial_nb with tf_idf features: 0.5447854105522766\n",
      "Recall of multinomial_nb with tf_idf features: 0.5905827918319712\n",
      "F1 Score of multinomial_nb with tf_idf features: 0.5622180475094294\n",
      "Accuracy of multinomial_nb with tf_idf features: 0.7946963216424294\n",
      "--------------------------------------\n",
      "Training xgb with tf_idf features...\n",
      "Precision of xgb with tf_idf features: 0.7401572460207285\n",
      "Recall of xgb with tf_idf features: 0.6488583808765714\n",
      "F1 Score of xgb with tf_idf features: 0.6511038845043191\n",
      "Accuracy of xgb with tf_idf features: 0.8143712574850299\n",
      "--------------------------------------\n",
      "Training decision_tree with tf_idf features...\n",
      "Precision of decision_tree with tf_idf features: 0.6001096491228071\n",
      "Recall of decision_tree with tf_idf features: 0.5416411111851084\n",
      "F1 Score of decision_tree with tf_idf features: 0.5240468616156232\n",
      "Accuracy of decision_tree with tf_idf features: 0.693327630453379\n",
      "--------------------------------------\n",
      "Training random_forest with tf_idf features...\n",
      "Precision of random_forest with tf_idf features: 0.7876398727344341\n",
      "Recall of random_forest with tf_idf features: 0.6033552470927006\n",
      "F1 Score of random_forest with tf_idf features: 0.5737611471561895\n",
      "Accuracy of random_forest with tf_idf features: 0.80196749358426\n",
      "--------------------------------------\n",
      "Training svm with tf_idf features...\n",
      "Precision of svm with tf_idf features: 0.7995362789734775\n",
      "Recall of svm with tf_idf features: 0.638713039691072\n",
      "F1 Score of svm with tf_idf features: 0.6229510570972975\n",
      "Accuracy of svm with tf_idf features: 0.8314798973481609\n",
      "--------------------------------------\n",
      "Training perceptron with tf_idf features...\n",
      "Precision of perceptron with tf_idf features: 0.6917316209447462\n",
      "Recall of perceptron with tf_idf features: 0.6512140307257757\n",
      "F1 Score of perceptron with tf_idf features: 0.6511585127817324\n",
      "Accuracy of perceptron with tf_idf features: 0.8182207014542344\n",
      "--------------------------------------\n",
      "Training logistic_regression with tf_idf features...\n",
      "Precision of logistic_regression with tf_idf features: 0.7281494463070203\n",
      "Recall of logistic_regression with tf_idf features: 0.6383928429852185\n",
      "F1 Score of logistic_regression with tf_idf features: 0.6243973508570634\n",
      "Accuracy of logistic_regression with tf_idf features: 0.8284858853721129\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using word2vec features:\n",
      "Training multinomial_nb with word2vec features...\n",
      "Skipping multinomial_nb with word2vec features due to negative values or unsuitable feature type.\n",
      "Training xgb with word2vec features...\n",
      "Precision of xgb with word2vec features: 0.6123135529255402\n",
      "Recall of xgb with word2vec features: 0.5330036226943248\n",
      "F1 Score of xgb with word2vec features: 0.5068112574813474\n",
      "Accuracy of xgb with word2vec features: 0.7151411462788708\n",
      "--------------------------------------\n",
      "Training decision_tree with word2vec features...\n",
      "Precision of decision_tree with word2vec features: 0.43115454422307486\n",
      "Recall of decision_tree with word2vec features: 0.42092283037397155\n",
      "F1 Score of decision_tree with word2vec features: 0.41073368792337295\n",
      "Accuracy of decision_tree with word2vec features: 0.5560307955517536\n",
      "--------------------------------------\n",
      "Training random_forest with word2vec features...\n",
      "Precision of random_forest with word2vec features: 0.6712239528372237\n",
      "Recall of random_forest with word2vec features: 0.48692003327762395\n",
      "F1 Score of random_forest with word2vec features: 0.4644980561949342\n",
      "Accuracy of random_forest with word2vec features: 0.6612489307100086\n",
      "--------------------------------------\n",
      "Training svm with word2vec features...\n",
      "Precision of svm with word2vec features: 0.8395158767045472\n",
      "Recall of svm with word2vec features: 0.5682962977057678\n",
      "F1 Score of svm with word2vec features: 0.5370498860368055\n",
      "Accuracy of svm with word2vec features: 0.7583404619332763\n",
      "--------------------------------------\n",
      "Training perceptron with word2vec features...\n",
      "Precision of perceptron with word2vec features: 0.5789757449921015\n",
      "Recall of perceptron with word2vec features: 0.5720155055968354\n",
      "F1 Score of perceptron with word2vec features: 0.5736911523441502\n",
      "Accuracy of perceptron with word2vec features: 0.7018819503849444\n",
      "--------------------------------------\n",
      "Training logistic_regression with word2vec features...\n",
      "Precision of logistic_regression with word2vec features: 0.5988310451011416\n",
      "Recall of logistic_regression with word2vec features: 0.5882180508910856\n",
      "F1 Score of logistic_regression with word2vec features: 0.5901855088116977\n",
      "Accuracy of logistic_regression with word2vec features: 0.7177074422583405\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using combined_bow_negation features:\n",
      "Training multinomial_nb with combined_bow_negation features...\n",
      "Precision of multinomial_nb with combined_bow_negation features: 0.6644843814772554\n",
      "Recall of multinomial_nb with combined_bow_negation features: 0.64674681626798\n",
      "F1 Score of multinomial_nb with combined_bow_negation features: 0.6487211345654349\n",
      "Accuracy of multinomial_nb with combined_bow_negation features: 0.8058169375534645\n",
      "--------------------------------------\n",
      "Training xgb with combined_bow_negation features...\n",
      "Precision of xgb with combined_bow_negation features: 0.7165861931710018\n",
      "Recall of xgb with combined_bow_negation features: 0.6402982723754495\n",
      "F1 Score of xgb with combined_bow_negation features: 0.6382715538449512\n",
      "Accuracy of xgb with combined_bow_negation features: 0.8143712574850299\n",
      "--------------------------------------\n",
      "Training decision_tree with combined_bow_negation features...\n",
      "Precision of decision_tree with combined_bow_negation features: 0.6127407491035405\n",
      "Recall of decision_tree with combined_bow_negation features: 0.5834351852732772\n",
      "F1 Score of decision_tree with combined_bow_negation features: 0.5750088549433113\n",
      "Accuracy of decision_tree with combined_bow_negation features: 0.7523524379811805\n",
      "--------------------------------------\n",
      "Training random_forest with combined_bow_negation features...\n",
      "Precision of random_forest with combined_bow_negation features: 0.7907107868311568\n",
      "Recall of random_forest with combined_bow_negation features: 0.60850640763398\n",
      "F1 Score of random_forest with combined_bow_negation features: 0.5780033481948696\n",
      "Accuracy of random_forest with combined_bow_negation features: 0.8079555175363559\n",
      "--------------------------------------\n",
      "Training svm with combined_bow_negation features...\n",
      "Precision of svm with combined_bow_negation features: 0.6584497002094669\n",
      "Recall of svm with combined_bow_negation features: 0.608278496079731\n",
      "F1 Score of svm with combined_bow_negation features: 0.5806798828660429\n",
      "Accuracy of svm with combined_bow_negation features: 0.804961505560308\n",
      "--------------------------------------\n",
      "Training perceptron with combined_bow_negation features...\n",
      "Precision of perceptron with combined_bow_negation features: 0.6687085114466168\n",
      "Recall of perceptron with combined_bow_negation features: 0.6701094419443941\n",
      "F1 Score of perceptron with combined_bow_negation features: 0.6691839116588786\n",
      "Accuracy of perceptron with combined_bow_negation features: 0.7865697177074422\n",
      "--------------------------------------\n",
      "Training logistic_regression with combined_bow_negation features...\n",
      "Precision of logistic_regression with combined_bow_negation features: 0.6988522882562617\n",
      "Recall of logistic_regression with combined_bow_negation features: 0.6745598484536158\n",
      "F1 Score of logistic_regression with combined_bow_negation features: 0.6803958364670623\n",
      "Accuracy of logistic_regression with combined_bow_negation features: 0.8199315654405475\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using lsa_topic_matrix features:\n",
      "Training multinomial_nb with lsa_topic_matrix features...\n",
      "Skipping multinomial_nb with lsa_topic_matrix features due to negative values or unsuitable feature type.\n",
      "Training xgb with lsa_topic_matrix features...\n",
      "Precision of xgb with lsa_topic_matrix features: 0.5601004723186515\n",
      "Recall of xgb with lsa_topic_matrix features: 0.5146901470536934\n",
      "F1 Score of xgb with lsa_topic_matrix features: 0.5016478055778185\n",
      "Accuracy of xgb with lsa_topic_matrix features: 0.6775021385799829\n",
      "--------------------------------------\n",
      "Training decision_tree with lsa_topic_matrix features...\n",
      "Precision of decision_tree with lsa_topic_matrix features: 0.46740309248996287\n",
      "Recall of decision_tree with lsa_topic_matrix features: 0.4749252891971403\n",
      "F1 Score of decision_tree with lsa_topic_matrix features: 0.4628385424991868\n",
      "Accuracy of decision_tree with lsa_topic_matrix features: 0.6266039349871685\n",
      "--------------------------------------\n",
      "Training random_forest with lsa_topic_matrix features...\n",
      "Precision of random_forest with lsa_topic_matrix features: 0.5789119009363726\n",
      "Recall of random_forest with lsa_topic_matrix features: 0.5060054060282394\n",
      "F1 Score of random_forest with lsa_topic_matrix features: 0.4875345337088534\n",
      "Accuracy of random_forest with lsa_topic_matrix features: 0.6719418306244653\n",
      "--------------------------------------\n",
      "Training svm with lsa_topic_matrix features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of svm with lsa_topic_matrix features: 0.4482677394960919\n",
      "Recall of svm with lsa_topic_matrix features: 0.4906763455079964\n",
      "F1 Score of svm with lsa_topic_matrix features: 0.46457438832341413\n",
      "Accuracy of svm with lsa_topic_matrix features: 0.6659538066723696\n",
      "--------------------------------------\n",
      "Training perceptron with lsa_topic_matrix features...\n",
      "Precision of perceptron with lsa_topic_matrix features: 0.42667303796437067\n",
      "Recall of perceptron with lsa_topic_matrix features: 0.4422297065163043\n",
      "F1 Score of perceptron with lsa_topic_matrix features: 0.41879524370686066\n",
      "Accuracy of perceptron with lsa_topic_matrix features: 0.5585970915312233\n",
      "--------------------------------------\n",
      "Training logistic_regression with lsa_topic_matrix features...\n",
      "Precision of logistic_regression with lsa_topic_matrix features: 0.5824204955526814\n",
      "Recall of logistic_regression with lsa_topic_matrix features: 0.4862794284451802\n",
      "F1 Score of logistic_regression with lsa_topic_matrix features: 0.463657911688615\n",
      "Accuracy of logistic_regression with lsa_topic_matrix features: 0.6616766467065869\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Using lda_topic_matrix features:\n",
      "Training multinomial_nb with lda_topic_matrix features...\n",
      "Skipping multinomial_nb with lda_topic_matrix features due to negative values or unsuitable feature type.\n",
      "Training xgb with lda_topic_matrix features...\n",
      "Precision of xgb with lda_topic_matrix features: 0.512953406867488\n",
      "Recall of xgb with lda_topic_matrix features: 0.4929998594052101\n",
      "F1 Score of xgb with lda_topic_matrix features: 0.47314222158546454\n",
      "Accuracy of xgb with lda_topic_matrix features: 0.6608212147134302\n",
      "--------------------------------------\n",
      "Training decision_tree with lda_topic_matrix features...\n",
      "Precision of decision_tree with lda_topic_matrix features: 0.4970797254028911\n",
      "Recall of decision_tree with lda_topic_matrix features: 0.48072202295817784\n",
      "F1 Score of decision_tree with lda_topic_matrix features: 0.46970692685019744\n",
      "Accuracy of decision_tree with lda_topic_matrix features: 0.6364414029084687\n",
      "--------------------------------------\n",
      "Training random_forest with lda_topic_matrix features...\n",
      "Precision of random_forest with lda_topic_matrix features: 0.5179599061984965\n",
      "Recall of random_forest with lda_topic_matrix features: 0.4988241835723974\n",
      "F1 Score of random_forest with lda_topic_matrix features: 0.48713745137831893\n",
      "Accuracy of random_forest with lda_topic_matrix features: 0.6595380667236954\n",
      "--------------------------------------\n",
      "Training svm with lda_topic_matrix features...\n",
      "Precision of svm with lda_topic_matrix features: 0.4449932911036914\n",
      "Recall of svm with lda_topic_matrix features: 0.4936132966729776\n",
      "F1 Score of svm with lda_topic_matrix features: 0.46656363882459173\n",
      "Accuracy of svm with lda_topic_matrix features: 0.6646706586826348\n",
      "--------------------------------------\n",
      "Training perceptron with lda_topic_matrix features...\n",
      "Precision of perceptron with lda_topic_matrix features: 0.41924274405220824\n",
      "Recall of perceptron with lda_topic_matrix features: 0.45946398500604135\n",
      "F1 Score of perceptron with lda_topic_matrix features: 0.4100527407764282\n",
      "Accuracy of perceptron with lda_topic_matrix features: 0.5838323353293413\n",
      "--------------------------------------\n",
      "Training logistic_regression with lda_topic_matrix features...\n",
      "Precision of logistic_regression with lda_topic_matrix features: 0.44463704716336294\n",
      "Recall of logistic_regression with lda_topic_matrix features: 0.4927665566221731\n",
      "F1 Score of logistic_regression with lda_topic_matrix features: 0.46591480844972627\n",
      "Accuracy of logistic_regression with lda_topic_matrix features: 0.6633875106928999\n",
      "--------------------------------------\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\bruna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through each feature set\n",
    "for feature_key, feature_name in feature_set.items():\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features[feature_key], y, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Oversample to balance the classes\n",
    "    # oversampler = RandomOverSampler(random_state=42)\n",
    "    # X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # change -1 to 2 because of xbg\n",
    "    y_train_series = pd.Series(y_train)\n",
    "    y_test_series = pd.Series(y_test)\n",
    "    y_train_mapped = y_train_series.map({-1: 0, 0: 1, 1: 2})\n",
    "    y_test_mapped = y_test_series.map({-1: 0, 0: 1, 1: 2})\n",
    "\n",
    "\n",
    "    print(f\"Using {feature_name} features:\")\n",
    "    \n",
    "    # Loop through each classifier\n",
    "    for clf_name, clf in best_estimators.items():\n",
    "        print(f\"Training {clf_name} with {feature_name} features...\")   \n",
    "        \n",
    "        if clf_name == 'multinomial_nb' and feature_key == 4:\n",
    "            print(f\"Skipping {clf_name} with {feature_name} features due to negative values or unsuitable feature type.\")\n",
    "            continue\n",
    "        # Convert sparse to dense if necessary\n",
    "        if feature_key != 4 and clf_name in ['gaussian_nb', 'multinomial_nb', 'perceptron']:\n",
    "            X_train_dense = X_train.toarray()\n",
    "            X_test_dense = X_test.toarray()\n",
    "            clf.fit(X_train_dense, y_train)\n",
    "            y_pred = clf.predict(X_test_dense)\n",
    "        else:\n",
    "            if clf_name == 'xgb':\n",
    "                clf.fit(X_train, y_train_mapped)\n",
    "            else:\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        if clf_name == 'xgb':\n",
    "            acc = accuracy_score(y_test_mapped, y_pred)\n",
    "            precision = precision_score(y_test_mapped, y_pred, average='macro')\n",
    "            recall = recall_score(y_test_mapped, y_pred, average='macro')\n",
    "            f1 = f1_score(y_test_mapped, y_pred, average='macro')\n",
    "        else:\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='macro')\n",
    "            recall = recall_score(y_test, y_pred, average='macro')\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        print(f\"Precision of {clf_name} with {feature_name} features: {precision}\")\n",
    "        print(f\"Recall of {clf_name} with {feature_name} features: {recall}\")\n",
    "        print(f\"F1 Score of {clf_name} with {feature_name} features: {f1}\")\n",
    "        print(f\"Accuracy of {clf_name} with {feature_name} features: {acc}\")\n",
    "            \n",
    "    # Append the results to the list\n",
    "        results.append({\n",
    "            'feature_set': feature_name,\n",
    "            'classifier': clf_name,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'accuracy': acc\n",
    "        })\n",
    "        print(\"--------------------------------------\")\n",
    "\n",
    "    print(\"-------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert the results list to a DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(results)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the DataFrame to a CSV file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m results_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('classification_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg': 0.045, 'neu': 0.762, 'pos': 0.193, 'compound': 0.9762},\n",
       " {'neg': 0.063, 'neu': 0.891, 'pos': 0.046, 'compound': -0.4508},\n",
       " {'neg': 0.049, 'neu': 0.842, 'pos': 0.109, 'compound': 0.979},\n",
       " {'neg': 0.133, 'neu': 0.793, 'pos': 0.074, 'compound': -0.566},\n",
       " {'neg': 0.0, 'neu': 0.542, 'pos': 0.458, 'compound': 0.9871},\n",
       " {'neg': 0.086, 'neu': 0.881, 'pos': 0.034, 'compound': -0.6124},\n",
       " {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.9808},\n",
       " {'neg': 0.0, 'neu': 0.751, 'pos': 0.249, 'compound': 0.807},\n",
       " {'neg': 0.096, 'neu': 0.786, 'pos': 0.117, 'compound': 0.4547},\n",
       " {'neg': 0.029, 'neu': 0.812, 'pos': 0.159, 'compound': 0.9728},\n",
       " {'neg': 0.015, 'neu': 0.901, 'pos': 0.085, 'compound': 0.8786},\n",
       " {'neg': 0.023, 'neu': 0.901, 'pos': 0.076, 'compound': 0.9761},\n",
       " {'neg': 0.021, 'neu': 0.615, 'pos': 0.364, 'compound': 0.9858},\n",
       " {'neg': 0.0, 'neu': 0.799, 'pos': 0.201, 'compound': 0.8775},\n",
       " {'neg': 0.0, 'neu': 0.592, 'pos': 0.408, 'compound': 0.9803},\n",
       " {'neg': 0.0, 'neu': 0.583, 'pos': 0.417, 'compound': 0.8941},\n",
       " {'neg': 0.087, 'neu': 0.833, 'pos': 0.08, 'compound': -0.5795},\n",
       " {'neg': 0.0, 'neu': 0.721, 'pos': 0.279, 'compound': 0.9531},\n",
       " {'neg': 0.033, 'neu': 0.823, 'pos': 0.143, 'compound': 0.8481},\n",
       " {'neg': 0.049, 'neu': 0.835, 'pos': 0.115, 'compound': 0.866},\n",
       " {'neg': 0.0, 'neu': 0.583, 'pos': 0.417, 'compound': 0.9358},\n",
       " {'neg': 0.0, 'neu': 0.757, 'pos': 0.243, 'compound': 0.9521},\n",
       " {'neg': 0.016, 'neu': 0.724, 'pos': 0.261, 'compound': 0.971},\n",
       " {'neg': 0.0, 'neu': 0.826, 'pos': 0.174, 'compound': 0.714},\n",
       " {'neg': 0.0, 'neu': 0.56, 'pos': 0.44, 'compound': 0.9001},\n",
       " {'neg': 0.069, 'neu': 0.816, 'pos': 0.115, 'compound': 0.9042},\n",
       " {'neg': 0.057, 'neu': 0.899, 'pos': 0.044, 'compound': -0.3291},\n",
       " {'neg': 0.046, 'neu': 0.954, 'pos': 0.0, 'compound': -0.296},\n",
       " {'neg': 0.041, 'neu': 0.845, 'pos': 0.114, 'compound': 0.5927},\n",
       " {'neg': 0.0, 'neu': 0.685, 'pos': 0.315, 'compound': 0.971},\n",
       " {'neg': 0.074, 'neu': 0.791, 'pos': 0.135, 'compound': 0.9295},\n",
       " {'neg': 0.09, 'neu': 0.862, 'pos': 0.048, 'compound': -0.8389},\n",
       " {'neg': 0.041, 'neu': 0.8, 'pos': 0.159, 'compound': 0.9267},\n",
       " {'neg': 0.088, 'neu': 0.843, 'pos': 0.07, 'compound': -0.2455},\n",
       " {'neg': 0.032, 'neu': 0.72, 'pos': 0.247, 'compound': 0.9968},\n",
       " {'neg': 0.0, 'neu': 0.772, 'pos': 0.228, 'compound': 0.8917},\n",
       " {'neg': 0.078, 'neu': 0.843, 'pos': 0.079, 'compound': 0.043},\n",
       " {'neg': 0.117, 'neu': 0.676, 'pos': 0.207, 'compound': 0.7391},\n",
       " {'neg': 0.0, 'neu': 0.668, 'pos': 0.332, 'compound': 0.9714},\n",
       " {'neg': 0.018, 'neu': 0.704, 'pos': 0.279, 'compound': 0.9834},\n",
       " {'neg': 0.053, 'neu': 0.664, 'pos': 0.283, 'compound': 0.9888},\n",
       " {'neg': 0.049, 'neu': 0.775, 'pos': 0.176, 'compound': 0.9184},\n",
       " {'neg': 0.058, 'neu': 0.843, 'pos': 0.099, 'compound': 0.8722},\n",
       " {'neg': 0.019, 'neu': 0.696, 'pos': 0.285, 'compound': 0.9843},\n",
       " {'neg': 0.04, 'neu': 0.71, 'pos': 0.249, 'compound': 0.8975},\n",
       " {'neg': 0.0, 'neu': 0.877, 'pos': 0.123, 'compound': 0.8555},\n",
       " {'neg': 0.113, 'neu': 0.831, 'pos': 0.056, 'compound': -0.9831},\n",
       " {'neg': 0.058, 'neu': 0.773, 'pos': 0.169, 'compound': 0.6124},\n",
       " {'neg': 0.137, 'neu': 0.697, 'pos': 0.166, 'compound': 0.3908},\n",
       " {'neg': 0.0, 'neu': 0.613, 'pos': 0.387, 'compound': 0.9382},\n",
       " {'neg': 0.07, 'neu': 0.807, 'pos': 0.123, 'compound': 0.34},\n",
       " {'neg': 0.057, 'neu': 0.572, 'pos': 0.371, 'compound': 0.91},\n",
       " {'neg': 0.1, 'neu': 0.868, 'pos': 0.032, 'compound': -0.743},\n",
       " {'neg': 0.0, 'neu': 0.493, 'pos': 0.507, 'compound': 0.9468},\n",
       " {'neg': 0.071, 'neu': 0.752, 'pos': 0.177, 'compound': 0.9097},\n",
       " {'neg': 0.043, 'neu': 0.868, 'pos': 0.089, 'compound': 0.3612},\n",
       " {'neg': 0.0, 'neu': 0.595, 'pos': 0.405, 'compound': 0.9658},\n",
       " {'neg': 0.0, 'neu': 0.687, 'pos': 0.313, 'compound': 0.966},\n",
       " {'neg': 0.04, 'neu': 0.688, 'pos': 0.272, 'compound': 0.992},\n",
       " {'neg': 0.214, 'neu': 0.657, 'pos': 0.129, 'compound': -0.9092},\n",
       " {'neg': 0.021, 'neu': 0.799, 'pos': 0.181, 'compound': 0.9596},\n",
       " {'neg': 0.0, 'neu': 0.831, 'pos': 0.169, 'compound': 0.9473},\n",
       " {'neg': 0.096, 'neu': 0.756, 'pos': 0.149, 'compound': 0.4865},\n",
       " {'neg': 0.099, 'neu': 0.677, 'pos': 0.224, 'compound': 0.8716},\n",
       " {'neg': 0.0, 'neu': 0.709, 'pos': 0.291, 'compound': 0.9705},\n",
       " {'neg': 0.057, 'neu': 0.729, 'pos': 0.214, 'compound': 0.7911},\n",
       " {'neg': 0.215, 'neu': 0.785, 'pos': 0.0, 'compound': -0.8922},\n",
       " {'neg': 0.106, 'neu': 0.634, 'pos': 0.26, 'compound': 0.8673},\n",
       " {'neg': 0.063, 'neu': 0.865, 'pos': 0.072, 'compound': 0.1723},\n",
       " {'neg': 0.019, 'neu': 0.77, 'pos': 0.212, 'compound': 0.9676},\n",
       " {'neg': 0.0, 'neu': 0.883, 'pos': 0.117, 'compound': 0.9095},\n",
       " {'neg': 0.058, 'neu': 0.863, 'pos': 0.079, 'compound': 0.3738},\n",
       " {'neg': 0.033, 'neu': 0.818, 'pos': 0.149, 'compound': 0.9812},\n",
       " {'neg': 0.18, 'neu': 0.762, 'pos': 0.058, 'compound': -0.7269},\n",
       " {'neg': 0.0, 'neu': 0.851, 'pos': 0.149, 'compound': 0.9748},\n",
       " {'neg': 0.0, 'neu': 0.729, 'pos': 0.271, 'compound': 0.8858},\n",
       " {'neg': 0.085, 'neu': 0.745, 'pos': 0.171, 'compound': 0.7311},\n",
       " {'neg': 0.137, 'neu': 0.863, 'pos': 0.0, 'compound': -0.8621},\n",
       " {'neg': 0.057, 'neu': 0.836, 'pos': 0.107, 'compound': 0.4404},\n",
       " {'neg': 0.115, 'neu': 0.68, 'pos': 0.205, 'compound': 0.8098},\n",
       " {'neg': 0.0, 'neu': 0.733, 'pos': 0.267, 'compound': 0.8705},\n",
       " {'neg': 0.015, 'neu': 0.845, 'pos': 0.14, 'compound': 0.9855},\n",
       " {'neg': 0.0, 'neu': 0.703, 'pos': 0.297, 'compound': 0.9811},\n",
       " {'neg': 0.0, 'neu': 0.662, 'pos': 0.338, 'compound': 0.9656},\n",
       " {'neg': 0.0, 'neu': 0.634, 'pos': 0.366, 'compound': 0.9776},\n",
       " {'neg': 0.036, 'neu': 0.798, 'pos': 0.165, 'compound': 0.7783},\n",
       " {'neg': 0.0, 'neu': 0.708, 'pos': 0.292, 'compound': 0.9815},\n",
       " {'neg': 0.1, 'neu': 0.863, 'pos': 0.037, 'compound': -0.6249},\n",
       " {'neg': 0.09, 'neu': 0.812, 'pos': 0.098, 'compound': 0.6066},\n",
       " {'neg': 0.084, 'neu': 0.859, 'pos': 0.057, 'compound': -0.7796},\n",
       " {'neg': 0.0, 'neu': 0.761, 'pos': 0.239, 'compound': 0.6588},\n",
       " {'neg': 0.0, 'neu': 0.739, 'pos': 0.261, 'compound': 0.901},\n",
       " {'neg': 0.04, 'neu': 0.699, 'pos': 0.261, 'compound': 0.9468},\n",
       " {'neg': 0.096, 'neu': 0.84, 'pos': 0.064, 'compound': -0.3297},\n",
       " {'neg': 0.013, 'neu': 0.77, 'pos': 0.216, 'compound': 0.9714},\n",
       " {'neg': 0.04, 'neu': 0.915, 'pos': 0.045, 'compound': -0.0366},\n",
       " {'neg': 0.0, 'neu': 0.781, 'pos': 0.219, 'compound': 0.7096},\n",
       " {'neg': 0.02, 'neu': 0.766, 'pos': 0.214, 'compound': 0.9867},\n",
       " {'neg': 0.165, 'neu': 0.743, 'pos': 0.091, 'compound': -0.6638},\n",
       " {'neg': 0.0, 'neu': 0.755, 'pos': 0.245, 'compound': 0.9662},\n",
       " {'neg': 0.079, 'neu': 0.835, 'pos': 0.086, 'compound': -0.2714},\n",
       " {'neg': 0.0, 'neu': 0.651, 'pos': 0.349, 'compound': 0.8449},\n",
       " {'neg': 0.011, 'neu': 0.788, 'pos': 0.202, 'compound': 0.9837},\n",
       " {'neg': 0.022, 'neu': 0.856, 'pos': 0.122, 'compound': 0.9324},\n",
       " {'neg': 0.0, 'neu': 0.697, 'pos': 0.303, 'compound': 0.8614},\n",
       " {'neg': 0.058, 'neu': 0.912, 'pos': 0.03, 'compound': -0.3244},\n",
       " {'neg': 0.071, 'neu': 0.834, 'pos': 0.095, 'compound': 0.5406},\n",
       " {'neg': 0.0, 'neu': 0.638, 'pos': 0.362, 'compound': 0.926},\n",
       " {'neg': 0.042, 'neu': 0.762, 'pos': 0.196, 'compound': 0.9769},\n",
       " {'neg': 0.054, 'neu': 0.816, 'pos': 0.13, 'compound': 0.9353},\n",
       " {'neg': 0.287, 'neu': 0.713, 'pos': 0.0, 'compound': -0.858},\n",
       " {'neg': 0.141, 'neu': 0.734, 'pos': 0.125, 'compound': -0.4855},\n",
       " {'neg': 0.0, 'neu': 0.631, 'pos': 0.369, 'compound': 0.9295},\n",
       " {'neg': 0.02, 'neu': 0.851, 'pos': 0.128, 'compound': 0.7803},\n",
       " {'neg': 0.0, 'neu': 0.531, 'pos': 0.469, 'compound': 0.9421},\n",
       " {'neg': 0.0, 'neu': 0.663, 'pos': 0.337, 'compound': 0.9217},\n",
       " {'neg': 0.197, 'neu': 0.677, 'pos': 0.126, 'compound': -0.598},\n",
       " {'neg': 0.072, 'neu': 0.887, 'pos': 0.041, 'compound': -0.5994},\n",
       " {'neg': 0.081, 'neu': 0.699, 'pos': 0.22, 'compound': 0.6369},\n",
       " {'neg': 0.139, 'neu': 0.795, 'pos': 0.066, 'compound': -0.8876},\n",
       " {'neg': 0.0, 'neu': 0.593, 'pos': 0.407, 'compound': 0.9477},\n",
       " {'neg': 0.0, 'neu': 0.762, 'pos': 0.238, 'compound': 0.972},\n",
       " {'neg': 0.033, 'neu': 0.699, 'pos': 0.268, 'compound': 0.9976},\n",
       " {'neg': 0.0, 'neu': 0.842, 'pos': 0.158, 'compound': 0.6444},\n",
       " {'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'compound': 0.8739},\n",
       " {'neg': 0.024, 'neu': 0.673, 'pos': 0.302, 'compound': 0.9985},\n",
       " {'neg': 0.08, 'neu': 0.757, 'pos': 0.163, 'compound': 0.9598},\n",
       " {'neg': 0.0, 'neu': 0.709, 'pos': 0.291, 'compound': 0.9578},\n",
       " {'neg': 0.058, 'neu': 0.772, 'pos': 0.17, 'compound': 0.7381},\n",
       " {'neg': 0.0, 'neu': 0.758, 'pos': 0.242, 'compound': 0.983},\n",
       " {'neg': 0.028, 'neu': 0.738, 'pos': 0.234, 'compound': 0.929},\n",
       " {'neg': 0.0, 'neu': 0.81, 'pos': 0.19, 'compound': 0.6892},\n",
       " {'neg': 0.233, 'neu': 0.69, 'pos': 0.077, 'compound': -0.8877},\n",
       " {'neg': 0.05, 'neu': 0.615, 'pos': 0.336, 'compound': 0.9917},\n",
       " {'neg': 0.073, 'neu': 0.927, 'pos': 0.0, 'compound': -0.2411},\n",
       " {'neg': 0.0, 'neu': 0.605, 'pos': 0.395, 'compound': 0.8002},\n",
       " {'neg': 0.056, 'neu': 0.852, 'pos': 0.092, 'compound': 0.5911},\n",
       " {'neg': 0.105, 'neu': 0.851, 'pos': 0.044, 'compound': -0.4019},\n",
       " {'neg': 0.0, 'neu': 0.735, 'pos': 0.265, 'compound': 0.9681},\n",
       " {'neg': 0.059, 'neu': 0.763, 'pos': 0.178, 'compound': 0.9963},\n",
       " {'neg': 0.182, 'neu': 0.764, 'pos': 0.054, 'compound': -0.6535},\n",
       " {'neg': 0.054, 'neu': 0.851, 'pos': 0.095, 'compound': 0.9585},\n",
       " {'neg': 0.048, 'neu': 0.778, 'pos': 0.175, 'compound': 0.7918},\n",
       " {'neg': 0.0, 'neu': 0.884, 'pos': 0.116, 'compound': 0.743},\n",
       " {'neg': 0.016, 'neu': 0.953, 'pos': 0.031, 'compound': 0.6911},\n",
       " {'neg': 0.036, 'neu': 0.908, 'pos': 0.056, 'compound': 0.6479},\n",
       " {'neg': 0.0, 'neu': 0.594, 'pos': 0.406, 'compound': 0.9501},\n",
       " {'neg': 0.076, 'neu': 0.873, 'pos': 0.051, 'compound': -0.4023},\n",
       " {'neg': 0.029, 'neu': 0.822, 'pos': 0.148, 'compound': 0.9608},\n",
       " {'neg': 0.0, 'neu': 0.93, 'pos': 0.07, 'compound': 0.4404},\n",
       " {'neg': 0.076, 'neu': 0.871, 'pos': 0.052, 'compound': -0.558},\n",
       " {'neg': 0.0, 'neu': 0.969, 'pos': 0.031, 'compound': 0.2382},\n",
       " {'neg': 0.0, 'neu': 0.652, 'pos': 0.348, 'compound': 0.9523},\n",
       " {'neg': 0.0, 'neu': 0.512, 'pos': 0.488, 'compound': 0.9866},\n",
       " {'neg': 0.04, 'neu': 0.919, 'pos': 0.041, 'compound': 0.0094},\n",
       " {'neg': 0.03, 'neu': 0.889, 'pos': 0.081, 'compound': 0.6187},\n",
       " {'neg': 0.0, 'neu': 0.707, 'pos': 0.293, 'compound': 0.969},\n",
       " {'neg': 0.058, 'neu': 0.761, 'pos': 0.181, 'compound': 0.6908},\n",
       " {'neg': 0.0, 'neu': 0.729, 'pos': 0.271, 'compound': 0.8885},\n",
       " {'neg': 0.055, 'neu': 0.862, 'pos': 0.083, 'compound': 0.8348},\n",
       " {'neg': 0.021, 'neu': 0.87, 'pos': 0.109, 'compound': 0.852},\n",
       " {'neg': 0.0, 'neu': 0.756, 'pos': 0.244, 'compound': 0.9526},\n",
       " {'neg': 0.104, 'neu': 0.896, 'pos': 0.0, 'compound': -0.7964},\n",
       " {'neg': 0.116, 'neu': 0.355, 'pos': 0.529, 'compound': 0.9296},\n",
       " {'neg': 0.022, 'neu': 0.916, 'pos': 0.062, 'compound': 0.7709},\n",
       " {'neg': 0.0, 'neu': 0.6, 'pos': 0.4, 'compound': 0.9493},\n",
       " {'neg': 0.112, 'neu': 0.66, 'pos': 0.228, 'compound': 0.8352},\n",
       " {'neg': 0.042, 'neu': 0.822, 'pos': 0.136, 'compound': 0.7789},\n",
       " {'neg': 0.028, 'neu': 0.719, 'pos': 0.253, 'compound': 0.9606},\n",
       " {'neg': 0.086, 'neu': 0.789, 'pos': 0.124, 'compound': 0.849},\n",
       " {'neg': 0.035, 'neu': 0.823, 'pos': 0.141, 'compound': 0.687},\n",
       " {'neg': 0.177, 'neu': 0.724, 'pos': 0.099, 'compound': -0.6485},\n",
       " {'neg': 0.012, 'neu': 0.688, 'pos': 0.3, 'compound': 0.9978},\n",
       " {'neg': 0.0, 'neu': 0.713, 'pos': 0.287, 'compound': 0.946},\n",
       " {'neg': 0.055, 'neu': 0.862, 'pos': 0.083, 'compound': 0.4471},\n",
       " {'neg': 0.077, 'neu': 0.923, 'pos': 0.0, 'compound': -0.6809},\n",
       " {'neg': 0.017, 'neu': 0.758, 'pos': 0.225, 'compound': 0.9865},\n",
       " {'neg': 0.017, 'neu': 0.836, 'pos': 0.147, 'compound': 0.923},\n",
       " {'neg': 0.061, 'neu': 0.887, 'pos': 0.052, 'compound': -0.128},\n",
       " {'neg': 0.117, 'neu': 0.776, 'pos': 0.107, 'compound': -0.0023},\n",
       " {'neg': 0.0, 'neu': 0.456, 'pos': 0.544, 'compound': 0.9678},\n",
       " {'neg': 0.046, 'neu': 0.768, 'pos': 0.185, 'compound': 0.9984},\n",
       " {'neg': 0.041, 'neu': 0.948, 'pos': 0.01, 'compound': -0.6439},\n",
       " {'neg': 0.0, 'neu': 0.75, 'pos': 0.25, 'compound': 0.9183},\n",
       " {'neg': 0.101, 'neu': 0.899, 'pos': 0.0, 'compound': -0.8968},\n",
       " {'neg': 0.0, 'neu': 0.455, 'pos': 0.545, 'compound': 0.9186},\n",
       " {'neg': 0.04, 'neu': 0.831, 'pos': 0.129, 'compound': 0.8506},\n",
       " {'neg': 0.054, 'neu': 0.817, 'pos': 0.129, 'compound': 0.9901},\n",
       " {'neg': 0.137, 'neu': 0.863, 'pos': 0.0, 'compound': -0.6588},\n",
       " {'neg': 0.222, 'neu': 0.699, 'pos': 0.079, 'compound': -0.9357},\n",
       " {'neg': 0.072, 'neu': 0.812, 'pos': 0.116, 'compound': 0.6795},\n",
       " {'neg': 0.0, 'neu': 0.83, 'pos': 0.17, 'compound': 0.9199},\n",
       " {'neg': 0.027, 'neu': 0.841, 'pos': 0.133, 'compound': 0.8488},\n",
       " {'neg': 0.251, 'neu': 0.749, 'pos': 0.0, 'compound': -0.8556},\n",
       " {'neg': 0.08, 'neu': 0.455, 'pos': 0.466, 'compound': 0.8807},\n",
       " {'neg': 0.022, 'neu': 0.665, 'pos': 0.313, 'compound': 0.9783},\n",
       " {'neg': 0.0, 'neu': 0.632, 'pos': 0.368, 'compound': 0.9768},\n",
       " {'neg': 0.0, 'neu': 0.716, 'pos': 0.284, 'compound': 0.8591},\n",
       " {'neg': 0.046, 'neu': 0.78, 'pos': 0.174, 'compound': 0.9583},\n",
       " {'neg': 0.175, 'neu': 0.716, 'pos': 0.108, 'compound': -0.6956},\n",
       " {'neg': 0.041, 'neu': 0.959, 'pos': 0.0, 'compound': -0.1531},\n",
       " {'neg': 0.025, 'neu': 0.55, 'pos': 0.425, 'compound': 0.9867},\n",
       " {'neg': 0.209, 'neu': 0.762, 'pos': 0.029, 'compound': -0.8979},\n",
       " {'neg': 0.027, 'neu': 0.87, 'pos': 0.103, 'compound': 0.8385},\n",
       " {'neg': 0.039, 'neu': 0.858, 'pos': 0.103, 'compound': 0.4854},\n",
       " {'neg': 0.018, 'neu': 0.776, 'pos': 0.206, 'compound': 0.9983},\n",
       " {'neg': 0.187, 'neu': 0.813, 'pos': 0.0, 'compound': -0.9487},\n",
       " {'neg': 0.0, 'neu': 0.737, 'pos': 0.263, 'compound': 0.836},\n",
       " {'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'compound': 0.9468},\n",
       " {'neg': 0.063, 'neu': 0.937, 'pos': 0.0, 'compound': -0.4019},\n",
       " {'neg': 0.0, 'neu': 0.512, 'pos': 0.488, 'compound': 0.8622},\n",
       " {'neg': 0.0, 'neu': 0.706, 'pos': 0.294, 'compound': 0.9492},\n",
       " {'neg': 0.0, 'neu': 0.668, 'pos': 0.332, 'compound': 0.9468},\n",
       " {'neg': 0.0, 'neu': 0.621, 'pos': 0.379, 'compound': 0.9823},\n",
       " {'neg': 0.049, 'neu': 0.662, 'pos': 0.289, 'compound': 0.9781},\n",
       " {'neg': 0.108, 'neu': 0.798, 'pos': 0.093, 'compound': -0.1606},\n",
       " {'neg': 0.04, 'neu': 0.795, 'pos': 0.165, 'compound': 0.9965},\n",
       " {'neg': 0.261, 'neu': 0.739, 'pos': 0.0, 'compound': -0.9001},\n",
       " {'neg': 0.0, 'neu': 0.883, 'pos': 0.117, 'compound': 0.5106},\n",
       " {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'compound': 0.8555},\n",
       " {'neg': 0.107, 'neu': 0.753, 'pos': 0.14, 'compound': 0.7013},\n",
       " {'neg': 0.012, 'neu': 0.924, 'pos': 0.064, 'compound': 0.7633},\n",
       " {'neg': 0.0, 'neu': 0.485, 'pos': 0.515, 'compound': 0.93},\n",
       " {'neg': 0.073, 'neu': 0.861, 'pos': 0.066, 'compound': 0.0405},\n",
       " {'neg': 0.0, 'neu': 0.911, 'pos': 0.089, 'compound': 0.6969},\n",
       " {'neg': 0.1, 'neu': 0.677, 'pos': 0.223, 'compound': 0.5859},\n",
       " {'neg': 0.114, 'neu': 0.844, 'pos': 0.042, 'compound': -0.745},\n",
       " {'neg': 0.0, 'neu': 0.808, 'pos': 0.192, 'compound': 0.9719},\n",
       " {'neg': 0.031, 'neu': 0.922, 'pos': 0.047, 'compound': 0.1531},\n",
       " {'neg': 0.026, 'neu': 0.838, 'pos': 0.136, 'compound': 0.9114},\n",
       " {'neg': 0.062, 'neu': 0.862, 'pos': 0.076, 'compound': 0.0312},\n",
       " {'neg': 0.0, 'neu': 0.863, 'pos': 0.137, 'compound': 0.6701},\n",
       " {'neg': 0.023, 'neu': 0.786, 'pos': 0.19, 'compound': 0.9892},\n",
       " {'neg': 0.13, 'neu': 0.776, 'pos': 0.094, 'compound': -0.5839},\n",
       " {'neg': 0.03, 'neu': 0.866, 'pos': 0.105, 'compound': 0.9474},\n",
       " {'neg': 0.0, 'neu': 0.829, 'pos': 0.171, 'compound': 0.6597},\n",
       " {'neg': 0.052, 'neu': 0.888, 'pos': 0.06, 'compound': -0.1303},\n",
       " {'neg': 0.027, 'neu': 0.794, 'pos': 0.179, 'compound': 0.9931},\n",
       " {'neg': 0.0, 'neu': 0.716, 'pos': 0.284, 'compound': 0.9415},\n",
       " {'neg': 0.024, 'neu': 0.823, 'pos': 0.152, 'compound': 0.9486},\n",
       " {'neg': 0.0, 'neu': 0.706, 'pos': 0.294, 'compound': 0.91},\n",
       " {'neg': 0.079, 'neu': 0.823, 'pos': 0.099, 'compound': 0.2111},\n",
       " {'neg': 0.0, 'neu': 0.75, 'pos': 0.25, 'compound': 0.8654},\n",
       " {'neg': 0.015, 'neu': 0.724, 'pos': 0.262, 'compound': 0.9804},\n",
       " {'neg': 0.0, 'neu': 0.627, 'pos': 0.373, 'compound': 0.9458},\n",
       " {'neg': 0.012, 'neu': 0.763, 'pos': 0.225, 'compound': 0.9985},\n",
       " {'neg': 0.0, 'neu': 0.566, 'pos': 0.434, 'compound': 0.9647},\n",
       " {'neg': 0.08, 'neu': 0.92, 'pos': 0.0, 'compound': -0.4688},\n",
       " {'neg': 0.02, 'neu': 0.822, 'pos': 0.157, 'compound': 0.9854},\n",
       " {'neg': 0.036, 'neu': 0.768, 'pos': 0.196, 'compound': 0.9674},\n",
       " {'neg': 0.066, 'neu': 0.781, 'pos': 0.153, 'compound': 0.9398},\n",
       " {'neg': 0.0, 'neu': 0.722, 'pos': 0.278, 'compound': 0.8585},\n",
       " {'neg': 0.031, 'neu': 0.826, 'pos': 0.143, 'compound': 0.9446},\n",
       " {'neg': 0.036, 'neu': 0.826, 'pos': 0.138, 'compound': 0.8193},\n",
       " {'neg': 0.023, 'neu': 0.914, 'pos': 0.063, 'compound': 0.6524},\n",
       " {'neg': 0.073, 'neu': 0.858, 'pos': 0.069, 'compound': 0.1027},\n",
       " {'neg': 0.06, 'neu': 0.67, 'pos': 0.271, 'compound': 0.982},\n",
       " {'neg': 0.047, 'neu': 0.746, 'pos': 0.206, 'compound': 0.9637},\n",
       " {'neg': 0.039, 'neu': 0.88, 'pos': 0.081, 'compound': 0.9223},\n",
       " {'neg': 0.099, 'neu': 0.743, 'pos': 0.158, 'compound': 0.8481},\n",
       " {'neg': 0.02, 'neu': 0.921, 'pos': 0.059, 'compound': 0.8074},\n",
       " {'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'compound': 0.9441},\n",
       " {'neg': 0.0, 'neu': 0.778, 'pos': 0.222, 'compound': 0.8126},\n",
       " {'neg': 0.105, 'neu': 0.815, 'pos': 0.08, 'compound': -0.6157},\n",
       " {'neg': 0.025, 'neu': 0.771, 'pos': 0.204, 'compound': 0.9561},\n",
       " {'neg': 0.029, 'neu': 0.866, 'pos': 0.106, 'compound': 0.9775},\n",
       " {'neg': 0.0, 'neu': 0.907, 'pos': 0.093, 'compound': 0.2382},\n",
       " {'neg': 0.012, 'neu': 0.689, 'pos': 0.299, 'compound': 0.9929},\n",
       " {'neg': 0.032, 'neu': 0.853, 'pos': 0.115, 'compound': 0.9268},\n",
       " {'neg': 0.207, 'neu': 0.77, 'pos': 0.023, 'compound': -0.9926},\n",
       " {'neg': 0.052, 'neu': 0.883, 'pos': 0.065, 'compound': 0.2984},\n",
       " {'neg': 0.0, 'neu': 0.692, 'pos': 0.308, 'compound': 0.9493},\n",
       " {'neg': 0.0, 'neu': 0.436, 'pos': 0.564, 'compound': 0.9476},\n",
       " {'neg': 0.053, 'neu': 0.89, 'pos': 0.058, 'compound': 0.1116},\n",
       " {'neg': 0.049, 'neu': 0.868, 'pos': 0.082, 'compound': 0.6232},\n",
       " {'neg': 0.06, 'neu': 0.874, 'pos': 0.066, 'compound': 0.0129},\n",
       " {'neg': 0.084, 'neu': 0.529, 'pos': 0.387, 'compound': 0.9874},\n",
       " {'neg': 0.0, 'neu': 0.901, 'pos': 0.099, 'compound': 0.4927},\n",
       " {'neg': 0.044, 'neu': 0.925, 'pos': 0.031, 'compound': -0.3493},\n",
       " {'neg': 0.172, 'neu': 0.828, 'pos': 0.0, 'compound': -0.6833},\n",
       " {'neg': 0.055, 'neu': 0.823, 'pos': 0.121, 'compound': 0.9845},\n",
       " {'neg': 0.041, 'neu': 0.729, 'pos': 0.23, 'compound': 0.8072},\n",
       " {'neg': 0.0, 'neu': 0.852, 'pos': 0.148, 'compound': 0.8883},\n",
       " {'neg': 0.035, 'neu': 0.721, 'pos': 0.244, 'compound': 0.996},\n",
       " {'neg': 0.0, 'neu': 0.795, 'pos': 0.205, 'compound': 0.9052},\n",
       " {'neg': 0.0, 'neu': 0.672, 'pos': 0.328, 'compound': 0.9444},\n",
       " {'neg': 0.0, 'neu': 0.74, 'pos': 0.26, 'compound': 0.9777},\n",
       " {'neg': 0.083, 'neu': 0.884, 'pos': 0.032, 'compound': -0.4215},\n",
       " {'neg': 0.0, 'neu': 0.795, 'pos': 0.205, 'compound': 0.9451},\n",
       " {'neg': 0.112, 'neu': 0.888, 'pos': 0.0, 'compound': -0.2263},\n",
       " {'neg': 0.013, 'neu': 0.688, 'pos': 0.298, 'compound': 0.9882},\n",
       " {'neg': 0.0, 'neu': 0.747, 'pos': 0.253, 'compound': 0.9042},\n",
       " {'neg': 0.08, 'neu': 0.867, 'pos': 0.054, 'compound': -0.7284},\n",
       " {'neg': 0.033, 'neu': 0.842, 'pos': 0.126, 'compound': 0.9349},\n",
       " {'neg': 0.016, 'neu': 0.823, 'pos': 0.161, 'compound': 0.8741},\n",
       " {'neg': 0.0, 'neu': 0.563, 'pos': 0.437, 'compound': 0.9849},\n",
       " {'neg': 0.0, 'neu': 0.701, 'pos': 0.299, 'compound': 0.9563},\n",
       " {'neg': 0.0, 'neu': 0.808, 'pos': 0.192, 'compound': 0.8885},\n",
       " {'neg': 0.02, 'neu': 0.83, 'pos': 0.15, 'compound': 0.8086},\n",
       " {'neg': 0.0, 'neu': 0.642, 'pos': 0.358, 'compound': 0.9747},\n",
       " {'neg': 0.196, 'neu': 0.654, 'pos': 0.151, 'compound': -0.8375},\n",
       " {'neg': 0.035, 'neu': 0.693, 'pos': 0.273, 'compound': 0.9243},\n",
       " {'neg': 0.117, 'neu': 0.753, 'pos': 0.13, 'compound': 0.2057},\n",
       " {'neg': 0.0, 'neu': 0.712, 'pos': 0.288, 'compound': 0.9933},\n",
       " {'neg': 0.131, 'neu': 0.795, 'pos': 0.073, 'compound': -0.9172},\n",
       " {'neg': 0.0, 'neu': 0.749, 'pos': 0.251, 'compound': 0.9527},\n",
       " {'neg': 0.0, 'neu': 0.69, 'pos': 0.31, 'compound': 0.8953},\n",
       " {'neg': 0.108, 'neu': 0.695, 'pos': 0.198, 'compound': 0.8238},\n",
       " {'neg': 0.063, 'neu': 0.796, 'pos': 0.141, 'compound': 0.9679},\n",
       " {'neg': 0.0, 'neu': 0.461, 'pos': 0.539, 'compound': 0.9451},\n",
       " {'neg': 0.0, 'neu': 0.736, 'pos': 0.264, 'compound': 0.9641},\n",
       " {'neg': 0.0, 'neu': 0.768, 'pos': 0.232, 'compound': 0.7876},\n",
       " {'neg': 0.0, 'neu': 0.698, 'pos': 0.302, 'compound': 0.8175},\n",
       " {'neg': 0.06, 'neu': 0.86, 'pos': 0.079, 'compound': 0.3867},\n",
       " {'neg': 0.037, 'neu': 0.805, 'pos': 0.159, 'compound': 0.9797},\n",
       " {'neg': 0.0, 'neu': 0.885, 'pos': 0.115, 'compound': 0.7263},\n",
       " {'neg': 0.201, 'neu': 0.765, 'pos': 0.034, 'compound': -0.8555},\n",
       " {'neg': 0.064, 'neu': 0.853, 'pos': 0.083, 'compound': 0.8424},\n",
       " {'neg': 0.102, 'neu': 0.821, 'pos': 0.078, 'compound': -0.6365},\n",
       " {'neg': 0.0, 'neu': 0.885, 'pos': 0.115, 'compound': 0.6908},\n",
       " {'neg': 0.058, 'neu': 0.759, 'pos': 0.183, 'compound': 0.4835},\n",
       " {'neg': 0.075, 'neu': 0.886, 'pos': 0.039, 'compound': -0.6321},\n",
       " {'neg': 0.051, 'neu': 0.797, 'pos': 0.152, 'compound': 0.879},\n",
       " {'neg': 0.0, 'neu': 0.886, 'pos': 0.114, 'compound': 0.6597},\n",
       " {'neg': 0.036, 'neu': 0.754, 'pos': 0.21, 'compound': 0.9915},\n",
       " {'neg': 0.0, 'neu': 0.81, 'pos': 0.19, 'compound': 0.9292},\n",
       " {'neg': 0.112, 'neu': 0.777, 'pos': 0.111, 'compound': -0.0484},\n",
       " {'neg': 0.317, 'neu': 0.6, 'pos': 0.083, 'compound': -0.8848},\n",
       " {'neg': 0.025, 'neu': 0.77, 'pos': 0.205, 'compound': 0.9124},\n",
       " {'neg': 0.043, 'neu': 0.814, 'pos': 0.143, 'compound': 0.9848},\n",
       " {'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.8799},\n",
       " {'neg': 0.0, 'neu': 0.884, 'pos': 0.116, 'compound': 0.6486},\n",
       " {'neg': 0.023, 'neu': 0.618, 'pos': 0.359, 'compound': 0.9854},\n",
       " {'neg': 0.048, 'neu': 0.859, 'pos': 0.093, 'compound': 0.2023},\n",
       " {'neg': 0.0, 'neu': 0.714, 'pos': 0.286, 'compound': 0.9153},\n",
       " {'neg': 0.0, 'neu': 0.898, 'pos': 0.102, 'compound': 0.7845},\n",
       " {'neg': 0.144, 'neu': 0.81, 'pos': 0.047, 'compound': -0.5873},\n",
       " {'neg': 0.135, 'neu': 0.764, 'pos': 0.101, 'compound': -0.8462},\n",
       " {'neg': 0.016, 'neu': 0.742, 'pos': 0.242, 'compound': 0.9966},\n",
       " {'neg': 0.101, 'neu': 0.875, 'pos': 0.024, 'compound': -0.7469},\n",
       " {'neg': 0.0, 'neu': 0.648, 'pos': 0.352, 'compound': 0.9889},\n",
       " {'neg': 0.031, 'neu': 0.697, 'pos': 0.272, 'compound': 0.9534},\n",
       " {'neg': 0.007, 'neu': 0.782, 'pos': 0.211, 'compound': 0.9982},\n",
       " {'neg': 0.086, 'neu': 0.806, 'pos': 0.109, 'compound': 0.6036},\n",
       " {'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'compound': 0.9667},\n",
       " {'neg': 0.062, 'neu': 0.601, 'pos': 0.337, 'compound': 0.9607},\n",
       " {'neg': 0.0, 'neu': 0.661, 'pos': 0.339, 'compound': 0.9638},\n",
       " {'neg': 0.07, 'neu': 0.903, 'pos': 0.027, 'compound': -0.4291},\n",
       " {'neg': 0.038, 'neu': 0.842, 'pos': 0.119, 'compound': 0.9229},\n",
       " {'neg': 0.228, 'neu': 0.741, 'pos': 0.031, 'compound': -0.8625},\n",
       " {'neg': 0.032, 'neu': 0.771, 'pos': 0.197, 'compound': 0.8992},\n",
       " {'neg': 0.0, 'neu': 0.634, 'pos': 0.366, 'compound': 0.6369},\n",
       " {'neg': 0.06, 'neu': 0.884, 'pos': 0.056, 'compound': 0.1456},\n",
       " {'neg': 0.014, 'neu': 0.858, 'pos': 0.128, 'compound': 0.967},\n",
       " {'neg': 0.066, 'neu': 0.788, 'pos': 0.147, 'compound': 0.9619},\n",
       " {'neg': 0.099, 'neu': 0.797, 'pos': 0.104, 'compound': 0.1775},\n",
       " {'neg': 0.047, 'neu': 0.811, 'pos': 0.142, 'compound': 0.9861},\n",
       " {'neg': 0.033, 'neu': 0.779, 'pos': 0.188, 'compound': 0.9099},\n",
       " {'neg': 0.082, 'neu': 0.867, 'pos': 0.051, 'compound': -0.3971},\n",
       " {'neg': 0.077, 'neu': 0.854, 'pos': 0.07, 'compound': 0.455},\n",
       " {'neg': 0.158, 'neu': 0.727, 'pos': 0.116, 'compound': -0.831},\n",
       " {'neg': 0.036, 'neu': 0.85, 'pos': 0.114, 'compound': 0.7956},\n",
       " {'neg': 0.041, 'neu': 0.799, 'pos': 0.16, 'compound': 0.9458},\n",
       " {'neg': 0.0, 'neu': 0.831, 'pos': 0.169, 'compound': 0.7448},\n",
       " {'neg': 0.032, 'neu': 0.696, 'pos': 0.272, 'compound': 0.8841},\n",
       " {'neg': 0.102, 'neu': 0.817, 'pos': 0.08, 'compound': -0.3753},\n",
       " {'neg': 0.117, 'neu': 0.821, 'pos': 0.062, 'compound': -0.6642},\n",
       " {'neg': 0.013, 'neu': 0.682, 'pos': 0.305, 'compound': 0.9965},\n",
       " {'neg': 0.0, 'neu': 0.77, 'pos': 0.23, 'compound': 0.9832},\n",
       " {'neg': 0.052, 'neu': 0.872, 'pos': 0.076, 'compound': 0.296},\n",
       " {'neg': 0.055, 'neu': 0.741, 'pos': 0.204, 'compound': 0.984},\n",
       " {'neg': 0.031, 'neu': 0.757, 'pos': 0.212, 'compound': 0.9129},\n",
       " {'neg': 0.0, 'neu': 0.531, 'pos': 0.469, 'compound': 0.9507},\n",
       " {'neg': 0.275, 'neu': 0.698, 'pos': 0.027, 'compound': -0.9201},\n",
       " {'neg': 0.0, 'neu': 0.823, 'pos': 0.177, 'compound': 0.9314},\n",
       " {'neg': 0.016, 'neu': 0.754, 'pos': 0.23, 'compound': 0.9721},\n",
       " {'neg': 0.0, 'neu': 0.695, 'pos': 0.305, 'compound': 0.9493},\n",
       " {'neg': 0.0, 'neu': 0.807, 'pos': 0.193, 'compound': 0.8689},\n",
       " {'neg': 0.009, 'neu': 0.78, 'pos': 0.211, 'compound': 0.9927},\n",
       " {'neg': 0.0, 'neu': 0.814, 'pos': 0.186, 'compound': 0.9297},\n",
       " {'neg': 0.0, 'neu': 0.682, 'pos': 0.318, 'compound': 0.9011},\n",
       " {'neg': 0.022, 'neu': 0.682, 'pos': 0.296, 'compound': 0.9823},\n",
       " {'neg': 0.05, 'neu': 0.79, 'pos': 0.16, 'compound': 0.9725},\n",
       " {'neg': 0.0, 'neu': 0.791, 'pos': 0.209, 'compound': 0.6696},\n",
       " {'neg': 0.09, 'neu': 0.91, 'pos': 0.0, 'compound': -0.7797},\n",
       " {'neg': 0.274, 'neu': 0.678, 'pos': 0.047, 'compound': -0.9371},\n",
       " {'neg': 0.028, 'neu': 0.816, 'pos': 0.156, 'compound': 0.8762},\n",
       " {'neg': 0.042, 'neu': 0.569, 'pos': 0.388, 'compound': 0.9432},\n",
       " {'neg': 0.097, 'neu': 0.871, 'pos': 0.032, 'compound': -0.7465},\n",
       " {'neg': 0.131, 'neu': 0.692, 'pos': 0.177, 'compound': 0.6871},\n",
       " {'neg': 0.024, 'neu': 0.86, 'pos': 0.115, 'compound': 0.7227},\n",
       " {'neg': 0.0, 'neu': 0.78, 'pos': 0.22, 'compound': 0.9001},\n",
       " {'neg': 0.0, 'neu': 0.749, 'pos': 0.251, 'compound': 0.9078},\n",
       " {'neg': 0.072, 'neu': 0.79, 'pos': 0.137, 'compound': 0.784},\n",
       " {'neg': 0.218, 'neu': 0.681, 'pos': 0.101, 'compound': -0.9515},\n",
       " {'neg': 0.069, 'neu': 0.82, 'pos': 0.11, 'compound': 0.3182},\n",
       " {'neg': 0.038, 'neu': 0.846, 'pos': 0.116, 'compound': 0.5423},\n",
       " {'neg': 0.0, 'neu': 0.899, 'pos': 0.101, 'compound': 0.8713},\n",
       " {'neg': 0.067, 'neu': 0.752, 'pos': 0.181, 'compound': 0.9404},\n",
       " {'neg': 0.087, 'neu': 0.689, 'pos': 0.224, 'compound': 0.9417},\n",
       " {'neg': 0.0, 'neu': 0.655, 'pos': 0.345, 'compound': 0.9758},\n",
       " {'neg': 0.136, 'neu': 0.813, 'pos': 0.051, 'compound': -0.704},\n",
       " {'neg': 0.135, 'neu': 0.865, 'pos': 0.0, 'compound': -0.4973},\n",
       " {'neg': 0.055, 'neu': 0.808, 'pos': 0.137, 'compound': 0.2086},\n",
       " {'neg': 0.031, 'neu': 0.861, 'pos': 0.108, 'compound': 0.8146},\n",
       " {'neg': 0.057, 'neu': 0.826, 'pos': 0.117, 'compound': 0.9554},\n",
       " {'neg': 0.163, 'neu': 0.772, 'pos': 0.066, 'compound': -0.9152},\n",
       " {'neg': 0.074, 'neu': 0.705, 'pos': 0.221, 'compound': 0.659},\n",
       " {'neg': 0.041, 'neu': 0.799, 'pos': 0.16, 'compound': 0.9458},\n",
       " {'neg': 0.059, 'neu': 0.724, 'pos': 0.217, 'compound': 0.9815},\n",
       " {'neg': 0.023, 'neu': 0.736, 'pos': 0.241, 'compound': 0.9536},\n",
       " {'neg': 0.014, 'neu': 0.683, 'pos': 0.303, 'compound': 0.9752},\n",
       " {'neg': 0.114, 'neu': 0.539, 'pos': 0.347, 'compound': 0.9134},\n",
       " {'neg': 0.046, 'neu': 0.954, 'pos': 0.0, 'compound': -0.5458},\n",
       " {'neg': 0.037, 'neu': 0.751, 'pos': 0.213, 'compound': 0.8571},\n",
       " {'neg': 0.032, 'neu': 0.784, 'pos': 0.184, 'compound': 0.9929},\n",
       " {'neg': 0.0, 'neu': 0.668, 'pos': 0.332, 'compound': 0.9714},\n",
       " {'neg': 0.0, 'neu': 0.737, 'pos': 0.263, 'compound': 0.9475},\n",
       " {'neg': 0.0, 'neu': 0.647, 'pos': 0.353, 'compound': 0.8856},\n",
       " {'neg': 0.093, 'neu': 0.739, 'pos': 0.168, 'compound': 0.4703},\n",
       " {'neg': 0.021, 'neu': 0.951, 'pos': 0.028, 'compound': -0.0258},\n",
       " {'neg': 0.045, 'neu': 0.876, 'pos': 0.078, 'compound': 0.5389},\n",
       " {'neg': 0.086, 'neu': 0.858, 'pos': 0.056, 'compound': -0.9222},\n",
       " {'neg': 0.0, 'neu': 0.883, 'pos': 0.117, 'compound': 0.6757},\n",
       " {'neg': 0.04, 'neu': 0.797, 'pos': 0.163, 'compound': 0.9513},\n",
       " {'neg': 0.087, 'neu': 0.82, 'pos': 0.092, 'compound': -0.1918},\n",
       " {'neg': 0.0, 'neu': 0.642, 'pos': 0.358, 'compound': 0.9808},\n",
       " {'neg': 0.016, 'neu': 0.773, 'pos': 0.211, 'compound': 0.9676},\n",
       " {'neg': 0.0, 'neu': 0.879, 'pos': 0.121, 'compound': 0.7224},\n",
       " {'neg': 0.0, 'neu': 0.836, 'pos': 0.164, 'compound': 0.8944},\n",
       " {'neg': 0.101, 'neu': 0.824, 'pos': 0.075, 'compound': -0.4628},\n",
       " {'neg': 0.047, 'neu': 0.656, 'pos': 0.298, 'compound': 0.8977},\n",
       " {'neg': 0.062, 'neu': 0.922, 'pos': 0.015, 'compound': -0.921},\n",
       " {'neg': 0.082, 'neu': 0.623, 'pos': 0.295, 'compound': 0.7301},\n",
       " {'neg': 0.0, 'neu': 0.551, 'pos': 0.449, 'compound': 0.886},\n",
       " {'neg': 0.0, 'neu': 0.666, 'pos': 0.334, 'compound': 0.98},\n",
       " {'neg': 0.067, 'neu': 0.779, 'pos': 0.154, 'compound': 0.9814},\n",
       " {'neg': 0.074, 'neu': 0.839, 'pos': 0.086, 'compound': 0.2755},\n",
       " {'neg': 0.069, 'neu': 0.876, 'pos': 0.056, 'compound': -0.0777},\n",
       " {'neg': 0.082, 'neu': 0.841, 'pos': 0.077, 'compound': -0.4086},\n",
       " {'neg': 0.04, 'neu': 0.895, 'pos': 0.065, 'compound': 0.4909},\n",
       " {'neg': 0.047, 'neu': 0.724, 'pos': 0.228, 'compound': 0.9143},\n",
       " {'neg': 0.058, 'neu': 0.705, 'pos': 0.237, 'compound': 0.8227},\n",
       " {'neg': 0.156, 'neu': 0.79, 'pos': 0.055, 'compound': -0.6945},\n",
       " {'neg': 0.0, 'neu': 0.538, 'pos': 0.462, 'compound': 0.9187},\n",
       " {'neg': 0.0, 'neu': 0.665, 'pos': 0.335, 'compound': 0.9273},\n",
       " {'neg': 0.024, 'neu': 0.813, 'pos': 0.163, 'compound': 0.8555},\n",
       " {'neg': 0.019, 'neu': 0.874, 'pos': 0.107, 'compound': 0.8677},\n",
       " {'neg': 0.062, 'neu': 0.927, 'pos': 0.01, 'compound': -0.7996},\n",
       " {'neg': 0.123, 'neu': 0.877, 'pos': 0.0, 'compound': -0.3326},\n",
       " {'neg': 0.0, 'neu': 0.757, 'pos': 0.243, 'compound': 0.9406},\n",
       " {'neg': 0.0, 'neu': 0.602, 'pos': 0.398, 'compound': 0.9734},\n",
       " {'neg': 0.057, 'neu': 0.864, 'pos': 0.079, 'compound': 0.3975},\n",
       " {'neg': 0.064, 'neu': 0.821, 'pos': 0.114, 'compound': 0.3869},\n",
       " {'neg': 0.013, 'neu': 0.879, 'pos': 0.108, 'compound': 0.9461},\n",
       " {'neg': 0.036, 'neu': 0.637, 'pos': 0.327, 'compound': 0.9744},\n",
       " {'neg': 0.078, 'neu': 0.723, 'pos': 0.199, 'compound': 0.8601},\n",
       " {'neg': 0.031, 'neu': 0.638, 'pos': 0.331, 'compound': 0.9688},\n",
       " {'neg': 0.034, 'neu': 0.709, 'pos': 0.257, 'compound': 0.8687},\n",
       " {'neg': 0.03, 'neu': 0.97, 'pos': 0.0, 'compound': -0.555},\n",
       " {'neg': 0.205, 'neu': 0.795, 'pos': 0.0, 'compound': -0.802},\n",
       " {'neg': 0.1, 'neu': 0.788, 'pos': 0.112, 'compound': 0.3193},\n",
       " {'neg': 0.025, 'neu': 0.795, 'pos': 0.18, 'compound': 0.8536},\n",
       " {'neg': 0.058, 'neu': 0.615, 'pos': 0.327, 'compound': 0.9485},\n",
       " {'neg': 0.144, 'neu': 0.785, 'pos': 0.071, 'compound': -0.8812},\n",
       " {'neg': 0.0, 'neu': 0.982, 'pos': 0.018, 'compound': 0.2732},\n",
       " {'neg': 0.043, 'neu': 0.894, 'pos': 0.064, 'compound': 0.0129},\n",
       " {'neg': 0.0, 'neu': 0.736, 'pos': 0.264, 'compound': 0.8384},\n",
       " {'neg': 0.014, 'neu': 0.78, 'pos': 0.206, 'compound': 0.9777},\n",
       " {'neg': 0.043, 'neu': 0.82, 'pos': 0.137, 'compound': 0.9574},\n",
       " {'neg': 0.0, 'neu': 0.732, 'pos': 0.268, 'compound': 0.9551},\n",
       " {'neg': 0.035, 'neu': 0.672, 'pos': 0.293, 'compound': 0.9676},\n",
       " {'neg': 0.143, 'neu': 0.69, 'pos': 0.167, 'compound': 0.4601},\n",
       " {'neg': 0.066, 'neu': 0.825, 'pos': 0.109, 'compound': 0.6393},\n",
       " {'neg': 0.0, 'neu': 0.794, 'pos': 0.206, 'compound': 0.9258},\n",
       " {'neg': 0.089, 'neu': 0.812, 'pos': 0.099, 'compound': 0.6963},\n",
       " {'neg': 0.029, 'neu': 0.795, 'pos': 0.176, 'compound': 0.8265},\n",
       " {'neg': 0.045, 'neu': 0.929, 'pos': 0.026, 'compound': -0.3089},\n",
       " {'neg': 0.226, 'neu': 0.743, 'pos': 0.031, 'compound': -0.8868},\n",
       " {'neg': 0.039, 'neu': 0.936, 'pos': 0.025, 'compound': -0.09},\n",
       " {'neg': 0.029, 'neu': 0.892, 'pos': 0.079, 'compound': 0.9674},\n",
       " {'neg': 0.0, 'neu': 0.951, 'pos': 0.049, 'compound': 0.2911},\n",
       " {'neg': 0.021, 'neu': 0.64, 'pos': 0.338, 'compound': 0.9946},\n",
       " {'neg': 0.082, 'neu': 0.693, 'pos': 0.225, 'compound': 0.9624},\n",
       " {'neg': 0.023, 'neu': 0.764, 'pos': 0.213, 'compound': 0.9901},\n",
       " {'neg': 0.123, 'neu': 0.613, 'pos': 0.264, 'compound': 0.5719},\n",
       " {'neg': 0.073, 'neu': 0.9, 'pos': 0.027, 'compound': -0.4939},\n",
       " {'neg': 0.076, 'neu': 0.924, 'pos': 0.0, 'compound': -0.9352},\n",
       " {'neg': 0.039, 'neu': 0.718, 'pos': 0.243, 'compound': 0.9831},\n",
       " {'neg': 0.041, 'neu': 0.718, 'pos': 0.241, 'compound': 0.9948},\n",
       " {'neg': 0.0, 'neu': 0.703, 'pos': 0.297, 'compound': 0.91},\n",
       " {'neg': 0.082, 'neu': 0.854, 'pos': 0.064, 'compound': -0.5765},\n",
       " {'neg': 0.029, 'neu': 0.72, 'pos': 0.251, 'compound': 0.9993},\n",
       " {'neg': 0.016, 'neu': 0.769, 'pos': 0.215, 'compound': 0.9865},\n",
       " {'neg': 0.025, 'neu': 0.912, 'pos': 0.062, 'compound': 0.6941},\n",
       " {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.9325},\n",
       " {'neg': 0.1, 'neu': 0.787, 'pos': 0.113, 'compound': -0.3767},\n",
       " {'neg': 0.0, 'neu': 0.772, 'pos': 0.228, 'compound': 0.7783},\n",
       " {'neg': 0.037, 'neu': 0.913, 'pos': 0.05, 'compound': 0.2513},\n",
       " {'neg': 0.0, 'neu': 0.78, 'pos': 0.22, 'compound': 0.952},\n",
       " {'neg': 0.0, 'neu': 0.941, 'pos': 0.059, 'compound': 0.3919},\n",
       " {'neg': 0.0, 'neu': 0.843, 'pos': 0.157, 'compound': 0.9936},\n",
       " {'neg': 0.067, 'neu': 0.873, 'pos': 0.061, 'compound': -0.0064},\n",
       " {'neg': 0.0, 'neu': 0.557, 'pos': 0.443, 'compound': 0.9642},\n",
       " {'neg': 0.014, 'neu': 0.807, 'pos': 0.179, 'compound': 0.9413},\n",
       " {'neg': 0.062, 'neu': 0.684, 'pos': 0.254, 'compound': 0.7845},\n",
       " {'neg': 0.0, 'neu': 0.673, 'pos': 0.327, 'compound': 0.9274},\n",
       " {'neg': 0.106, 'neu': 0.859, 'pos': 0.035, 'compound': -0.6295},\n",
       " {'neg': 0.078, 'neu': 0.642, 'pos': 0.28, 'compound': 0.9554},\n",
       " {'neg': 0.132, 'neu': 0.817, 'pos': 0.051, 'compound': -0.9683},\n",
       " {'neg': 0.035, 'neu': 0.74, 'pos': 0.225, 'compound': 0.9117},\n",
       " {'neg': 0.065, 'neu': 0.783, 'pos': 0.152, 'compound': 0.9877},\n",
       " {'neg': 0.011, 'neu': 0.714, 'pos': 0.275, 'compound': 0.9843},\n",
       " {'neg': 0.026, 'neu': 0.864, 'pos': 0.111, 'compound': 0.9768},\n",
       " {'neg': 0.018, 'neu': 0.803, 'pos': 0.179, 'compound': 0.9274},\n",
       " {'neg': 0.0, 'neu': 0.767, 'pos': 0.233, 'compound': 0.9397},\n",
       " {'neg': 0.058, 'neu': 0.857, 'pos': 0.085, 'compound': 0.9442},\n",
       " {'neg': 0.035, 'neu': 0.816, 'pos': 0.15, 'compound': 0.8299},\n",
       " {'neg': 0.0, 'neu': 0.695, 'pos': 0.305, 'compound': 0.8779},\n",
       " {'neg': 0.097, 'neu': 0.903, 'pos': 0.0, 'compound': -0.5962},\n",
       " {'neg': 0.025, 'neu': 0.813, 'pos': 0.161, 'compound': 0.9863},\n",
       " {'neg': 0.023, 'neu': 0.655, 'pos': 0.323, 'compound': 0.975},\n",
       " {'neg': 0.011, 'neu': 0.818, 'pos': 0.171, 'compound': 0.9653},\n",
       " {'neg': 0.0, 'neu': 0.721, 'pos': 0.279, 'compound': 0.9118},\n",
       " {'neg': 0.059, 'neu': 0.824, 'pos': 0.118, 'compound': 0.986},\n",
       " {'neg': 0.049, 'neu': 0.88, 'pos': 0.071, 'compound': 0.8896},\n",
       " {'neg': 0.017, 'neu': 0.815, 'pos': 0.168, 'compound': 0.8716},\n",
       " {'neg': 0.036, 'neu': 0.869, 'pos': 0.095, 'compound': 0.5778},\n",
       " {'neg': 0.0, 'neu': 0.77, 'pos': 0.23, 'compound': 0.985},\n",
       " {'neg': 0.043, 'neu': 0.699, 'pos': 0.258, 'compound': 0.9951},\n",
       " {'neg': 0.116, 'neu': 0.745, 'pos': 0.138, 'compound': 0.3818},\n",
       " {'neg': 0.0, 'neu': 0.804, 'pos': 0.196, 'compound': 0.9367},\n",
       " {'neg': 0.039, 'neu': 0.872, 'pos': 0.089, 'compound': 0.7734},\n",
       " {'neg': 0.049, 'neu': 0.818, 'pos': 0.134, 'compound': 0.8381},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.0, 'neu': 0.64, 'pos': 0.36, 'compound': 0.9718},\n",
       " {'neg': 0.0, 'neu': 0.796, 'pos': 0.204, 'compound': 0.875},\n",
       " {'neg': 0.168, 'neu': 0.78, 'pos': 0.052, 'compound': -0.9731},\n",
       " {'neg': 0.02, 'neu': 0.856, 'pos': 0.123, 'compound': 0.9867},\n",
       " {'neg': 0.028, 'neu': 0.864, 'pos': 0.108, 'compound': 0.8959},\n",
       " {'neg': 0.0, 'neu': 0.652, 'pos': 0.348, 'compound': 0.967},\n",
       " {'neg': 0.0, 'neu': 0.63, 'pos': 0.37, 'compound': 0.8316},\n",
       " {'neg': 0.058, 'neu': 0.717, 'pos': 0.225, 'compound': 0.9157},\n",
       " {'neg': 0.078, 'neu': 0.802, 'pos': 0.12, 'compound': 0.6987},\n",
       " {'neg': 0.0, 'neu': 0.726, 'pos': 0.274, 'compound': 0.9708},\n",
       " {'neg': 0.054, 'neu': 0.717, 'pos': 0.229, 'compound': 0.9468},\n",
       " {'neg': 0.0, 'neu': 0.793, 'pos': 0.207, 'compound': 0.8805},\n",
       " {'neg': 0.027, 'neu': 0.763, 'pos': 0.21, 'compound': 0.9313},\n",
       " {'neg': 0.0, 'neu': 0.771, 'pos': 0.229, 'compound': 0.9081},\n",
       " {'neg': 0.07, 'neu': 0.827, 'pos': 0.102, 'compound': 0.2595},\n",
       " {'neg': 0.086, 'neu': 0.739, 'pos': 0.175, 'compound': 0.9831},\n",
       " {'neg': 0.0, 'neu': 0.596, 'pos': 0.404, 'compound': 0.91},\n",
       " {'neg': 0.056, 'neu': 0.805, 'pos': 0.139, 'compound': 0.7721},\n",
       " {'neg': 0.059, 'neu': 0.699, 'pos': 0.242, 'compound': 0.9562},\n",
       " {'neg': 0.034, 'neu': 0.869, 'pos': 0.097, 'compound': 0.6908},\n",
       " {'neg': 0.01, 'neu': 0.746, 'pos': 0.244, 'compound': 0.9893},\n",
       " {'neg': 0.0, 'neu': 0.919, 'pos': 0.081, 'compound': 0.6421},\n",
       " {'neg': 0.028, 'neu': 0.821, 'pos': 0.151, 'compound': 0.9605},\n",
       " {'neg': 0.0, 'neu': 0.822, 'pos': 0.178, 'compound': 0.743},\n",
       " {'neg': 0.431, 'neu': 0.439, 'pos': 0.129, 'compound': -0.9133},\n",
       " {'neg': 0.089, 'neu': 0.86, 'pos': 0.051, 'compound': -0.7075},\n",
       " {'neg': 0.026, 'neu': 0.806, 'pos': 0.168, 'compound': 0.9493},\n",
       " {'neg': 0.052, 'neu': 0.692, 'pos': 0.257, 'compound': 0.9286},\n",
       " {'neg': 0.066, 'neu': 0.5, 'pos': 0.434, 'compound': 0.9804},\n",
       " {'neg': 0.082, 'neu': 0.822, 'pos': 0.096, 'compound': 0.3526},\n",
       " {'neg': 0.112, 'neu': 0.805, 'pos': 0.084, 'compound': -0.3812},\n",
       " {'neg': 0.057, 'neu': 0.902, 'pos': 0.042, 'compound': -0.4615},\n",
       " {'neg': 0.067, 'neu': 0.746, 'pos': 0.187, 'compound': 0.9874},\n",
       " {'neg': 0.0, 'neu': 0.808, 'pos': 0.192, 'compound': 0.6105},\n",
       " {'neg': 0.057, 'neu': 0.598, 'pos': 0.344, 'compound': 0.9186},\n",
       " {'neg': 0.054, 'neu': 0.837, 'pos': 0.11, 'compound': 0.9659},\n",
       " {'neg': 0.059, 'neu': 0.851, 'pos': 0.09, 'compound': 0.4494},\n",
       " {'neg': 0.017, 'neu': 0.808, 'pos': 0.175, 'compound': 0.9848},\n",
       " {'neg': 0.012, 'neu': 0.632, 'pos': 0.356, 'compound': 0.989},\n",
       " {'neg': 0.009, 'neu': 0.799, 'pos': 0.191, 'compound': 0.9669},\n",
       " {'neg': 0.066, 'neu': 0.775, 'pos': 0.159, 'compound': 0.8012},\n",
       " {'neg': 0.075, 'neu': 0.809, 'pos': 0.115, 'compound': 0.6551},\n",
       " {'neg': 0.028, 'neu': 0.872, 'pos': 0.1, 'compound': 0.7183},\n",
       " {'neg': 0.296, 'neu': 0.704, 'pos': 0.0, 'compound': -0.8957},\n",
       " {'neg': 0.079, 'neu': 0.781, 'pos': 0.141, 'compound': 0.3595},\n",
       " {'neg': 0.109, 'neu': 0.777, 'pos': 0.115, 'compound': -0.0249},\n",
       " {'neg': 0.083, 'neu': 0.646, 'pos': 0.271, 'compound': 0.9698},\n",
       " {'neg': 0.023, 'neu': 0.747, 'pos': 0.23, 'compound': 0.9968},\n",
       " {'neg': 0.027, 'neu': 0.874, 'pos': 0.099, 'compound': 0.9842},\n",
       " {'neg': 0.045, 'neu': 0.912, 'pos': 0.043, 'compound': 0.2247},\n",
       " {'neg': 0.016, 'neu': 0.637, 'pos': 0.347, 'compound': 0.9916},\n",
       " {'neg': 0.0, 'neu': 0.781, 'pos': 0.219, 'compound': 0.9169},\n",
       " {'neg': 0.0, 'neu': 0.747, 'pos': 0.253, 'compound': 0.9908},\n",
       " {'neg': 0.055, 'neu': 0.786, 'pos': 0.159, 'compound': 0.4684},\n",
       " {'neg': 0.0, 'neu': 0.886, 'pos': 0.114, 'compound': 0.6249},\n",
       " {'neg': 0.0, 'neu': 0.615, 'pos': 0.385, 'compound': 0.9796},\n",
       " {'neg': 0.053, 'neu': 0.805, 'pos': 0.142, 'compound': 0.7268},\n",
       " {'neg': 0.162, 'neu': 0.838, 'pos': 0.0, 'compound': -0.7351},\n",
       " {'neg': 0.15, 'neu': 0.767, 'pos': 0.082, 'compound': -0.369},\n",
       " {'neg': 0.112, 'neu': 0.852, 'pos': 0.036, 'compound': -0.9778},\n",
       " {'neg': 0.0, 'neu': 0.854, 'pos': 0.146, 'compound': 0.5859},\n",
       " {'neg': 0.043, 'neu': 0.87, 'pos': 0.087, 'compound': 0.8276},\n",
       " {'neg': 0.047, 'neu': 0.908, 'pos': 0.044, 'compound': -0.3472},\n",
       " {'neg': 0.017, 'neu': 0.89, 'pos': 0.093, 'compound': 0.9464},\n",
       " {'neg': 0.087, 'neu': 0.758, 'pos': 0.155, 'compound': 0.8857},\n",
       " {'neg': 0.192, 'neu': 0.808, 'pos': 0.0, 'compound': -0.6116},\n",
       " {'neg': 0.019, 'neu': 0.906, 'pos': 0.074, 'compound': 0.8325},\n",
       " {'neg': 0.028, 'neu': 0.598, 'pos': 0.374, 'compound': 0.9676},\n",
       " {'neg': 0.032, 'neu': 0.723, 'pos': 0.244, 'compound': 0.8899},\n",
       " {'neg': 0.023, 'neu': 0.875, 'pos': 0.102, 'compound': 0.8735},\n",
       " {'neg': 0.0, 'neu': 0.751, 'pos': 0.249, 'compound': 0.9754},\n",
       " {'neg': 0.0, 'neu': 0.602, 'pos': 0.398, 'compound': 0.985},\n",
       " {'neg': 0.0, 'neu': 0.695, 'pos': 0.305, 'compound': 0.8779},\n",
       " {'neg': 0.0, 'neu': 0.898, 'pos': 0.102, 'compound': 0.8061},\n",
       " {'neg': 0.0, 'neu': 0.702, 'pos': 0.298, 'compound': 0.9801},\n",
       " {'neg': 0.089, 'neu': 0.833, 'pos': 0.078, 'compound': -0.5522},\n",
       " {'neg': 0.189, 'neu': 0.758, 'pos': 0.053, 'compound': -0.8315},\n",
       " {'neg': 0.048, 'neu': 0.617, 'pos': 0.335, 'compound': 0.9963},\n",
       " {'neg': 0.0, 'neu': 0.781, 'pos': 0.219, 'compound': 0.7096},\n",
       " {'neg': 0.031, 'neu': 0.762, 'pos': 0.208, 'compound': 0.9901},\n",
       " {'neg': 0.097, 'neu': 0.837, 'pos': 0.066, 'compound': -0.6774},\n",
       " {'neg': 0.075, 'neu': 0.835, 'pos': 0.089, 'compound': 0.5366},\n",
       " {'neg': 0.098, 'neu': 0.857, 'pos': 0.045, 'compound': -0.5165},\n",
       " {'neg': 0.18, 'neu': 0.724, 'pos': 0.096, 'compound': -0.6542},\n",
       " {'neg': 0.184, 'neu': 0.679, 'pos': 0.136, 'compound': -0.5385},\n",
       " {'neg': 0.203, 'neu': 0.712, 'pos': 0.085, 'compound': -0.8395},\n",
       " {'neg': 0.029, 'neu': 0.792, 'pos': 0.18, 'compound': 0.9084},\n",
       " {'neg': 0.026, 'neu': 0.754, 'pos': 0.22, 'compound': 0.9929},\n",
       " {'neg': 0.0, 'neu': 0.681, 'pos': 0.319, 'compound': 0.9646},\n",
       " {'neg': 0.017, 'neu': 0.858, 'pos': 0.125, 'compound': 0.9722},\n",
       " {'neg': 0.0, 'neu': 0.788, 'pos': 0.212, 'compound': 0.9072},\n",
       " {'neg': 0.146, 'neu': 0.656, 'pos': 0.199, 'compound': 0.6146},\n",
       " {'neg': 0.136, 'neu': 0.784, 'pos': 0.08, 'compound': -0.7154},\n",
       " {'neg': 0.019, 'neu': 0.883, 'pos': 0.098, 'compound': 0.8442},\n",
       " {'neg': 0.094, 'neu': 0.832, 'pos': 0.074, 'compound': -0.6997},\n",
       " {'neg': 0.0, 'neu': 0.785, 'pos': 0.215, 'compound': 0.9818},\n",
       " {'neg': 0.0, 'neu': 0.774, 'pos': 0.226, 'compound': 0.9688},\n",
       " {'neg': 0.117, 'neu': 0.862, 'pos': 0.021, 'compound': -0.8559},\n",
       " {'neg': 0.04, 'neu': 0.634, 'pos': 0.326, 'compound': 0.9091},\n",
       " {'neg': 0.215, 'neu': 0.704, 'pos': 0.081, 'compound': -0.6416},\n",
       " {'neg': 0.015, 'neu': 0.787, 'pos': 0.199, 'compound': 0.999},\n",
       " {'neg': 0.08, 'neu': 0.753, 'pos': 0.167, 'compound': 0.4753},\n",
       " {'neg': 0.039, 'neu': 0.883, 'pos': 0.078, 'compound': 0.7401},\n",
       " {'neg': 0.034, 'neu': 0.697, 'pos': 0.268, 'compound': 0.9549},\n",
       " {'neg': 0.048, 'neu': 0.813, 'pos': 0.139, 'compound': 0.9722},\n",
       " {'neg': 0.016, 'neu': 0.768, 'pos': 0.216, 'compound': 0.9381},\n",
       " {'neg': 0.169, 'neu': 0.727, 'pos': 0.104, 'compound': -0.9479},\n",
       " {'neg': 0.093, 'neu': 0.799, 'pos': 0.109, 'compound': 0.1799},\n",
       " {'neg': 0.07, 'neu': 0.811, 'pos': 0.119, 'compound': 0.8475},\n",
       " {'neg': 0.0, 'neu': 0.927, 'pos': 0.073, 'compound': 0.5927},\n",
       " {'neg': 0.046, 'neu': 0.662, 'pos': 0.291, 'compound': 0.9612},\n",
       " {'neg': 0.064, 'neu': 0.927, 'pos': 0.009, 'compound': -0.9539},\n",
       " {'neg': 0.0, 'neu': 0.567, 'pos': 0.433, 'compound': 0.9668},\n",
       " {'neg': 0.119, 'neu': 0.814, 'pos': 0.067, 'compound': -0.9337},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.019, 'neu': 0.593, 'pos': 0.388, 'compound': 0.9741},\n",
       " {'neg': 0.059, 'neu': 0.805, 'pos': 0.136, 'compound': 0.9092},\n",
       " {'neg': 0.056, 'neu': 0.784, 'pos': 0.16, 'compound': 0.9272},\n",
       " {'neg': 0.065, 'neu': 0.847, 'pos': 0.088, 'compound': 0.7357},\n",
       " {'neg': 0.0, 'neu': 0.873, 'pos': 0.127, 'compound': 0.7964},\n",
       " {'neg': 0.0, 'neu': 0.645, 'pos': 0.355, 'compound': 0.9514},\n",
       " {'neg': 0.025, 'neu': 0.671, 'pos': 0.303, 'compound': 0.9953},\n",
       " {'neg': 0.0, 'neu': 0.593, 'pos': 0.407, 'compound': 0.9741},\n",
       " {'neg': 0.031, 'neu': 0.742, 'pos': 0.228, 'compound': 0.9679},\n",
       " {'neg': 0.0, 'neu': 0.861, 'pos': 0.139, 'compound': 0.714},\n",
       " {'neg': 0.118, 'neu': 0.776, 'pos': 0.106, 'compound': -0.1429},\n",
       " {'neg': 0.0, 'neu': 0.699, 'pos': 0.301, 'compound': 0.9239},\n",
       " {'neg': 0.119, 'neu': 0.798, 'pos': 0.083, 'compound': 0.0},\n",
       " {'neg': 0.0, 'neu': 0.81, 'pos': 0.19, 'compound': 0.8271},\n",
       " {'neg': 0.0, 'neu': 0.591, 'pos': 0.409, 'compound': 0.886},\n",
       " {'neg': 0.134, 'neu': 0.749, 'pos': 0.117, 'compound': -0.7282},\n",
       " {'neg': 0.013, 'neu': 0.745, 'pos': 0.242, 'compound': 0.9748},\n",
       " {'neg': 0.061, 'neu': 0.882, 'pos': 0.057, 'compound': -0.1753},\n",
       " {'neg': 0.0, 'neu': 0.602, 'pos': 0.398, 'compound': 0.966},\n",
       " {'neg': 0.032, 'neu': 0.813, 'pos': 0.154, 'compound': 0.765},\n",
       " {'neg': 0.033, 'neu': 0.897, 'pos': 0.07, 'compound': 0.8661},\n",
       " {'neg': 0.0, 'neu': 0.562, 'pos': 0.438, 'compound': 0.9744},\n",
       " {'neg': 0.034, 'neu': 0.765, 'pos': 0.201, 'compound': 0.974},\n",
       " {'neg': 0.084, 'neu': 0.775, 'pos': 0.141, 'compound': 0.9566},\n",
       " {'neg': 0.0, 'neu': 0.717, 'pos': 0.283, 'compound': 0.877},\n",
       " {'neg': 0.03, 'neu': 0.722, 'pos': 0.248, 'compound': 0.9622},\n",
       " {'neg': 0.0, 'neu': 0.639, 'pos': 0.361, 'compound': 0.9661},\n",
       " {'neg': 0.121, 'neu': 0.811, 'pos': 0.068, 'compound': -0.3885},\n",
       " {'neg': 0.023, 'neu': 0.763, 'pos': 0.214, 'compound': 0.9906},\n",
       " {'neg': 0.0, 'neu': 0.658, 'pos': 0.342, 'compound': 0.9858},\n",
       " {'neg': 0.014, 'neu': 0.768, 'pos': 0.218, 'compound': 0.9807},\n",
       " {'neg': 0.097, 'neu': 0.63, 'pos': 0.273, 'compound': 0.8391},\n",
       " {'neg': 0.074, 'neu': 0.808, 'pos': 0.118, 'compound': 0.9861},\n",
       " {'neg': 0.061, 'neu': 0.777, 'pos': 0.163, 'compound': 0.7628},\n",
       " {'neg': 0.093, 'neu': 0.85, 'pos': 0.057, 'compound': -0.8863},\n",
       " {'neg': 0.0, 'neu': 0.752, 'pos': 0.248, 'compound': 0.9628},\n",
       " {'neg': 0.038, 'neu': 0.92, 'pos': 0.042, 'compound': 0.1258},\n",
       " {'neg': 0.045, 'neu': 0.918, 'pos': 0.037, 'compound': -0.3739},\n",
       " {'neg': 0.008, 'neu': 0.839, 'pos': 0.152, 'compound': 0.9955},\n",
       " {'neg': 0.048, 'neu': 0.916, 'pos': 0.036, 'compound': -0.3003},\n",
       " {'neg': 0.039, 'neu': 0.877, 'pos': 0.084, 'compound': 0.8331},\n",
       " {'neg': 0.084, 'neu': 0.82, 'pos': 0.096, 'compound': 0.4828},\n",
       " {'neg': 0.073, 'neu': 0.767, 'pos': 0.16, 'compound': 0.7592},\n",
       " {'neg': 0.0, 'neu': 0.864, 'pos': 0.136, 'compound': 0.8243},\n",
       " {'neg': 0.0, 'neu': 0.697, 'pos': 0.303, 'compound': 0.8126},\n",
       " {'neg': 0.198, 'neu': 0.74, 'pos': 0.061, 'compound': -0.6679},\n",
       " {'neg': 0.0, 'neu': 0.878, 'pos': 0.122, 'compound': 0.9509},\n",
       " {'neg': 0.082, 'neu': 0.731, 'pos': 0.187, 'compound': 0.8822},\n",
       " {'neg': 0.033, 'neu': 0.849, 'pos': 0.118, 'compound': 0.5922},\n",
       " {'neg': 0.076, 'neu': 0.767, 'pos': 0.157, 'compound': 0.9361},\n",
       " {'neg': 0.0, 'neu': 0.777, 'pos': 0.223, 'compound': 0.8221},\n",
       " {'neg': 0.075, 'neu': 0.877, 'pos': 0.048, 'compound': -0.4939},\n",
       " {'neg': 0.026, 'neu': 0.821, 'pos': 0.152, 'compound': 0.9099},\n",
       " {'neg': 0.028, 'neu': 0.908, 'pos': 0.065, 'compound': 0.5267},\n",
       " {'neg': 0.067, 'neu': 0.846, 'pos': 0.087, 'compound': 0.7297},\n",
       " {'neg': 0.068, 'neu': 0.841, 'pos': 0.091, 'compound': 0.5119},\n",
       " {'neg': 0.041, 'neu': 0.928, 'pos': 0.031, 'compound': -0.3887},\n",
       " {'neg': 0.106, 'neu': 0.802, 'pos': 0.092, 'compound': -0.5483},\n",
       " {'neg': 0.03, 'neu': 0.815, 'pos': 0.155, 'compound': 0.8663},\n",
       " {'neg': 0.135, 'neu': 0.813, 'pos': 0.052, 'compound': -0.9213},\n",
       " {'neg': 0.0, 'neu': 0.971, 'pos': 0.029, 'compound': 0.4939},\n",
       " {'neg': 0.029, 'neu': 0.734, 'pos': 0.237, 'compound': 0.913},\n",
       " {'neg': 0.054, 'neu': 0.822, 'pos': 0.123, 'compound': 0.9869},\n",
       " {'neg': 0.087, 'neu': 0.775, 'pos': 0.138, 'compound': 0.1635},\n",
       " {'neg': 0.0, 'neu': 0.873, 'pos': 0.127, 'compound': 0.1531},\n",
       " {'neg': 0.091, 'neu': 0.867, 'pos': 0.042, 'compound': -0.7569},\n",
       " {'neg': 0.116, 'neu': 0.816, 'pos': 0.069, 'compound': -0.9682},\n",
       " {'neg': 0.0, 'neu': 0.682, 'pos': 0.318, 'compound': 0.908},\n",
       " {'neg': 0.035, 'neu': 0.743, 'pos': 0.221, 'compound': 0.9758},\n",
       " {'neg': 0.038, 'neu': 0.643, 'pos': 0.318, 'compound': 0.9146},\n",
       " {'neg': 0.0, 'neu': 0.779, 'pos': 0.221, 'compound': 0.6597},\n",
       " {'neg': 0.085, 'neu': 0.813, 'pos': 0.101, 'compound': 0.2492},\n",
       " {'neg': 0.115, 'neu': 0.683, 'pos': 0.202, 'compound': 0.9032},\n",
       " {'neg': 0.019, 'neu': 0.862, 'pos': 0.118, 'compound': 0.952},\n",
       " {'neg': 0.0, 'neu': 0.788, 'pos': 0.212, 'compound': 0.875},\n",
       " {'neg': 0.0, 'neu': 0.666, 'pos': 0.334, 'compound': 0.9835},\n",
       " {'neg': 0.0, 'neu': 0.59, 'pos': 0.41, 'compound': 0.9918},\n",
       " {'neg': 0.054, 'neu': 0.586, 'pos': 0.36, 'compound': 0.807},\n",
       " {'neg': 0.025, 'neu': 0.78, 'pos': 0.195, 'compound': 0.9771},\n",
       " {'neg': 0.152, 'neu': 0.629, 'pos': 0.219, 'compound': 0.3693},\n",
       " {'neg': 0.0, 'neu': 0.63, 'pos': 0.37, 'compound': 0.9168},\n",
       " {'neg': 0.011, 'neu': 0.8, 'pos': 0.189, 'compound': 0.9803},\n",
       " {'neg': 0.055, 'neu': 0.874, 'pos': 0.071, 'compound': 0.4102},\n",
       " {'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'compound': 0.9744},\n",
       " {'neg': 0.0, 'neu': 0.64, 'pos': 0.36, 'compound': 0.9674},\n",
       " {'neg': 0.144, 'neu': 0.819, 'pos': 0.036, 'compound': -0.8062},\n",
       " {'neg': 0.066, 'neu': 0.887, 'pos': 0.047, 'compound': -0.4986},\n",
       " {'neg': 0.095, 'neu': 0.856, 'pos': 0.049, 'compound': -0.681},\n",
       " {'neg': 0.105, 'neu': 0.778, 'pos': 0.117, 'compound': -0.0534},\n",
       " {'neg': 0.0, 'neu': 0.657, 'pos': 0.343, 'compound': 0.9542},\n",
       " {'neg': 0.0, 'neu': 0.866, 'pos': 0.134, 'compound': 0.8892},\n",
       " {'neg': 0.053, 'neu': 0.833, 'pos': 0.114, 'compound': 0.9442},\n",
       " {'neg': 0.129, 'neu': 0.808, 'pos': 0.063, 'compound': -0.9137},\n",
       " {'neg': 0.018, 'neu': 0.744, 'pos': 0.238, 'compound': 0.973},\n",
       " {'neg': 0.022, 'neu': 0.733, 'pos': 0.245, 'compound': 0.9496},\n",
       " {'neg': 0.039, 'neu': 0.86, 'pos': 0.101, 'compound': 0.8786},\n",
       " {'neg': 0.0, 'neu': 0.662, 'pos': 0.338, 'compound': 0.8655},\n",
       " {'neg': 0.065, 'neu': 0.828, 'pos': 0.107, 'compound': 0.4102},\n",
       " {'neg': 0.0, 'neu': 0.732, 'pos': 0.268, 'compound': 0.8307},\n",
       " {'neg': 0.0, 'neu': 0.7, 'pos': 0.3, 'compound': 0.9557},\n",
       " {'neg': 0.053, 'neu': 0.868, 'pos': 0.08, 'compound': 0.5207},\n",
       " {'neg': 0.377, 'neu': 0.623, 'pos': 0.0, 'compound': -0.9169},\n",
       " {'neg': 0.079, 'neu': 0.805, 'pos': 0.116, 'compound': 0.367},\n",
       " {'neg': 0.0, 'neu': 0.361, 'pos': 0.639, 'compound': 0.9694},\n",
       " {'neg': 0.13, 'neu': 0.648, 'pos': 0.222, 'compound': 0.4007},\n",
       " {'neg': 0.037, 'neu': 0.806, 'pos': 0.158, 'compound': 0.9635},\n",
       " {'neg': 0.072, 'neu': 0.928, 'pos': 0.0, 'compound': -0.6124},\n",
       " {'neg': 0.02, 'neu': 0.966, 'pos': 0.014, 'compound': -0.2407},\n",
       " {'neg': 0.072, 'neu': 0.86, 'pos': 0.067, 'compound': -0.0516},\n",
       " {'neg': 0.0, 'neu': 0.617, 'pos': 0.383, 'compound': 0.9283},\n",
       " {'neg': 0.0, 'neu': 0.715, 'pos': 0.285, 'compound': 0.9524},\n",
       " {'neg': 0.307, 'neu': 0.617, 'pos': 0.076, 'compound': -0.8671},\n",
       " {'neg': 0.072, 'neu': 0.886, 'pos': 0.042, 'compound': -0.8046},\n",
       " {'neg': 0.039, 'neu': 0.857, 'pos': 0.104, 'compound': 0.9963},\n",
       " {'neg': 0.0, 'neu': 0.798, 'pos': 0.202, 'compound': 0.9958},\n",
       " {'neg': 0.027, 'neu': 0.715, 'pos': 0.258, 'compound': 0.9748},\n",
       " {'neg': 0.023, 'neu': 0.785, 'pos': 0.192, 'compound': 0.9406},\n",
       " {'neg': 0.012, 'neu': 0.773, 'pos': 0.215, 'compound': 0.9762},\n",
       " {'neg': 0.055, 'neu': 0.681, 'pos': 0.264, 'compound': 0.9632},\n",
       " {'neg': 0.033, 'neu': 0.608, 'pos': 0.359, 'compound': 0.9625},\n",
       " {'neg': 0.0, 'neu': 0.867, 'pos': 0.133, 'compound': 0.69},\n",
       " {'neg': 0.046, 'neu': 0.794, 'pos': 0.16, 'compound': 0.9568},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.049, 'neu': 0.738, 'pos': 0.213, 'compound': 0.9732},\n",
       " {'neg': 0.092, 'neu': 0.871, 'pos': 0.037, 'compound': -0.7531},\n",
       " {'neg': 0.056, 'neu': 0.841, 'pos': 0.102, 'compound': 0.6755},\n",
       " {'neg': 0.239, 'neu': 0.684, 'pos': 0.077, 'compound': -0.6908},\n",
       " {'neg': 0.018, 'neu': 0.804, 'pos': 0.179, 'compound': 0.9546},\n",
       " {'neg': 0.008, 'neu': 0.794, 'pos': 0.198, 'compound': 0.991},\n",
       " {'neg': 0.072, 'neu': 0.632, 'pos': 0.297, 'compound': 0.9928},\n",
       " {'neg': 0.082, 'neu': 0.765, 'pos': 0.153, 'compound': 0.4019},\n",
       " {'neg': 0.097, 'neu': 0.816, 'pos': 0.087, 'compound': -0.3525},\n",
       " {'neg': 0.035, 'neu': 0.612, 'pos': 0.353, 'compound': 0.926},\n",
       " {'neg': 0.033, 'neu': 0.652, 'pos': 0.315, 'compound': 0.8848},\n",
       " {'neg': 0.0, 'neu': 0.723, 'pos': 0.277, 'compound': 0.807},\n",
       " {'neg': 0.07, 'neu': 0.553, 'pos': 0.377, 'compound': 0.8957},\n",
       " {'neg': 0.0, 'neu': 0.837, 'pos': 0.163, 'compound': 0.8885},\n",
       " {'neg': 0.0, 'neu': 0.711, 'pos': 0.289, 'compound': 0.9926},\n",
       " {'neg': 0.063, 'neu': 0.836, 'pos': 0.101, 'compound': 0.4878},\n",
       " {'neg': 0.137, 'neu': 0.696, 'pos': 0.167, 'compound': 0.4743},\n",
       " {'neg': 0.084, 'neu': 0.837, 'pos': 0.079, 'compound': -0.2424},\n",
       " {'neg': 0.097, 'neu': 0.812, 'pos': 0.091, 'compound': -0.6579},\n",
       " {'neg': 0.0, 'neu': 0.679, 'pos': 0.321, 'compound': 0.9519},\n",
       " {'neg': 0.0, 'neu': 0.632, 'pos': 0.368, 'compound': 0.8663},\n",
       " {'neg': 0.044, 'neu': 0.724, 'pos': 0.232, 'compound': 0.9905},\n",
       " {'neg': 0.083, 'neu': 0.783, 'pos': 0.134, 'compound': 0.9362},\n",
       " {'neg': 0.032, 'neu': 0.713, 'pos': 0.254, 'compound': 0.9458},\n",
       " {'neg': 0.0, 'neu': 0.849, 'pos': 0.151, 'compound': 0.749},\n",
       " {'neg': 0.255, 'neu': 0.66, 'pos': 0.086, 'compound': -0.8492},\n",
       " {'neg': 0.0, 'neu': 0.777, 'pos': 0.223, 'compound': 0.9118},\n",
       " {'neg': 0.038, 'neu': 0.82, 'pos': 0.142, 'compound': 0.7345},\n",
       " {'neg': 0.084, 'neu': 0.824, 'pos': 0.092, 'compound': 0.4424},\n",
       " {'neg': 0.077, 'neu': 0.798, 'pos': 0.125, 'compound': 0.2263},\n",
       " {'neg': 0.0, 'neu': 0.684, 'pos': 0.316, 'compound': 0.9903},\n",
       " {'neg': 0.039, 'neu': 0.864, 'pos': 0.098, 'compound': 0.9394},\n",
       " {'neg': 0.067, 'neu': 0.865, 'pos': 0.068, 'compound': 0.2952},\n",
       " {'neg': 0.0, 'neu': 0.738, 'pos': 0.262, 'compound': 0.7964},\n",
       " {'neg': 0.083, 'neu': 0.82, 'pos': 0.097, 'compound': 0.7077},\n",
       " {'neg': 0.091, 'neu': 0.757, 'pos': 0.151, 'compound': 0.2846},\n",
       " {'neg': 0.0, 'neu': 0.582, 'pos': 0.418, 'compound': 0.8883},\n",
       " {'neg': 0.0, 'neu': 0.568, 'pos': 0.432, 'compound': 0.9759},\n",
       " {'neg': 0.0, 'neu': 0.719, 'pos': 0.281, 'compound': 0.9745},\n",
       " {'neg': 0.061, 'neu': 0.816, 'pos': 0.123, 'compound': 0.9871},\n",
       " {'neg': 0.0, 'neu': 0.733, 'pos': 0.267, 'compound': 0.8988},\n",
       " {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'compound': 0.9736},\n",
       " {'neg': 0.036, 'neu': 0.847, 'pos': 0.117, 'compound': 0.9186},\n",
       " {'neg': 0.0, 'neu': 0.687, 'pos': 0.313, 'compound': 0.9901},\n",
       " {'neg': 0.0, 'neu': 0.534, 'pos': 0.466, 'compound': 0.9403},\n",
       " {'neg': 0.054, 'neu': 0.897, 'pos': 0.05, 'compound': 0.1583},\n",
       " {'neg': 0.0, 'neu': 0.791, 'pos': 0.209, 'compound': 0.9481},\n",
       " {'neg': 0.063, 'neu': 0.753, 'pos': 0.184, 'compound': 0.758},\n",
       " {'neg': 0.052, 'neu': 0.922, 'pos': 0.026, 'compound': -0.611},\n",
       " {'neg': 0.072, 'neu': 0.797, 'pos': 0.131, 'compound': 0.9497},\n",
       " {'neg': 0.224, 'neu': 0.662, 'pos': 0.114, 'compound': -0.7326},\n",
       " {'neg': 0.021, 'neu': 0.841, 'pos': 0.139, 'compound': 0.9856},\n",
       " {'neg': 0.14, 'neu': 0.749, 'pos': 0.111, 'compound': 0.1531},\n",
       " {'neg': 0.07, 'neu': 0.808, 'pos': 0.123, 'compound': 0.8664},\n",
       " {'neg': 0.053, 'neu': 0.68, 'pos': 0.267, 'compound': 0.979},\n",
       " {'neg': 0.067, 'neu': 0.784, 'pos': 0.148, 'compound': 0.7774},\n",
       " {'neg': 0.063, 'neu': 0.797, 'pos': 0.14, 'compound': 0.9518},\n",
       " {'neg': 0.0, 'neu': 0.752, 'pos': 0.248, 'compound': 0.993},\n",
       " {'neg': 0.059, 'neu': 0.748, 'pos': 0.193, 'compound': 0.9618},\n",
       " {'neg': 0.047, 'neu': 0.77, 'pos': 0.183, 'compound': 0.7549},\n",
       " {'neg': 0.0, 'neu': 0.773, 'pos': 0.227, 'compound': 0.9749},\n",
       " {'neg': 0.0, 'neu': 0.603, 'pos': 0.397, 'compound': 0.9768},\n",
       " {'neg': 0.028, 'neu': 0.592, 'pos': 0.38, 'compound': 0.9683},\n",
       " {'neg': 0.016, 'neu': 0.889, 'pos': 0.095, 'compound': 0.9393},\n",
       " {'neg': 0.164, 'neu': 0.688, 'pos': 0.147, 'compound': 0.1406},\n",
       " {'neg': 0.113, 'neu': 0.795, 'pos': 0.092, 'compound': -0.8707},\n",
       " {'neg': 0.156, 'neu': 0.78, 'pos': 0.064, 'compound': -0.566},\n",
       " {'neg': 0.02, 'neu': 0.794, 'pos': 0.186, 'compound': 0.9932},\n",
       " {'neg': 0.098, 'neu': 0.902, 'pos': 0.0, 'compound': -0.0772},\n",
       " {'neg': 0.154, 'neu': 0.806, 'pos': 0.04, 'compound': -0.6996},\n",
       " {'neg': 0.014, 'neu': 0.795, 'pos': 0.191, 'compound': 0.9786},\n",
       " {'neg': 0.0, 'neu': 0.692, 'pos': 0.308, 'compound': 0.9467},\n",
       " {'neg': 0.085, 'neu': 0.743, 'pos': 0.172, 'compound': 0.9636},\n",
       " {'neg': 0.0, 'neu': 0.75, 'pos': 0.25, 'compound': 0.9072},\n",
       " {'neg': 0.0, 'neu': 0.664, 'pos': 0.336, 'compound': 0.7634},\n",
       " {'neg': 0.0, 'neu': 0.645, 'pos': 0.355, 'compound': 0.9564},\n",
       " {'neg': 0.075, 'neu': 0.886, 'pos': 0.039, 'compound': -0.7506},\n",
       " {'neg': 0.157, 'neu': 0.643, 'pos': 0.2, 'compound': 0.6364},\n",
       " {'neg': 0.027, 'neu': 0.817, 'pos': 0.156, 'compound': 0.9826},\n",
       " {'neg': 0.024, 'neu': 0.728, 'pos': 0.248, 'compound': 0.9127},\n",
       " {'neg': 0.0, 'neu': 0.842, 'pos': 0.158, 'compound': 0.9091},\n",
       " {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.107, 'neu': 0.893, 'pos': 0.0, 'compound': -0.587},\n",
       " {'neg': 0.108, 'neu': 0.833, 'pos': 0.06, 'compound': -0.8428},\n",
       " {'neg': 0.023, 'neu': 0.85, 'pos': 0.126, 'compound': 0.9418},\n",
       " {'neg': 0.026, 'neu': 0.844, 'pos': 0.13, 'compound': 0.8122},\n",
       " {'neg': 0.076, 'neu': 0.863, 'pos': 0.061, 'compound': -0.4351},\n",
       " {'neg': 0.033, 'neu': 0.695, 'pos': 0.273, 'compound': 0.8944},\n",
       " {'neg': 0.06, 'neu': 0.904, 'pos': 0.036, 'compound': -0.2212},\n",
       " {'neg': 0.0, 'neu': 0.374, 'pos': 0.626, 'compound': 0.9499},\n",
       " {'neg': 0.176, 'neu': 0.824, 'pos': 0.0, 'compound': -0.6027},\n",
       " {'neg': 0.026, 'neu': 0.792, 'pos': 0.182, 'compound': 0.9885},\n",
       " {'neg': 0.0, 'neu': 0.796, 'pos': 0.204, 'compound': 0.8834},\n",
       " {'neg': 0.0, 'neu': 0.824, 'pos': 0.176, 'compound': 0.7184},\n",
       " {'neg': 0.043, 'neu': 0.762, 'pos': 0.196, 'compound': 0.9833},\n",
       " {'neg': 0.0, 'neu': 0.811, 'pos': 0.189, 'compound': 0.8824},\n",
       " {'neg': 0.0, 'neu': 0.913, 'pos': 0.087, 'compound': 0.6369},\n",
       " {'neg': 0.0, 'neu': 0.432, 'pos': 0.568, 'compound': 0.9499},\n",
       " {'neg': 0.066, 'neu': 0.895, 'pos': 0.039, 'compound': -0.2023},\n",
       " {'neg': 0.065, 'neu': 0.88, 'pos': 0.055, 'compound': -0.2149},\n",
       " {'neg': 0.0, 'neu': 0.61, 'pos': 0.39, 'compound': 0.9686},\n",
       " {'neg': 0.0, 'neu': 0.532, 'pos': 0.468, 'compound': 0.9495},\n",
       " {'neg': 0.014, 'neu': 0.868, 'pos': 0.118, 'compound': 0.9485},\n",
       " {'neg': 0.079, 'neu': 0.766, 'pos': 0.155, 'compound': 0.8272},\n",
       " {'neg': 0.027, 'neu': 0.782, 'pos': 0.192, 'compound': 0.9513},\n",
       " {'neg': 0.185, 'neu': 0.815, 'pos': 0.0, 'compound': -0.6994},\n",
       " {'neg': 0.0, 'neu': 0.451, 'pos': 0.549, 'compound': 0.9438},\n",
       " {'neg': 0.0, 'neu': 0.69, 'pos': 0.31, 'compound': 0.992},\n",
       " {'neg': 0.223, 'neu': 0.645, 'pos': 0.132, 'compound': -0.2023},\n",
       " {'neg': 0.013, 'neu': 0.665, 'pos': 0.322, 'compound': 0.9934},\n",
       " {'neg': 0.028, 'neu': 0.806, 'pos': 0.167, 'compound': 0.9896},\n",
       " {'neg': 0.037, 'neu': 0.907, 'pos': 0.056, 'compound': 0.1901},\n",
       " {'neg': 0.104, 'neu': 0.758, 'pos': 0.138, 'compound': 0.1027},\n",
       " {'neg': 0.242, 'neu': 0.694, 'pos': 0.063, 'compound': -0.8412},\n",
       " {'neg': 0.067, 'neu': 0.788, 'pos': 0.145, 'compound': 0.4939},\n",
       " {'neg': 0.022, 'neu': 0.831, 'pos': 0.147, 'compound': 0.8214},\n",
       " {'neg': 0.042, 'neu': 0.651, 'pos': 0.306, 'compound': 0.9536},\n",
       " {'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'compound': 0.9674},\n",
       " {'neg': 0.067, 'neu': 0.909, 'pos': 0.025, 'compound': -0.8934},\n",
       " {'neg': 0.031, 'neu': 0.936, 'pos': 0.033, 'compound': 0.3415},\n",
       " {'neg': 0.023, 'neu': 0.741, 'pos': 0.235, 'compound': 0.953},\n",
       " {'neg': 0.0, 'neu': 0.863, 'pos': 0.137, 'compound': 0.7351},\n",
       " {'neg': 0.056, 'neu': 0.804, 'pos': 0.14, 'compound': 0.9411},\n",
       " {'neg': 0.034, 'neu': 0.742, 'pos': 0.224, 'compound': 0.9914},\n",
       " {'neg': 0.0, 'neu': 0.844, 'pos': 0.156, 'compound': 0.5719},\n",
       " {'neg': 0.027, 'neu': 0.796, 'pos': 0.177, 'compound': 0.9168},\n",
       " {'neg': 0.025, 'neu': 0.756, 'pos': 0.219, 'compound': 0.9985},\n",
       " {'neg': 0.045, 'neu': 0.806, 'pos': 0.149, 'compound': 0.9934},\n",
       " {'neg': 0.041, 'neu': 0.913, 'pos': 0.047, 'compound': -0.4503},\n",
       " {'neg': 0.024, 'neu': 0.937, 'pos': 0.04, 'compound': 0.4423},\n",
       " {'neg': 0.0, 'neu': 0.644, 'pos': 0.356, 'compound': 0.8739},\n",
       " {'neg': 0.084, 'neu': 0.702, 'pos': 0.214, 'compound': 0.9731},\n",
       " {'neg': 0.037, 'neu': 0.748, 'pos': 0.215, 'compound': 0.9618},\n",
       " {'neg': 0.121, 'neu': 0.856, 'pos': 0.024, 'compound': -0.7478},\n",
       " {'neg': 0.043, 'neu': 0.899, 'pos': 0.058, 'compound': 0.34},\n",
       " {'neg': 0.064, 'neu': 0.81, 'pos': 0.126, 'compound': 0.9582},\n",
       " {'neg': 0.039, 'neu': 0.704, 'pos': 0.257, 'compound': 0.9633},\n",
       " {'neg': 0.045, 'neu': 0.769, 'pos': 0.186, 'compound': 0.7902},\n",
       " {'neg': 0.0, 'neu': 0.708, 'pos': 0.292, 'compound': 0.9811},\n",
       " {'neg': 0.014, 'neu': 0.829, 'pos': 0.157, 'compound': 0.9915},\n",
       " {'neg': 0.336, 'neu': 0.542, 'pos': 0.123, 'compound': -0.8166},\n",
       " {'neg': 0.075, 'neu': 0.728, 'pos': 0.197, 'compound': 0.9682},\n",
       " {'neg': 0.034, 'neu': 0.643, 'pos': 0.323, 'compound': 0.9814},\n",
       " {'neg': 0.224, 'neu': 0.65, 'pos': 0.126, 'compound': -0.7662},\n",
       " {'neg': 0.027, 'neu': 0.938, 'pos': 0.034, 'compound': -0.2617},\n",
       " {'neg': 0.068, 'neu': 0.893, 'pos': 0.04, 'compound': -0.8585},\n",
       " {'neg': 0.033, 'neu': 0.789, 'pos': 0.178, 'compound': 0.9773},\n",
       " {'neg': 0.018, 'neu': 0.84, 'pos': 0.142, 'compound': 0.8614},\n",
       " {'neg': 0.0, 'neu': 0.468, 'pos': 0.532, 'compound': 0.9601},\n",
       " {'neg': 0.041, 'neu': 0.931, 'pos': 0.027, 'compound': -0.3653},\n",
       " {'neg': 0.058, 'neu': 0.842, 'pos': 0.101, 'compound': 0.5823},\n",
       " {'neg': 0.037, 'neu': 0.886, 'pos': 0.076, 'compound': 0.8194},\n",
       " {'neg': 0.08, 'neu': 0.882, 'pos': 0.038, 'compound': -0.7337},\n",
       " {'neg': 0.0, 'neu': 0.809, 'pos': 0.191, 'compound': 0.8179},\n",
       " {'neg': 0.0, 'neu': 0.861, 'pos': 0.139, 'compound': 0.8687},\n",
       " {'neg': 0.027, 'neu': 0.826, 'pos': 0.147, 'compound': 0.9862},\n",
       " {'neg': 0.105, 'neu': 0.895, 'pos': 0.0, 'compound': -0.905},\n",
       " {'neg': 0.0, 'neu': 0.675, 'pos': 0.325, 'compound': 0.847},\n",
       " {'neg': 0.071, 'neu': 0.704, 'pos': 0.225, 'compound': 0.9769},\n",
       " {'neg': 0.104, 'neu': 0.896, 'pos': 0.0, 'compound': -0.5881},\n",
       " {'neg': 0.016, 'neu': 0.787, 'pos': 0.197, 'compound': 0.9834},\n",
       " {'neg': 0.05, 'neu': 0.689, 'pos': 0.261, 'compound': 0.8096},\n",
       " {'neg': 0.0, 'neu': 0.74, 'pos': 0.26, 'compound': 0.9411},\n",
       " {'neg': 0.075, 'neu': 0.745, 'pos': 0.18, 'compound': 0.7457},\n",
       " {'neg': 0.019, 'neu': 0.744, 'pos': 0.237, 'compound': 0.9743},\n",
       " {'neg': 0.07, 'neu': 0.833, 'pos': 0.097, 'compound': 0.5453},\n",
       " {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.9876},\n",
       " {'neg': 0.07, 'neu': 0.743, 'pos': 0.187, 'compound': 0.9991},\n",
       " {'neg': 0.0, 'neu': 0.886, 'pos': 0.114, 'compound': 0.4019},\n",
       " {'neg': 0.091, 'neu': 0.838, 'pos': 0.071, 'compound': -0.4118},\n",
       " {'neg': 0.06, 'neu': 0.804, 'pos': 0.136, 'compound': 0.9241},\n",
       " {'neg': 0.27, 'neu': 0.648, 'pos': 0.082, 'compound': -0.6851},\n",
       " {'neg': 0.0, 'neu': 0.754, 'pos': 0.246, 'compound': 0.8176},\n",
       " {'neg': 0.039, 'neu': 0.833, 'pos': 0.128, 'compound': 0.631},\n",
       " {'neg': 0.127, 'neu': 0.785, 'pos': 0.088, 'compound': -0.8865},\n",
       " {'neg': 0.199, 'neu': 0.801, 'pos': 0.0, 'compound': -0.9278},\n",
       " {'neg': 0.0, 'neu': 0.581, 'pos': 0.419, 'compound': 0.905},\n",
       " {'neg': 0.067, 'neu': 0.837, 'pos': 0.096, 'compound': 0.5812},\n",
       " {'neg': 0.025, 'neu': 0.755, 'pos': 0.22, 'compound': 0.9407},\n",
       " {'neg': 0.041, 'neu': 0.718, 'pos': 0.241, 'compound': 0.9823},\n",
       " {'neg': 0.0, 'neu': 0.761, 'pos': 0.239, 'compound': 0.9833},\n",
       " {'neg': 0.097, 'neu': 0.619, 'pos': 0.283, 'compound': 0.4953},\n",
       " {'neg': 0.043, 'neu': 0.719, 'pos': 0.237, 'compound': 0.8437},\n",
       " {'neg': 0.0, 'neu': 0.852, 'pos': 0.148, 'compound': 0.7884},\n",
       " {'neg': 0.057, 'neu': 0.768, 'pos': 0.175, 'compound': 0.9661},\n",
       " {'neg': 0.131, 'neu': 0.834, 'pos': 0.035, 'compound': -0.7105},\n",
       " {'neg': 0.089, 'neu': 0.81, 'pos': 0.101, 'compound': 0.0772},\n",
       " {'neg': 0.112, 'neu': 0.845, 'pos': 0.044, 'compound': -0.7572},\n",
       " {'neg': 0.058, 'neu': 0.803, 'pos': 0.139, 'compound': 0.959},\n",
       " {'neg': 0.082, 'neu': 0.8, 'pos': 0.117, 'compound': 0.8783},\n",
       " {'neg': 0.0, 'neu': 0.6, 'pos': 0.4, 'compound': 0.9134},\n",
       " {'neg': 0.038, 'neu': 0.83, 'pos': 0.133, 'compound': 0.4611},\n",
       " {'neg': 0.039, 'neu': 0.5, 'pos': 0.461, 'compound': 0.9764},\n",
       " {'neg': 0.12, 'neu': 0.69, 'pos': 0.19, 'compound': 0.4478},\n",
       " {'neg': 0.047, 'neu': 0.871, 'pos': 0.081, 'compound': 0.6272},\n",
       " {'neg': 0.033, 'neu': 0.695, 'pos': 0.272, 'compound': 0.969},\n",
       " {'neg': 0.0, 'neu': 0.598, 'pos': 0.402, 'compound': 0.8555},\n",
       " {'neg': 0.024, 'neu': 0.819, 'pos': 0.157, 'compound': 0.8897},\n",
       " {'neg': 0.0, 'neu': 0.689, 'pos': 0.311, 'compound': 0.9501},\n",
       " {'neg': 0.083, 'neu': 0.715, 'pos': 0.202, 'compound': 0.9789},\n",
       " {'neg': 0.042, 'neu': 0.936, 'pos': 0.022, 'compound': -0.203},\n",
       " {'neg': 0.0, 'neu': 0.74, 'pos': 0.26, 'compound': 0.985},\n",
       " {'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'compound': 0.8864},\n",
       " {'neg': 0.016, 'neu': 0.878, 'pos': 0.106, 'compound': 0.8928},\n",
       " {'neg': 0.09, 'neu': 0.804, 'pos': 0.106, 'compound': 0.8264},\n",
       " {'neg': 0.047, 'neu': 0.807, 'pos': 0.146, 'compound': 0.9827},\n",
       " {'neg': 0.094, 'neu': 0.824, 'pos': 0.082, 'compound': -0.2023},\n",
       " {'neg': 0.055, 'neu': 0.852, 'pos': 0.093, 'compound': 0.8515},\n",
       " {'neg': 0.0, 'neu': 0.703, 'pos': 0.297, 'compound': 0.9708},\n",
       " {'neg': 0.27, 'neu': 0.624, 'pos': 0.107, 'compound': -0.8057},\n",
       " {'neg': 0.047, 'neu': 0.892, 'pos': 0.061, 'compound': -0.0152},\n",
       " {'neg': 0.0, 'neu': 0.73, 'pos': 0.27, 'compound': 0.8625},\n",
       " {'neg': 0.0, 'neu': 0.766, 'pos': 0.234, 'compound': 0.9042},\n",
       " {'neg': 0.017, 'neu': 0.891, 'pos': 0.092, 'compound': 0.9326},\n",
       " {'neg': 0.024, 'neu': 0.915, 'pos': 0.06, 'compound': 0.552},\n",
       " {'neg': 0.035, 'neu': 0.704, 'pos': 0.261, 'compound': 0.8915},\n",
       " {'neg': 0.026, 'neu': 0.858, 'pos': 0.116, 'compound': 0.9946},\n",
       " {'neg': 0.103, 'neu': 0.673, 'pos': 0.224, 'compound': 0.9816},\n",
       " {'neg': 0.007, 'neu': 0.871, 'pos': 0.122, 'compound': 0.9896},\n",
       " {'neg': 0.015, 'neu': 0.836, 'pos': 0.149, 'compound': 0.9765},\n",
       " {'neg': 0.018, 'neu': 0.765, 'pos': 0.217, 'compound': 0.9954},\n",
       " {'neg': 0.035, 'neu': 0.792, 'pos': 0.173, 'compound': 0.9907},\n",
       " {'neg': 0.0, 'neu': 0.946, 'pos': 0.054, 'compound': 0.4201},\n",
       " {'neg': 0.0, 'neu': 0.7, 'pos': 0.3, 'compound': 0.8402},\n",
       " {'neg': 0.096, 'neu': 0.653, 'pos': 0.251, 'compound': 0.6705},\n",
       " {'neg': 0.037, 'neu': 0.953, 'pos': 0.01, 'compound': -0.5919},\n",
       " {'neg': 0.025, 'neu': 0.676, 'pos': 0.299, 'compound': 0.9248},\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VADER sentiment analysis\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "res = []\n",
    "y_pred_vader = []\n",
    "for review in review_df['text']:\n",
    "    sentiment = vader.polarity_scores(review)\n",
    "    res.append(sentiment)\n",
    "    pred = round(sentiment['compound'])\n",
    "    y_pred_vader.append(pred)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance metrics - VADER\n",
    "\n",
    "y_true = review_df['sentiment']\n",
    "y_true = y_true.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1887 1305 1299]\n",
      " [  90  267  910]\n",
      " [  52  332 5544]]\n",
      "Accuracy: 0.6587369501968167\n",
      "Precision: 0.7353544226999599\n",
      "Recall: 0.6587369501968167\n",
      "F1-score: 0.651836225229989\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true, y_pred_vader)\n",
    "accuracy = accuracy_score(y_true, y_pred_vader)\n",
    "precision = precision_score(y_true, y_pred_vader, average='weighted')\n",
    "recall = recall_score(y_true, y_pred_vader, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred_vader, average='weighted')\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLzom-2TITa4gasV7_fCCA</td>\n",
       "      <td>Great experience purchasing a washer and dryer...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a5JHzBrWxRd_OmIvV7znDA</td>\n",
       "      <td>Went here based on the high ratings and raves ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.4508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X-o--dwf0HuFMittYi4wCA</td>\n",
       "      <td>oh Millers, how i wanted to like you.  You are...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INGNbsyo-MouZZzcxnCSGQ</td>\n",
       "      <td>This place gets two stars from me only because...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.5660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>k7VatXVLism-cTDJE8TTUw</td>\n",
       "      <td>This place was awesome. Clean, beautiful and t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11681</th>\n",
       "      <td>IlU-MQzMKc7jAHWwK5VFGQ</td>\n",
       "      <td>To be fair, I tried them in their first week. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11682</th>\n",
       "      <td>Qt3BsRvQuJccDQfFWM1XPw</td>\n",
       "      <td>Awful place. It's dirty. Had two birthday part...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11683</th>\n",
       "      <td>3CQQ8Im_UX6QqDECuXYK8A</td>\n",
       "      <td>A truly vegetarian delight!  I took a Jewish f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11684</th>\n",
       "      <td>ery1nBM7zKweFLBe-bT5ag</td>\n",
       "      <td>I have a 2011 Toyota Sienna Limited. During th...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.8859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11685</th>\n",
       "      <td>N5_SaVzmwkZUslgWDGGsQQ</td>\n",
       "      <td>I'm a single father raising my 17 year old son...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.9541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11686 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    review_id  \\\n",
       "0      LLzom-2TITa4gasV7_fCCA   \n",
       "1      a5JHzBrWxRd_OmIvV7znDA   \n",
       "2      X-o--dwf0HuFMittYi4wCA   \n",
       "3      INGNbsyo-MouZZzcxnCSGQ   \n",
       "4      k7VatXVLism-cTDJE8TTUw   \n",
       "...                       ...   \n",
       "11681  IlU-MQzMKc7jAHWwK5VFGQ   \n",
       "11682  Qt3BsRvQuJccDQfFWM1XPw   \n",
       "11683  3CQQ8Im_UX6QqDECuXYK8A   \n",
       "11684  ery1nBM7zKweFLBe-bT5ag   \n",
       "11685  N5_SaVzmwkZUslgWDGGsQQ   \n",
       "\n",
       "                                                    text  sentiment  compound  \n",
       "0      Great experience purchasing a washer and dryer...          1    0.9762  \n",
       "1      Went here based on the high ratings and raves ...         -1   -0.4508  \n",
       "2      oh Millers, how i wanted to like you.  You are...         -1    0.9790  \n",
       "3      This place gets two stars from me only because...         -1   -0.5660  \n",
       "4      This place was awesome. Clean, beautiful and t...          1    0.9871  \n",
       "...                                                  ...        ...       ...  \n",
       "11681  To be fair, I tried them in their first week. ...          0    0.8462  \n",
       "11682  Awful place. It's dirty. Had two birthday part...         -1    0.5896  \n",
       "11683  A truly vegetarian delight!  I took a Jewish f...          1    0.9884  \n",
       "11684  I have a 2011 Toyota Sienna Limited. During th...         -1   -0.8859  \n",
       "11685  I'm a single father raising my 17 year old son...         -1   -0.9541  \n",
       "\n",
       "[11686 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe with VADER sentiment scores\n",
    "\n",
    "vader_df = pd.DataFrame(res)\n",
    "vader_df = vader_df['compound']\n",
    "vader_df = pd.concat([review_df, vader_df], axis=1)\n",
    "\n",
    "vader_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABMvElEQVR4nO3dd1QU1/8+8GfpHQsIFgRDUbGLygeMARWD0VgSuyiIPRZUorFFSSxBTaISJaIm9oYtahQrX7FXiL3EhhiliI1iFF3u7w9/TFwB3cXFxfF5nbPnOHfuzLx3HeXhzp1ZhRBCgIiIiEgm9HRdABEREZE2MdwQERGRrDDcEBERkaww3BAREZGsMNwQERGRrDDcEBERkaww3BAREZGsMNwQERGRrDDcEBERkaww3BBRiZaYmAiFQoElS5boupR3qlevXrCwsNB1GTrxof6dk/Yw3JDWtW3bFmZmZsjMzCy0T0BAAIyMjHDv3j2p7eHDhzAxMYFCocDFixcL3K5Xr15QKBTSy8LCAh999BE6duyIDRs2IDc3N982vr6+Ktu8/KpWrZrUb8mSJSrrDAwMULFiRfTq1Qu3b9/W6DM4deoUevToAQcHBxgbG6NMmTLw8/PD4sWLoVQqNdoXqS8xMRHBwcFwdnaGiYkJ7O3t8cknnyAsLEzXpZVYOTk5iIiIQL169WBlZYVSpUqhRo0a6N+/Py5dulSsx161ahVmz55drMcoTjExMfjuu+90XQYVwEDXBZD8BAQE4M8//8Qff/yBwMDAfOsfP36MzZs3o2XLlihbtqzUvm7dOigUCtjb22PlypWYMmVKgfs3NjbGb7/9BgD4999/cfPmTfz555/o2LEjfH19sXnzZlhZWalsU6lSJYSHh+fbl7W1db62SZMmoUqVKnjy5AmOHj2KJUuW4ODBgzh37hxMTEze+P5/++03DBw4EHZ2dujZsydcXV2RmZmJ2NhY9OnTB8nJyRg3btwb90OauXr1Kho2bAhTU1P07t0bTk5OSE5ORkJCAqZPn47vv/9e1yWWSB06dMD27dvRrVs39OvXD8+ePcOlS5ewdetWeHt7q/wCoG2rVq3CuXPnMHz4cJV2R0dH/PvvvzA0NCy2Y2tDTEwMIiMjGXBKIkGkZY8fPxaWlpbC39+/wPWrVq0SAMSaNWtU2j/55BPx5ZdfihEjRogqVaoUuG1QUJAwNzcvcF14eLgAIDp37qzS7uPjI2rUqPHGuhcvXiwAiBMnTqi0jx49WgAQ0dHRb9zHkSNHhL6+vvj4449FRkZGvvUnTpwQixcvfuN+6D83btwQAN74uQ0aNEgYGBiIxMTEfOtSU1OLqbqCZWVlvfU+Xneua8vx48cFADF16tR8654/fy7S09OL9fitW7cWjo6OxXqM4jR48GDBH6MlEy9LkdaZmpriyy+/RGxsLNLS0vKtX7VqFSwtLdG2bVupLSkpCQcOHEDXrl3RtWtX3LhxA4cPH9bouGPGjMGnn36KdevW4e+//37r95GnSZMmAIBr1669se/3338PhUKBlStXwtLSMt/6Bg0aoFevXtJydnY2vv76a+nyVdWqVfHTTz9BCKGynUKhwJAhQ7Bu3Tq4u7vD1NQUXl5eOHv2LABg/vz5cHFxgYmJCXx9fZGYmKiyva+vL2rWrIn4+Hh4e3vD1NQUVapUQVRUVL4a09LS0KdPH9jZ2cHExAR16tTB0qVLVfrExcVBoVAgLi5Opb2guRJ5c0du376N9u3bw8LCAra2thg5cmS+S3QPHz5Er169YG1tjVKlSiEoKAgPHz4s5NNWde3aNVSqVAmOjo751pUrVy5f2/bt2+Hj4wNLS0tYWVmhYcOGWLVqlUqfdevWwcPDA6amprCxsUGPHj3yXaLMe3/Xrl1Dq1atYGlpiYCAAABAbm4uZs+ejRo1asDExAR2dnYYMGAAHjx4oNZ7AoDr16/D398f5ubmqFChAiZNmiSdH0IIODk5oV27dvm2e/LkCaytrTFgwIBC9513Tjdu3DjfOn19fZWRVQC4ffs2evfuDTs7OxgbG6NGjRpYtGiRSp+8c2Pt2rWYOnUqKlWqBBMTEzRv3hxXr16V+vn6+mLbtm24efOmdCnYyckJwOvPo6SkJHz++eewsLBAxYoVERkZCQA4e/YsmjVrBnNzczg6Oub7uwRenF/Dhw+X/r25uLhg+vTpKpez8479008/YcGCBXB2doaxsTEaNmyIEydOqNSTd+yXL2dTCaHjcEUytWvXLgFAzJkzR6X93r17wtDQUAQGBqq0T5s2TVhYWIjHjx8LIYRwdnYWgwYNyrffN/02u3z5cgFAzJ07V2rz8fER1apVE3fv3s33evk37MJGbubOnSsAiHnz5r32PWdnZwtDQ0PRrFmz1/bLk5ubK5o1ayYUCoXo27evmDt3rmjTpo0AIIYPH67SF4CoXbu2cHBwENOmTRPTpk0T1tbWonLlymLu3LnC3d1d/Pzzz+Lbb78VRkZGomnTpirb+/j4iAoVKohy5cqJIUOGiF9++UV8/PHHAoD4/fffpX6PHz8W1atXF4aGhmLEiBHil19+EU2aNBEAxOzZs6V+e/fuFQDE3r17VY5T0ChLUFCQMDExETVq1BC9e/cW8+bNEx06dBAAxK+//qryeXzyySdCT09PDBo0SMyZM0c0a9ZM1K5dW62Rm/79+wt9fX0RGxv7xs9+8eLFQqFQiJo1a4qpU6eKyMhI0bdvX9GzZ0+VPgBEw4YNxaxZs8SYMWOEqampcHJyEg8ePFB5f8bGxsLZ2VkEBQWJqKgosWzZMiGEEH379hUGBgaiX79+IioqSowePVqYm5uLhg0bipycnNfWmPe5ubq6ip49e4q5c+eKzz//XAAQEyZMkPqNHz9eGBoainv37qlsv3btWgFA7N+/v9BjHD58WAAQ/fr1E8+ePXttPSkpKaJSpUrCwcFBTJo0ScybN0+0bdtWABCzZs2S+uWdG/Xq1RMeHh5i1qxZ4rvvvhNmZmaiUaNGUr9du3aJunXrChsbG7F8+XKxfPly8ccffwghXn8eubu7i4EDB4rIyEjh7e0t9atQoYIYNWqUmDNnjqhRo4bQ19cX169fl7bPzs4WtWvXFmXLlhXjxo0TUVFRIjAwUCgUCjFs2DCpX96x69WrJ1xcXMT06dPFjBkzhI2NjahUqZL093b48GHRokULAUCqf/ny5a/9DOndYbihYvH8+XNRvnx54eXlpdIeFRUlAIidO3eqtNeqVUsEBARIy+PGjRM2Njb5/sN9U7j566+/BAAxYsQIqc3Hx0cAKPA1YMAAqV/eD7M9e/aIu3fvilu3bon169cLW1tbYWxsLG7duvXa93z69GkBQOU/ytfZtGmTACCmTJmi0t6xY0ehUCjE1atXpTYAwtjYWNy4cUNqmz9/vgAg7O3tVS6BjR07VgBQ6Zv3Gfz8889S29OnT0XdunVFuXLlpP+wZ8+eLQCIFStWSP1ycnKEl5eXsLCwkI6jabgBICZNmqTSN++H36ufx4wZM6S258+fS+HqTeHm3LlzwtTUVAAQdevWFcOGDRObNm0S2dnZKv0ePnwoLC0thaenp/j3339V1uXm5krvuVy5cqJmzZoqfbZu3SoAiIkTJ+Z7f2PGjFHZ14EDBwQAsXLlSpX2HTt2FNj+qrz9Dh06VKW+1q1bCyMjI3H37l0hhBCXL18uMHy3bdtWODk5Se+pILm5udK5YWdnJ7p16yYiIyPFzZs38/Xt06ePKF++fL5LVV27dhXW1tbSLyZ550b16tXF06dPpX4RERECgDh79qzUVthlqdedRz/88IPU9uDBA2FqaioUCoXKZe5Lly4JACIsLExqmzx5sjA3Nxd///23yrHGjBkj9PX1RVJSksqxy5YtK+7fvy/127x5swAg/vzzT6mNl6VKLl6WomKhr6+Prl274siRIyqXSFatWgU7Ozs0b95cajtz5gzOnj2Lbt26SW3dunVDeno6du7cqdFx826dffVOLScnJ+zevTvf69WJjADg5+cHW1tbODg4oGPHjjA3N8eWLVtQqVKl1x47IyMDAAq8HFWQmJgY6OvrIyQkRKX966+/hhAC27dvV2lv3ry5NGwPAJ6engBeTAh9+Zh57devX1fZ3sDAQOUShZGREQYMGIC0tDTEx8dLNdnb26v8XRgaGiIkJARZWVnYt2+fWu+tIAMHDlRZbtKkiUqNMTExMDAwwFdffSW16evrY+jQoWrtv0aNGtJdaomJiYiIiED79u1hZ2eHhQsXSv12796NzMxMjBkzJt8E8bzLCidPnkRaWhoGDRqk0qd169aoVq0atm3blu/4L9cNvLikZW1tjRYtWiA9PV16eXh4wMLCAnv37lXrfQ0ZMkSlviFDhiAnJwd79uwBALi5ucHT0xMrV66U+t2/fx/bt29HQEDAay+VKBQK7Ny5E1OmTEHp0qWxevVqDB48GI6OjujSpYt0SVAIgQ0bNqBNmzYQQqi8H39/fzx69AgJCQkq+w4ODoaRkZG0nHd599XzUlN9+/aV/lyqVClUrVoV5ubm6Ny5s9RetWpVlCpVSuVY69atQ5MmTVC6dGmV+v38/KBUKrF//36V43Tp0gWlS5fWev30bvBuKSo2AQEBmDVrFlatWoVx48bhn3/+wYEDBxASEgJ9fX2p34oVK2Bubo6PPvpIuiZvYmICJycnrFy5Eq1bt1b7mFlZWQDyBwxzc3P4+fmptY/IyEi4ubnh0aNHWLRoEfbv3w9jY+M3bpd3h9brboF/2c2bN1GhQoV8tVavXl1a/7LKlSurLOfd6eXg4FBg+6vzOipUqABzc3OVNjc3NwAv5hn873//w82bN+Hq6go9PdXfewqrSV0mJiawtbVVaStdurRKjTdv3kT58uXzPdulatWqah/Hzc0Ny5cvh1KpxIULF7B161bMmDED/fv3R5UqVeDn5yfNM6lZs2ah+8l7nwUdu1q1ajh48KBKm4GBQb7we+XKFTx69KjA+T4ACpyP9io9PT189NFH+d4jAJVfGgIDAzFkyBDcvHkTjo6OWLduHZ49e4aePXu+8RjGxsYYP348xo8fj+TkZOzbtw8RERFYu3YtDA0NsWLFCty9excPHz7EggULsGDBArXez6vna15Q0GS+0asKOo+sra1RqVKlfCHO2tpa5VhXrlzBmTNn8m3/Luund4fhhoqNh4cHqlWrhtWrV2PcuHFYvXo1hBDSZEvgxW+Eq1evRnZ2Ntzd3fPtIy0tDVlZWWo/zOzcuXMAABcXlyLX3ahRIzRo0AAA0L59e3z88cfo3r07Ll++/No6XFxcYGBgIE3y1baXA6E67eKVScnaVNhoQGHP8CmsxuKir6+PWrVqoVatWvDy8kLTpk2xcuVKtQOupoyNjfMFwtzcXJQrV05lROVlhf2QLYquXbtixIgRWLlyJcaNG4cVK1agQYMGGgVDAChfvjy6du2KDh06oEaNGli7di2WLFkiTbjt0aMHgoKCCty2du3aKsvFcV6+zb+B3NxctGjRAt98802BffNCoyb7pJKL4YaKVUBAACZMmIAzZ85g1apVcHV1RcOGDaX1+/btwz///INJkyZJowN5Hjx4gP79+2PTpk3o0aOHWsdbvnw5FAoFWrRooZX69fX1ER4ejqZNm2Lu3LkYM2ZMoX3NzMzQrFkz/N///R9u3bqVb0TlVY6OjtizZw8yMzNVRm/yHpxW0F0/b+POnTvIzs5WGb3Ju6ss73KXo6Mjzpw5g9zcXJUf1q/WlPdb7Kt3MhV1ZCdv37GxsfnC7OXLl4u8TwBSUE1OTgYAODs7A3gRhAsLwXnv8/Lly2jWrJnKusuXL6v1d+Ps7Iw9e/agcePGMDU1LVLtubm5uH79usoP3lf/zgCgTJkyaN26NVauXImAgAAcOnTorR6OZ2hoiNq1a+PKlStIT0+Hra0tLC0toVQqtRoQ3+XdRc7OzsjKynpv6yfNcM4NFau8UZqJEyfi1KlTKqM2wH+XpEaNGoWOHTuqvPr16wdXV9dCf/N91bRp07Br1y506dIFrq6uWnsPvr6+aNSoEWbPno0nT568tm9YWBiEEOjZs6d0iexl8fHx0m3VrVq1glKpxNy5c1X6zJo1CwqFAp999pnW3gMAPH/+HPPnz5eWc3JyMH/+fNja2sLDw0OqKSUlBdHR0SrbzZkzBxYWFvDx8QHw4oe/vr5+vnkKv/76a5Hra9WqFZ4/f4558+ZJbUqlEnPmzFFr+wMHDuDZs2f52mNiYgD8d4np008/haWlJcLDw/P9feb9Vt6gQQOUK1cOUVFRePr0qbR++/btuHjxolqXSjt37gylUonJkyfnW/f8+XO1b3F/+fwQQmDu3LkwNDRUmbcGAD179sSFCxcwatQoac7bm1y5cgVJSUn52h8+fIgjR46gdOnSsLW1hb6+Pjp06IANGzZIo6Mvu3v3rlrv5VXm5uZ49OhRkbbVVOfOnXHkyJEC5/E9fPgQz58/13ifeb8oqPt3Se8OR26oWFWpUgXe3t7YvHkzAKiEm6dPn2LDhg1o0aJFoU/+bdu2LSIiIpCWlibNXXj+/DlWrFgB4MWzPG7evIktW7bgzJkzaNq0aYFzAh49eiRt8yp1RoVGjRqFTp06YcmSJfkmxr7M29sbkZGRGDRoEKpVq6byhOK4uDhs2bJFevJymzZt0LRpU4wfPx6JiYmoU6cOdu3ahc2bN2P48OHSCIO2VKhQAdOnT0diYiLc3NwQHR2NU6dOYcGCBdKTYPv374/58+ejV69eiI+Ph5OTE9avXy+NBOSNMFlbW6NTp06YM2cOFAoFnJ2dsXXrVrXmkRSmTZs2aNy4McaMGYPExES4u7tj48aNav/wmz59OuLj4/Hll19Kl0gSEhKwbNkylClTRpo8bmVlhVmzZqFv375o2LAhunfvjtKlS+P06dN4/Pgxli5dCkNDQ0yfPh3BwcHw8fFBt27dkJqaioiICDg5OWHEiBFvrMfHxwcDBgxAeHg4Tp06hU8//RSGhoa4cuUK1q1bh4iICHTs2PG1+zAxMcGOHTsQFBQET09PbN++Hdu2bcO4cePyXdZq3bo1ypYti3Xr1uGzzz4rdK7Py06fPo3u3bvjs88+Q5MmTVCmTBncvn0bS5cuxZ07dzB79mzp8sy0adOwd+9eeHp6ol+/fnB3d8f9+/eRkJCAPXv24P79+2883qs8PDwQHR2N0NBQNGzYEBYWFmjTpo3G+1HHqFGjsGXLFnz++efo1asXPDw8kJ2djbNnz2L9+vVITEyEjY2NxvUDQEhICPz9/dUOlfQO6OYmLfqQREZGCgAqz7gQQogNGzbke87Kq+Li4gQAERERIYT473bQvJeZmZlwcnISHTp0EOvXrxdKpTLfPl53K/jL/wQKe86NEEIolUrh7OwsnJ2dxfPnz9/4nuPj40X37t1FhQoVhKGhoShdurRo3ry5WLp0qUqNmZmZYsSIEVI/V1dX8eOPP+a7fReAGDx4sEpb3i2rP/74o0p73q2469atU/kMatSoIU6ePCm8vLyEiYmJcHR0VHkeUJ7U1FQRHBwsbGxshJGRkahVq1aBt2HfvXtXdOjQQZiZmYnSpUuLAQMGiHPnzhV4C29Bt++HhYXlu4323r17omfPnsLKykpYW1uLnj17Srf3v+lW8EOHDonBgweLmjVrCmtra2FoaCgqV64sevXqJa5du5av/5YtW4S3t7cwNTUVVlZWolGjRmL16tUqfaKjo0W9evWEsbGxKFOmjAgICBD//POPSp83PZ5gwYIFwsPDQ5iamgpLS0tRq1Yt8c0334g7d+689v3k7ffatWvi008/FWZmZsLOzk6EhYUVeJ4L8eIpzQDEqlWrXrvvPKmpqWLatGnCx8dHlC9fXhgYGIjSpUuLZs2aifXr1xfYf/DgwcLBwUEYGhoKe3t70bx5c7FgwQKpT0HnnxAF396dlZUlunfvLkqVKiUASLeFF3YreEGfc2FPIHd0dBStW7dWacvMzBRjx44VLi4uwsjISNjY2Ahvb2/x008/SY9DKOzflRAi3+3lz58/F0OHDhW2trZCoVDwtvASRCEEZ0cRyZ2vry/S09MLvKRA8jFixAj8/vvvSElJgZmZma7LIdIZzrkhIpKBJ0+eYMWKFejQoQODDX3wOOeGiOg9lpaWhj179mD9+vW4d+8ehg0bpuuSiHSO4YaI6D124cIFBAQEoFy5cvjll19Qt25dXZdEpHOcc0NERESywjk3REREJCsMN0RERCQrH9ycm9zcXNy5cweWlpZ8dDYREdF7QgiBzMxMVKhQId93ub3qgws3d+7ceeN3/hAREVHJdOvWLVSqVOm1fT64cJP3+Phbt27ByspKx9UQERGROjIyMuDg4KDyRcOF+eDCTd6lKCsrK4YbIiKi94w6U0o4oZiIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGTlg/viTCIiondFCIHs7Gxp2dzcXK0vfqS3w3BDRERUTLKzs9GuXTtpefPmzbCwsNBhRR8GXpYiIiIiWWG4ISIiIlnhZSkiIioWHqOW6boEnVM8z4H1S8u+E9ZAGBjprB5di/8x8J0chyM3REREJCsMN0RERCQrDDdEREQkKww3REREJCucUExERFRMhL4hHtXuprJMxY/hhoiIqLgoFB/03VG6wstSREREJCsMN0RERCQrDDdEREQkKww3REREJCsMN0RERCQrDDdEREQkKzoPN5GRkXBycoKJiQk8PT1x/Pjx1/afPXs2qlatClNTUzg4OGDEiBF48uTJO6qWiIiISjqdhpvo6GiEhoYiLCwMCQkJqFOnDvz9/ZGWllZg/1WrVmHMmDEICwvDxYsX8fvvvyM6Ohrjxo17x5UTERFRSaXTcDNz5kz069cPwcHBcHd3R1RUFMzMzLBo0aIC+x8+fBiNGzdG9+7d4eTkhE8//RTdunV742gPERERfTh0Fm5ycnIQHx8PPz+//4rR04Ofnx+OHDlS4Dbe3t6Ij4+Xwsz169cRExODVq1avZOaiYiIqOTT2dcvpKenQ6lUws7OTqXdzs4Oly5dKnCb7t27Iz09HR9//DGEEHj+/DkGDhz42stST58+xdOnT6XljIwM7bwBIiIiKpF0PqFYE3Fxcfjhhx/w66+/IiEhARs3bsS2bdswefLkQrcJDw+HtbW19HJwcHiHFRMREdG7prORGxsbG+jr6yM1NVWlPTU1Ffb29gVuM2HCBPTs2RN9+/YFANSqVQvZ2dno378/xo8fDz29/Flt7NixCA0NlZYzMjIYcIiIiGRMZyM3RkZG8PDwQGxsrNSWm5uL2NhYeHl5FbjN48eP8wUYfX19AIAQosBtjI2NYWVlpfIiIiIi+dLZyA0AhIaGIigoCA0aNECjRo0we/ZsZGdnIzg4GAAQGBiIihUrIjw8HADQpk0bzJw5E/Xq1YOnpyeuXr2KCRMmoE2bNlLIISIiog+bTsNNly5dcPfuXUycOBEpKSmoW7cuduzYIU0yTkpKUhmp+fbbb6FQKPDtt9/i9u3bsLW1RZs2bTB16lRdvQUiIiIqYRSisOs5MpWRkQFra2s8evSIl6iIiIqRx6hlui6BSpj4HwOLvK0mP7/fq7uliIiIiN6E4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGTFQNcFEBFpgxAC2dnZ0rK5uTkUCoUOKyIiXWG4ISJZyM7ORrt27aTlzZs3w8LCQocVEZGu8LIUERERyQrDDREREckKww0RERHJis7DTWRkJJycnGBiYgJPT08cP378tf0fPnyIwYMHo3z58jA2NoabmxtiYmLeUbVERERU0hUp3Fy7dg3ffvstunXrhrS0NADA9u3bcf78eY32Ex0djdDQUISFhSEhIQF16tSBv7+/tM9X5eTkoEWLFkhMTMT69etx+fJlLFy4EBUrVizK2yAiIiIZ0jjc7Nu3D7Vq1cKxY8ewceNGZGVlAQBOnz6NsLAwjfY1c+ZM9OvXD8HBwXB3d0dUVBTMzMywaNGiAvsvWrQI9+/fx6ZNm9C4cWM4OTnBx8cHderU0fRtEBERkUxpHG7GjBmDKVOmYPfu3TAyMpLamzVrhqNHj6q9n5ycHMTHx8PPz++/YvT04OfnhyNHjhS4zZYtW+Dl5YXBgwfDzs4ONWvWxA8//AClUqnp2yAiIiKZ0vg5N2fPnsWqVavytZcrVw7p6elq7yc9PR1KpRJ2dnYq7XZ2drh06VKB21y/fh3/93//h4CAAMTExODq1asYNGgQnj17Vuio0dOnT/H06VNpOSMjQ+0aiYiI6P2j8chNqVKlkJycnK/9r7/+Kva5L7m5uShXrhwWLFgADw8PdOnSBePHj0dUVFSh24SHh8Pa2lp6OTg4FGuNREREpFsah5uuXbti9OjRSElJgUKhQG5uLg4dOoSRI0ciMDBQ7f3Y2NhAX18fqampKu2pqamwt7cvcJvy5cvDzc0N+vr6Ulv16tWRkpKCnJycArcZO3YsHj16JL1u3bqldo1ERET0/tH4stQPP/yAwYMHw8HBAUqlEu7u7lAqlejevTu+/fZbtfdjZGQEDw8PxMbGon379gBejMzExsZiyJAhBW7TuHFjrFq1Crm5udDTe5HL/v77b5QvX15l/s/LjI2NYWxsrNmbJHoPeYxapusSdErxPAfWLy37TlgDYVDw/wsfgvgf1f9lk0huNBq5EUIgJSUFv/zyC65fv46tW7dixYoVuHTpEpYvX64yoqKO0NBQLFy4EEuXLsXFixfx1VdfITs7G8HBwQCAwMBAjB07Vur/1Vdf4f79+xg2bBj+/vtvbNu2TQpbRERERICGIzdCCLi4uOD8+fNwdXV96/krXbp0wd27dzFx4kSkpKSgbt262LFjhzTJOCkpSRqhAQAHBwfs3LkTI0aMQO3atVGxYkUMGzYMo0ePfqs6iIiISD40Cjd6enpwdXXFvXv34OrqqpUChgwZUuhlqLi4uHxtXl5eGt1yTkRERB8WjScUT5s2DaNGjcK5c+eKox4iIiKit6LxhOLAwEA8fvwYderUgZGREUxNTVXW379/X2vFEREREWlK43Aze/bsYiiDiIiISDs0DjdBQUHFUQcRERGRVmgcbgBAqVRi06ZNuHjxIgCgRo0aaNu2rca3ghMRERFpm8bh5urVq2jVqhVu376NqlWrAnjxFQcODg7Ytm0bnJ2dtV4kERERkbo0vlsqJCQEzs7OuHXrFhISEpCQkICkpCRUqVIFISEhxVEjERERkdo0HrnZt28fjh49ijJlykhtZcuWxbRp09C4cWOtFkdEpC6hb4hHtbupLBPRh0njcGNsbIzMzMx87VlZWYV+vxMRUbFTKD7o75Iiov9ofFnq888/R//+/XHs2DEIISCEwNGjRzFw4EC0bdu2OGokIiIiUpvG4eaXX36Bs7MzvLy8YGJiAhMTEzRu3BguLi6IiIgojhqJiIiI1KbxZalSpUph8+bNuHr1qnQrePXq1eHi4qL14oiIiIg0VaTn3ACAi4sLAw0RERGVOBpflurQoQOmT5+er33GjBno1KmTVooiIiIiKiqNw83+/fvRqlWrfO2fffYZ9u/fr5WiiIiIiIpK43BT2C3fhoaGyMjI0EpRREREREWlcbipVasWoqOj87WvWbMG7u7uWimKiIiIqKg0nlA8YcIEfPnll7h27RqaNWsGAIiNjcXq1auxbt06rRdIREREpAmNw02bNm2wadMm/PDDD1i/fj1MTU1Ru3Zt7NmzBz4+PsVRIxEREZHainQreOvWrdG6dWtt10JERET01or8nBsAePLkCaKjo5GdnY0WLVrA1dVVW3URERERFYna4SY0NBTPnj3DnDlzAAA5OTn43//+hwsXLsDMzAzffPMNdu/eDS8vr2IrloiIiOhN1L5bateuXWjRooW0vHLlSiQlJeHKlSt48OABOnXqhClTphRLkURERETqUjvcJCUlqdzqvWvXLnTs2BGOjo5QKBQYNmwY/vrrr2IpkoiIiEhdaocbPT09CCGk5aNHj+J///uftFyqVCk8ePBAu9URERERaUjtcFO9enX8+eefAIDz588jKSkJTZs2ldbfvHkTdnZ22q+QiIiISANqTyj+5ptv0LVrV2zbtg3nz59Hq1atUKVKFWl9TEwMGjVqVCxFEhEREalL7ZGbL774AjExMahduzZGjBiR7ysYzMzMMGjQIK0XSERERKQJjZ5z07x5czRv3rzAdWFhYVopiIiIiOhtaPzFmUREREQlGcMNERERyQrDDREREckKww0RERHJCsMNERERyYpad0vVq1cPCoVCrR0mJCS8VUFEREREb0OtcNO+fXvpz0+ePMGvv/4Kd3d36RvAjx49ivPnz/M5N0RERKRzaoWbl59h07dvX4SEhGDy5Mn5+ty6dUu71RERERFpSOM5N+vWrUNgYGC+9h49emDDhg1aKYqIiIioqDQON6ampjh06FC+9kOHDsHExEQrRREREREVlUZfvwAAw4cPx1dffYWEhATpizKPHTuGRYsWYcKECVovkIiIiEgTGoebMWPG4KOPPkJERARWrFgBAKhevToWL16Mzp07a71AIiIiIk1oHG4AoHPnzgwyREREVCIVKdwAQE5ODtLS0pCbm6vSXrly5bcuioiIiKioNA43V65cQe/evXH48GGVdiEEFAoFlEql1oojIiIi0pTG4aZXr14wMDDA1q1bUb58ebWfXExERET0Lmgcbk6dOoX4+HhUq1atOOohIiIieisaP+fG3d0d6enpxVELERER0VvTONxMnz4d33zzDeLi4nDv3j1kZGSovIiIiIh0SePLUn5+fgCA5s2bq7RzQjERERGVBBqHm7179xZHHURERERaoXG48fHxKY46iIiIiLRC43Czf//+167/5JNPilwMERER0dvSONz4+vrma3v5WTecc0NERES6pPHdUg8ePFB5paWlYceOHWjYsCF27dpVHDUSERERqU3jkRtra+t8bS1atICRkRFCQ0MRHx+vlcKIiIiIikLjkZvC2NnZ4fLly9raHREREVGRaDxyc+bMGZVlIQSSk5Mxbdo01K1bV1t1ERERERWJxiM3devWRb169VC3bl3pz61atUJOTg5+++23IhURGRkJJycnmJiYwNPTE8ePH1druzVr1kChUKB9+/ZFOi4RERHJj8YjNzdu3FBZ1tPTg62tLUxMTIpUQHR0NEJDQxEVFQVPT0/Mnj0b/v7+uHz5MsqVK1fodomJiRg5ciSaNGlSpOMSERGRPGk8cuPo6KjycnBwKHKwAYCZM2eiX79+CA4Ohru7O6KiomBmZoZFixYVuo1SqURAQAC+//57fPTRR0U+NhEREclPkSYU79u3D23atIGLiwtcXFzQtm1bHDhwQOP95OTkID4+Xvq+KuDFSJCfnx+OHDlS6HaTJk1CuXLl0KdPnzce4+nTp/xyTyIiog+IxuFmxYoV8PPzg5mZGUJCQhASEgJTU1M0b94cq1at0mhf6enpUCqVsLOzU2m3s7NDSkpKgdscPHgQv//+OxYuXKjWMcLDw2FtbS29HBwcNKqRiIiI3i8az7mZOnUqZsyYgREjRkhtISEhmDlzJiZPnozu3btrtcCXZWZmomfPnli4cCFsbGzU2mbs2LEIDQ2VljMyMhhwiIiIZEzjcHP9+nW0adMmX3vbtm0xbtw4jfZlY2MDfX19pKamqrSnpqbC3t4+X/9r164hMTFR5fi5ubkAAAMDA1y+fBnOzs4q2xgbG8PY2FijuoiIiOj9pfFlKQcHB8TGxuZr37Nnj8YjIkZGRvDw8FDZX25uLmJjY+Hl5ZWvf7Vq1XD27FmcOnVKerVt2xZNmzbFqVOnOCJDREREmo/cfP311wgJCcGpU6fg7e0NADh06BCWLFmCiIgIjQsIDQ1FUFAQGjRogEaNGmH27NnIzs5GcHAwACAwMBAVK1ZEeHg4TExMULNmTZXtS5UqBQD52omIiOjDpHG4+eqrr2Bvb4+ff/4Za9euBQBUr14d0dHRaNeuncYFdOnSBXfv3sXEiRORkpKCunXrYseOHdIk46SkJOjpae1bIoiIiEjmFEIIoesi3qWMjAxYW1vj0aNHsLKy0nU5RFrjMWqZrkugEiT+x0Bdl8BzkvJ5m/NSk5/fGo/c5Dl58iQuXrwIAHB3d4eHh0dRd0VERESkNRqHm3/++QfdunXDoUOHpPkuDx8+hLe3N9asWYNKlSppu0YiIiIitWk8maVv37549uwZLl68iPv37+P+/fu4ePEicnNz0bdv3+KokYiIiEhtGo/c7Nu3D4cPH0bVqlWltqpVq2LOnDn8EksiIiLSuSI95+bZs2f52pVKJSpUqKCVooiIiIiKSuNw8+OPP2Lo0KE4efKk1Hby5EkMGzYMP/30k1aLIyIiItKUxpelevXqhcePH8PT0xMGBi82f/78OQwMDNC7d2/07t1b6nv//n3tVUpERESkBo3DzezZs4uhDCIiIiLt0DjcBAUFFUcdRERERFpR5If4paWlIS0tTfpW7jy1a9d+66KIiIiIikrjcBMfH4+goCBcvHgRr35zg0KhgFKp1FpxRERERJrSONz07t0bbm5u+P3332FnZweFQlEcdREREREVicbh5vr169iwYQNcXFyKox4iIiKit6Lxc26aN2+O06dPF0ctRERERG9N45Gb3377DUFBQTh37hxq1qwJQ0NDlfVt27bVWnFEREREmtI43Bw5cgSHDh3C9u3b863jhGIiIiLSNY0vSw0dOhQ9evRAcnIycnNzVV4MNkRERKRrGoebe/fuYcSIEbCzsyuOeoiIiIjeisbh5ssvv8TevXuLoxYiIiKit6bxnBs3NzeMHTsWBw8eRK1atfJNKA4JCdFacURERESaKtLdUhYWFti3bx/27dunsk6hUDDcEBERkU5pHG5u3LhRHHUQERERaYXGc25eJoTI9/1SRERERLpUpHCzbNky1KpVC6ampjA1NUXt2rWxfPlybddGREREpDGNL0vNnDkTEyZMwJAhQ9C4cWMAwMGDBzFw4ECkp6djxIgRWi+SiIiISF0ah5s5c+Zg3rx5CAwMlNratm2LGjVq4LvvvmO4ISIiIp3S+LJUcnIyvL2987V7e3sjOTlZK0URERERFZXG4cbFxQVr167N1x4dHQ1XV1etFEVERERUVBpflvr+++/RpUsX7N+/X5pzc+jQIcTGxhYYeoiIiIjeJY1Hbjp06IBjx47BxsYGmzZtwqZNm2BjY4Pjx4/jiy++KI4aiYiIiNSm8cgNAHh4eGDFihXaroWIiIjorak9cnPnzh2MHDkSGRkZ+dY9evQIo0aNQmpqqlaLIyIiItKU2uFm5syZyMjIgJWVVb511tbWyMzMxMyZM7VaHBEREZGm1A43O3bsUHm2zasCAwOxdetWrRRFREREVFRqh5sbN26gcuXKha6vVKkSEhMTtVETERERUZGpHW5MTU1fG14SExNhamqqjZqIiIiIikztcOPp6fnaL8dctmwZGjVqpJWiiIiIiIpK7VvBR44ciRYtWsDa2hqjRo2CnZ0dACA1NRUzZszAkiVLsGvXrmIrlIiIiEgdaoebpk2bIjIyEsOGDcOsWbNgZWUFhUKBR48ewdDQEHPmzEGzZs2Ks1YiIiKiN9LoIX4DBgzA559/jrVr1+Lq1asQQsDNzQ0dO3ZEpUqViqtGIiIiIrVp/ITiihUrYsSIEcVRCxEREdFb0/i7pYiIiIhKMoYbIiIikhWGGyIiIpIVhhsiIiKSFa2Fm+TkZAwZMkRbuyMiIiIqEo3uljp//jz27t0LIyMjdO7cGaVKlUJ6ejqmTp2KqKgofPTRR8VVJxEREZFa1B652bJlC+rVq4eQkBAMHDgQDRo0wN69e1G9enVcvHgRf/zxB86fP1+ctRIRERG9kdrhZsqUKRg8eDAyMjIwc+ZMXL9+HSEhIYiJicGOHTvQsmXL4qyTiIiISC1qh5vLly9j8ODBsLCwwNChQ6Gnp4dZs2ahYcOGxVkfERERkUbUDjeZmZmwsrICAOjr68PU1JRzbIiIiKjE0WhC8c6dO2FtbQ0AyM3NRWxsLM6dO6fSp23bttqrjoiIiEhDGoWboKAgleUBAwaoLCsUCiiVyrevioiIiKiI1A43ubm5xVkHERERkVbwCcVEREQkK2qHm0GDBiErK0taXr16NbKzs6Xlhw8folWrVtqtjoiIiEhDaoeb+fPn4/Hjx9LygAEDkJqaKi0/ffoUO3fu1G51RERERBpSO9wIIV67/DYiIyPh5OQEExMTeHp64vjx44X2XbhwIZo0aYLSpUujdOnS8PPze21/IiIi+rDofM5NdHQ0QkNDERYWhoSEBNSpUwf+/v5IS0srsH9cXBy6deuGvXv34siRI3BwcMCnn36K27dvv+PKiYiIqCTSebiZOXMm+vXrh+DgYLi7uyMqKgpmZmZYtGhRgf1XrlyJQYMGoW7duqhWrRp+++036Zk7RERERBo952bixIkwMzMDAOTk5GDq1KnSQ/1eno+jrpycHMTHx2Ps2LFSm56eHvz8/HDkyBG19vH48WM8e/YMZcqUKXD906dP8fTpU2k5IyND4zqJiIjo/aF2uPnkk09w+fJladnb2xvXr1/P10cT6enpUCqVsLOzU2m3s7PDpUuX1NrH6NGjUaFCBfj5+RW4Pjw8HN9//71GdREREdH7S+1wExcXV4xlFM20adOwZs0axMXFwcTEpMA+Y8eORWhoqLSckZEBBweHd1UiERERvWMaXZbSNhsbG+jr66vcUg4AqampsLe3f+22P/30E6ZNm4Y9e/agdu3ahfYzNjaGsbGxVuolIiKikk+jCcXZ2dmYOHEiatasCQsLC1haWqJ27dqYNGlSkebcGBkZwcPDQ2UycN7kYC8vr0K3mzFjBiZPnowdO3agQYMGGh+XiIiI5EvtkZucnBz4+Pjg3Llz+Oyzz9CmTRsIIXDx4kVMnToV27dvx/79+2FoaKhRAaGhoQgKCkKDBg3QqFEjzJ49G9nZ2QgODgYABAYGomLFiggPDwcATJ8+HRMnTsSqVavg5OSElJQUAICFhQUsLCw0OjYRERHJj9rhZt68efjnn39w+vRpVK1aVWXdpUuX4Ovri6ioKAwdOlSjArp06YK7d+9i4sSJSElJQd26dbFjxw5pknFSUhL09P4bYJo3bx5ycnLQsWNHlf2EhYXhu+++0+jYREREJD9qh5uNGzdiwoQJ+YINAFSrVg3jx4/H+vXrNQ43ADBkyBAMGTKkwHWvTmROTEzUeP9ERET04VB7zs2FCxfg6+tb6PqmTZviwoUL2qiJiIiIqMjUDjcPHz5E2bJlC11ftmxZPHr0SCtFERERERWV2uEmNzcX+vr6he9ITw9KpVIrRREREREVldpzboQQaN68OQwMCt7k+fPnWiuKiIiIqKjUDjdhYWFv7NOhQ4e3KoaIiIjobWk13BARERHpmkZPKC5MRkYG5s2bx6cFExERkc691XdL7d27F4sWLcLGjRthbW2NL774Qlt1ERERERWJxuHm9u3bWLJkCRYvXoyHDx/iwYMHWLVqFTp37gyFQlEcNRIRERGpTe3LUhs2bECrVq1QtWpVnDp1Cj///DPu3LkDPT091KpVi8GGiIiISgS1R266dOmC0aNHIzo6GpaWlsVZExEREVGRqT1y06dPH0RGRqJly5aIiorCgwcPirMuIiIioiJRO9zMnz8fycnJ6N+/P1avXo3y5cujXbt2EEIgNze3OGskIiIiUptGt4KbmpoiKCgI+/btw9mzZ1GjRg3Y2dmhcePG6N69OzZu3FhcdRIRERGpRe1wc+7cOZVlV1dX/PDDD7h16xZWrFiBx48fo1u3blovkIiIiEgTaoeb2rVrw9PTEwsXLkRmZuZ/O9DTQ5s2bbBp0ybcunWrWIokIiIiUpfa4Wbfvn2oUaMGvv76a5QvXx5BQUE4cOCASp9y5cppvUAiIiIiTagdbpo0aYJFixYhOTkZc+bMQWJiInx8fODm5obp06cjJSWlOOskIiIiUovG3y1lbm6O4OBg7Nu3D3///Tc6deqEyMhIVK5cGW3bti2OGomIiIjU9lZfnOni4oJx48bh22+/haWlJbZt26atuoiIiIiKpMhfnLl//34sWrQIGzZsgJ6eHjp37ow+ffposzYiIiIijWkUbu7cuYMlS5ZgyZIluHr1Kry9vfHLL7+gc+fOMDc3L64aiYiIiNSmdrj57LPPsGfPHtjY2CAwMBC9e/dG1apVi7M2IiIiIo2pHW4MDQ2xfv16fP7559DX1y/OmoiIiIiKTO1ws2XLluKsg4iIiEgr3upuKSIiIqKShuGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZKVEhJvIyEg4OTnBxMQEnp6eOH78+Gv7r1u3DtWqVYOJiQlq1aqFmJiYd1QpERERlXQ6DzfR0dEIDQ1FWFgYEhISUKdOHfj7+yMtLa3A/ocPH0a3bt3Qp08f/PXXX2jfvj3at2+Pc+fOvePKiYiIqCTSebiZOXMm+vXrh+DgYLi7uyMqKgpmZmZYtGhRgf0jIiLQsmVLjBo1CtWrV8fkyZNRv359zJ079x1XTkRERCWRTsNNTk4O4uPj4efnJ7Xp6enBz88PR44cKXCbI0eOqPQHAH9//0L7P336FBkZGSovIiIiki8DXR48PT0dSqUSdnZ2Ku12dna4dOlSgdukpKQU2D8lJaXA/uHh4fj++++1U/BLPEYt0/o+6f0V/2OgrksoETUQvYznJOmKzi9LFbexY8fi0aNH0uvWrVu6LomIiIiKkU5HbmxsbKCvr4/U1FSV9tTUVNjb2xe4jb29vUb9jY2NYWxsrJ2CiYiIqMTT6ciNkZERPDw8EBsbK7Xl5uYiNjYWXl5eBW7j5eWl0h8Adu/eXWh/IiIi+rDodOQGAEJDQxEUFIQGDRqgUaNGmD17NrKzsxEcHAwACAwMRMWKFREeHg4AGDZsGHx8fPDzzz+jdevWWLNmDU6ePIkFCxbo8m0QERFRCaHzcNOlSxfcvXsXEydOREpKCurWrYsdO3ZIk4aTkpKgp/ffAJO3tzdWrVqFb7/9FuPGjYOrqys2bdqEmjVr6uotEBERUQmiEEIIXRfxLmVkZMDa2hqPHj2ClZVVkffDu6XoZbwrhIioeGny81v2d0sRERHRh4XhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZIXhhoiIiGSF4YaIiIhkheGGiIiIZMVA1wXQe0oIKJTP/lvUNwQUCh0WRERE9ALDDRWJQvkM1mdWS8uPaneDMDDSYUVEREQv8LIUERERyQrDDREREckKww0RERHJCsMNERERyQrDDREREckKww0RERHJCsMNERERyQqfc1NE8T8G6roEncrKykK7dv895yZucldYWFjosCIiIqIXOHJDREREssJwQ0RERLLCcENERESywnBDREREssJwQ0RERLLCcENERESywnBDREREssJwQ0RERLLCh/hRkZibm2Pz5s0qy0RERCUBww0ViUKh4BOJiYioROJlKSIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikpUP7oszhRAAgIyMDB1XQkREROrK+7md93P8dT64cJOZmQkAcHBw0HElREREpKnMzExYW1u/to9CqBOBZCQ3Nxd37tyBpaUlFAqFrst5r2VkZMDBwQG3bt2ClZWVrssh4jlJJRLPS+0QQiAzMxMVKlSAnt7rZ9V8cCM3enp6qFSpkq7LkBUrKyv+g6UShecklUQ8L9/em0Zs8nBCMREREckKww0RERHJCsMNFZmxsTHCwsJgbGys61KIAPCcpJKJ5+W798FNKCYiIiJ548gNERERyQrDDREREckKww0RERHJCsMNvRPfffcd6tatq+syiIrMyckJs2fP1nUZ9B6Ji4uDQqHAw4cPX9uP55b2MdyQ1ikUCmzatEmlbeTIkYiNjdVNQfRB8vX1xfDhw3VdBn3AvL29kZycLD14bsmSJShVqlS+fidOnED//v3fcXXy9sE9oZh0w8LCAhYWFroug0iFEAJKpRIGBvyvkLTPyMgI9vb2b+xna2v7Dqr5sHDkRkZ8fX0REhKCb775BmXKlIG9vT2+++47af3Dhw/Rt29f2NrawsrKCs2aNcPp06dV9jFlyhSUK1cOlpaW6Nu3L8aMGaNyOenEiRNo0aIFbGxsYG1tDR8fHyQkJEjrnZycAABffPEFFAqFtPzyZaldu3bBxMQk31DtsGHD0KxZM2n54MGDaNKkCUxNTeHg4ICQkBBkZ2e/9edEuve252qvXr3Qvn17lX0OHz4cvr6+0vp9+/YhIiICCoUCCoUCiYmJ0mWC7du3w8PDA8bGxjh48CCuXbuGdu3awc7ODhYWFmjYsCH27NnzDj4J0jVfX18MGTIEQ4YMgbW1NWxsbDBhwgTpm6cfPHiAwMBAlC5dGmZmZvjss89w5coVafubN2+iTZs2KF26NMzNzVGjRg3ExMQAUL0sFRcXh+DgYDx69Eg6J/PO+ZcvS3Xv3h1dunRRqfHZs2ewsbHBsmXLALz4jsTw8HBUqVIFpqamqFOnDtavX1/Mn9T7heFGZpYuXQpzc3McO3YMM2bMwKRJk7B7924AQKdOnZCWlobt27cjPj4e9evXR/PmzXH//n0AwMqVKzF16lRMnz4d8fHxqFy5MubNm6ey/8zMTAQFBeHgwYM4evQoXF1d0apVK+nb1k+cOAEAWLx4MZKTk6XllzVv3hylSpXChg0bpDalUono6GgEBAQAAK5du4aWLVuiQ4cOOHPmDKKjo3Hw4EEMGTJE+x8a6cTbnKtvEhERAS8vL/Tr1w/JyclITk6Gg4ODtH7MmDGYNm0aLl68iNq1ayMrKwutWrVCbGws/vrrL7Rs2RJt2rRBUlJSsbx3KlmWLl0KAwMDHD9+HBEREZg5cyZ+++03AC+C8smTJ7FlyxYcOXIEQgi0atUKz549AwAMHjwYT58+xf79+3H27FlMnz69wFFqb29vzJ49G1ZWVtI5OXLkyHz9AgIC8OeffyIrK0tq27lzJx4/fowvvvgCABAeHo5ly5YhKioK58+fx4gRI9CjRw/s27evOD6e95Mg2fDx8REff/yxSlvDhg3F6NGjxYEDB4SVlZV48uSJynpnZ2cxf/58IYQQnp6eYvDgwSrrGzduLOrUqVPoMZVKpbC0tBR//vmn1AZA/PHHHyr9wsLCVPYzbNgw0axZM2l5586dwtjYWDx48EAIIUSfPn1E//79VfZx4MABoaenJ/79999C66H3w9ueq0FBQaJdu3Yq64cNGyZ8fHxUjjFs2DCVPnv37hUAxKZNm95YY40aNcScOXOkZUdHRzFr1qw3vzl6r/j4+Ijq1auL3NxcqW306NGievXq4u+//xYAxKFDh6R16enpwtTUVKxdu1YIIUStWrXEd999V+C+8863vP/XFi9eLKytrfP1e/ncevbsmbCxsRHLli2T1nfr1k106dJFCCHEkydPhJmZmTh8+LDKPvr06SO6deum8fuXK47cyEzt2rVVlsuXL4+0tDScPn0aWVlZKFu2rDT/xcLCAjdu3MC1a9cAAJcvX0ajRo1Utn91OTU1Ff369YOrqyusra1hZWWFrKwsjX/DDQgIQFxcHO7cuQPgxahR69atpcl2p0+fxpIlS1Rq9ff3R25uLm7cuKHRsahkeptz9W01aNBAZTkrKwsjR45E9erVUapUKVhYWODixYscuflA/O9//4NCoZCWvby8cOXKFVy4cAEGBgbw9PSU1pUtWxZVq1bFxYsXAQAhISGYMmUKGjdujLCwMJw5c+atajEwMEDnzp2xcuVKAEB2djY2b94sjWpfvXoVjx8/RosWLVT+fSxbtkxr/z7kgLPoZMbQ0FBlWaFQIDc3F1lZWShfvjzi4uLybVPQ7P3CBAUF4d69e4iIiICjoyOMjY3h5eWFnJwcjeps2LAhnJ2dsWbNGnz11Vf4448/sGTJEml9VlYWBgwYgJCQkHzbVq5cWaNjUcn0Nueqnp6eNCciT95lAnWYm5urLI8cORK7d+/GTz/9BBcXF5iamqJjx44an9f04enbty/8/f2xbds27Nq1C+Hh4fj5558xdOjQIu8zICAAPj4+SEtLw+7du2FqaoqWLVsCgHS5atu2bahYsaLKdvzuqv8w3Hwg6tevj5SUFBgYGEiTfF9VtWpVnDhxAoGBgVLbq3NmDh06hF9//RWtWrUCANy6dQvp6ekqfQwNDaFUKt9YU0BAAFauXIlKlSpBT08PrVu3Vqn3woULcHFxUfctkkyoc67a2tri3LlzKm2nTp1SCUxGRkZqnYfAi/O6V69e0pyGrKwsJCYmFql+ev8cO3ZMZTlvPqG7uzueP3+OY8eOwdvbGwBw7949XL58Ge7u7lJ/BwcHDBw4EAMHDsTYsWOxcOHCAsONuuekt7c3HBwcEB0dje3bt6NTp07Sue3u7g5jY2MkJSXBx8fnbd62rPGy1AfCz88PXl5eaN++PXbt2oXExEQcPnwY48ePx8mTJwEAQ4cOxe+//46lS5fiypUrmDJlCs6cOaMyXOvq6orly5fj4sWLOHbsGAICAmBqaqpyLCcnJ8TGxiIlJQUPHjwotKaAgAAkJCRg6tSp6Nixo8pvHaNHj8bhw4cxZMgQnDp1CleuXMHmzZs5ofgDoM652qxZM5w8eRLLli3DlStXEBYWli/sODk54dixY0hMTER6ejpyc3MLPaarqys2btyIU6dO4fTp0+jevftr+5O8JCUlITQ0FJcvX8bq1asxZ84cDBs2DK6urmjXrh369euHgwcP4vTp0+jRowcqVqyIdu3aAXhxl97OnTtx48YNJCQkYO/evahevXqBx3FyckJWVhZiY2ORnp6Ox48fF1pT9+7dERUVhd27d0uXpADA0tISI0eOxIgRI7B06VJcu3YNCQkJmDNnDpYuXardD+Y9xnDzgVAoFIiJicEnn3yC4OBguLm5oWvXrrh58ybs7OwAvAgbY8eOxciRI1G/fn3cuHEDvXr1gomJibSf33//HQ8ePED9+vXRs2dPhISEoFy5cirH+vnnn7F79244ODigXr16hdbk4uKCRo0a4cyZMyr/eIEX8zH27duHv//+G02aNEG9evUwceJEVKhQQYufCpVE6pyr/v7+mDBhAr755hs0bNgQmZmZKiOOwItLTfr6+nB3d4etre1r58/MnDkTpUuXhre3N9q0aQN/f3/Ur1+/WN8nlRyBgYH4999/0ahRIwwePBjDhg2THqq3ePFieHh44PPPP4eXlxeEEIiJiZFGUpRKJQYPHozq1aujZcuWcHNzw6+//lrgcby9vTFw4EB06dIFtra2mDFjRqE1BQQE4MKFC6hYsSIaN26ssm7y5MmYMGECwsPDpeNu27YNVapU0dIn8v5TiFcvXBO9pEWLFrC3t8fy5ct1XQoRkdb5+vqibt26/PoDmeGcG5I8fvwYUVFR8Pf3h76+PlavXo09e/ZIzx4hIiJ6HzDckCTvcsDUqVPx5MkTVK1aFRs2bICfn5+uSyMiIlIbL0sRERGRrHBCMREREckKww0RERHJCsMNERERyQrDDREREckKww0Rvdfi4uKgUCjw8OFDXZdCRCUEww0RacXdu3fx1VdfoXLlyjA2Noa9vT38/f1x6NAhrR3D19cXw4cPV2nz9vZGcnIyrK2ttXacourVqxfat2+v6zKIPnh8zg0RaUWHDh2Qk5ODpUuX4qOPPkJqaipiY2Nx7969Yj2ukZER7O3ti/UYRPSeEUREb+nBgwcCgIiLi3ttnz59+ggbGxthaWkpmjZtKk6dOiWtDwsLE3Xq1BHLli0Tjo6OwsrKSnTp0kVkZGQIIYQICgoSAFReN27cEHv37hUAxIMHD4QQQixevFhYW1uLP//8U7i5uQlTU1PRoUMHkZ2dLZYsWSIcHR1FqVKlxNChQ8Xz58+l4z958kR8/fXXokKFCsLMzEw0atRI7N27V1qft98dO3aIatWqCXNzc+Hv7y/u3Lkj1f9qfS9vT0TvDi9LEdFbs7CwgIWFBTZt2oSnT58W2KdTp05IS0vD9u3bER8fj/r166N58+a4f/++1OfatWvYtGkTtm7diq1bt2Lfvn2YNm0aACAiIgJeXl7o168fkpOTkZycDAcHhwKP9fjxY/zyyy9Ys2YNduzYgbi4OHzxxReIiYlBTEwMli9fjvnz52P9+vXSNkOGDMGRI0ewZs0anDlzBp06dULLli1x5coVlf3+9NNPWL58Ofbv34+kpCSMHDkSwIsv6uzcuTNatmwp1eft7f3Wny0RFYGu0xURycP69etF6dKlhYmJifD29hZjx44Vp0+fFkIIceDAAWFlZSWePHmiso2zs7OYP3++EOLFyIeZmZk0UiOEEKNGjRKenp7Sso+Pjxg2bJjKPgoauQEgrl69KvUZMGCAMDMzE5mZmVKbv7+/GDBggBBCiJs3bwp9fX1x+/ZtlX03b95cjB07ttD9RkZGCjs7O2k5KChItGvXTq3Pi4iKD+fcEJFWdOjQAa1bt8aBAwdw9OhRbN++HTNmzMBvv/2G7OxsZGVloWzZsirb/Pvvv7h27Zq07OTkBEtLS2m5fPnySEtL07gWMzMzODs7S8t2dnZwcnKChYWFSlvevs+ePQulUgk3NzeV/Tx9+lSl5lf3W9T6iKh4MdwQkdaYmJigRYsWaNGiBSZMmIC+ffsiLCwMgwYNQvny5REXF5dvm1KlSkl/NjQ0VFmnUCiQm5urcR0F7ed1+87KyoK+vj7i4+Ohr6+v0u/lQFTQPgS/no+oxGG4IaJi4+7ujk2bNqF+/fpISUmBgYEBnJycirw/IyMjKJVK7RX4/9WrVw9KpRJpaWlo0qRJkfdTXPURkWY4oZiI3tq9e/fQrFkzrFixAmfOnMGNGzewbt06zJgxA+3atYOfnx+8vLzQvn177Nq1C4mJiTh8+DDGjx+PkydPqn0cJycnHDt2DImJiUhPTy/SqE5B3NzcEBAQgMDAQGzcuBE3btzA8ePHER4ejm3btmlU35kzZ3D58mWkp6fj2bNnWqmPiDTDcENEb83CwgKenp6YNWsWPvnkE9SsWRMTJkxAv379MHfuXCgUCsTExOCTTz5BcHAw3Nzc0LVrV9y8eRN2dnZqH2fkyJHQ19eHu7s7bG1tkZSUpLX3sHjxYgQGBuLrr79G1apV0b59e5w4cQKVK1dWex/9+vVD1apV0aBBA9ja2mr1AYZEpD6F4AVjIiIikhGO3BAREZGsMNwQERGRrDDcEBERkaww3BAREZGsMNwQERGRrDDcEBERkaww3BAREZGsMNwQERGRrDDcEBERkaww3BAREZGsMNwQERGRrDDcEBERkaz8P/ilaotRa3icAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize VADER compound scores by sentiment\n",
    "\n",
    "negative_scores = vader_df[vader_df['sentiment'] == -1]['compound']\n",
    "neutral_scores = vader_df[vader_df['sentiment'] == 0]['compound']\n",
    "positive_scores = vader_df[vader_df['sentiment'] == 1]['compound']\n",
    "all_scores = pd.concat([negative_scores, neutral_scores, positive_scores])\n",
    "\n",
    "sentiments = ['negative'] * len(negative_scores) + ['neutral'] * len(neutral_scores) + ['positive'] * len(positive_scores)\n",
    "sentiment_scores = pd.DataFrame({'sentiment': sentiments, 'compound': all_scores})\n",
    "\n",
    "sns.barplot(data=sentiment_scores, x='sentiment', y='compound')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('VADER Compound Score')\n",
    "plt.title('VADER Compound Score by Sentiment')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
