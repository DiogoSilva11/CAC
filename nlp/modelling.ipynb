{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from xgboost import XGBClassifier\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load review sentiment data\n",
    "\n",
    "review_df = pd.read_csv('data/review_sentiment.csv')\n",
    "\n",
    "review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select and load feature data\n",
    "\n",
    "feature_set = {\n",
    "    0: 'bag_of_words',\n",
    "    1: 'one_hot',\n",
    "    2: 'n_grams',\n",
    "    3: 'tf_idf',\n",
    "    4: 'word2vec',\n",
    "    5: 'combined_features'\n",
    "}\n",
    "selected = 5\n",
    "\n",
    "X = None\n",
    "if selected == 4:\n",
    "    X = np.load('features/' + feature_set[selected] + '.npy')\n",
    "else:\n",
    "    X = sparse.load_npz('features/' + feature_set[selected] + '.npz')\n",
    "    \n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target labels\n",
    "\n",
    "y = review_df['sentiment']\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"\\nLabel distribution in the training set:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nLabel distribution in the test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling to balance the classes\n",
    "\n",
    "oversampler = RandomOverSampler()\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train.to_numpy())\n",
    "\n",
    "print(X_train_resampled.shape, y_train_resampled.shape)\n",
    "\n",
    "print(\"\\nLabel distribution after oversampling:\")\n",
    "print(pd.DataFrame(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers\n",
    "\n",
    "classifiers = {\n",
    "    'gaussian_nb': GaussianNB(),\n",
    "    'logistic_regression': LogisticRegression(max_iter=1000, random_state=0),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'svm': SVC(),\n",
    "    'perceptron': Perceptron(tol=1e-3, random_state=0),\n",
    "    'xgb': XGBClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a classifier\n",
    "\n",
    "selected = 'gaussian_nb'\n",
    "selected = 'decision_tree'\n",
    "\n",
    "clf = classifiers[selected]\n",
    "clf.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save classifier model\n",
    "\n",
    "joblib.dump(clf, 'models/' + selected + '.plk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a classifier model\n",
    "\n",
    "model = 'gaussian_nb'\n",
    "clf = joblib.load('models/' + model + '.plk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "\n",
    "y_pred_clf = clf.predict(X_test)\n",
    "print(y_pred_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "\n",
    "folds = 5\n",
    "cv_scores = cross_validate(clf, X, y, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], cv=folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for each classifier\n",
    "param_grids = {\n",
    "    'gaussian_nb': {},\n",
    "    'logistic_regression': {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'perceptron': {\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'penalty': [None, 'l2', 'l1', 'elasticnet']\n",
    "    },\n",
    "    'xgb': {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'n_estimators': [100, 200, 300]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_estimators = {}\n",
    "\n",
    "# Apply grid search to each classifier\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"Grid search for {clf_name}\")\n",
    "    param_grid = param_grids[clf_name]\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring='accuracy', cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_estimators[clf_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {clf_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best accuracy for {clf_name}: {grid_search.best_score_}\")\n",
    "    print(\"---------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER sentiment analysis\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "res = []\n",
    "y_pred_vader = []\n",
    "for review in review_df['text']:\n",
    "    sentiment = vader.polarity_scores(review)\n",
    "    res.append(sentiment)\n",
    "    pred = round(sentiment['compound'])\n",
    "    y_pred_vader.append(pred)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation results\n",
    "\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, y_pred):\n",
    "    '''\n",
    "    Evaluate the performance of a multi-label classifier\n",
    "    '''\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance metrics - classifier\n",
    "\n",
    "evaluate_model(y_test, y_pred_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance metrics - VADER\n",
    "\n",
    "y_true = review_df['sentiment']\n",
    "y_true = y_true.values\n",
    "\n",
    "evaluate_model(y_true, y_pred_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with VADER sentiment scores\n",
    "\n",
    "vader_df = pd.DataFrame(res)\n",
    "vader_df = vader_df['compound']\n",
    "vader_df = pd.concat([review_df, vader_df], axis=1)\n",
    "\n",
    "vader_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize VADER compound scores by sentiment\n",
    "\n",
    "negative_scores = vader_df[vader_df['sentiment'] == -1]['compound']\n",
    "neutral_scores = vader_df[vader_df['sentiment'] == 0]['compound']\n",
    "positive_scores = vader_df[vader_df['sentiment'] == 1]['compound']\n",
    "all_scores = pd.concat([negative_scores, neutral_scores, positive_scores])\n",
    "\n",
    "sentiments = ['negative'] * len(negative_scores) + ['neutral'] * len(neutral_scores) + ['positive'] * len(positive_scores)\n",
    "sentiment_scores = pd.DataFrame({'sentiment': sentiments, 'compound': all_scores})\n",
    "\n",
    "sns.barplot(data=sentiment_scores, x='sentiment', y='compound')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('VADER Compound Score')\n",
    "plt.title('VADER Compound Score by Sentiment')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
